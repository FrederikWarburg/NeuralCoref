{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d48a6972378bf6116b7a6b9883992d548128640c"
   },
   "source": [
    "This Kernel implements a modified version of **a state-of-art end-to-end neural correference resolution model** published in 2017: https://www.aclweb.org/anthology/D17-1018.\n",
    "This completition only focus on a specific case of  the generic reference resolution problem, and we only need pick out the correct mention from two candidates, which simplifies the model implementation.\n",
    "\n",
    "You can compare the result of this model  with the result by other non-RNN based DL models implemented in another kernel: https://www.kaggle.com/keyit92/coreference-resolution-by-mlp-cnn-coattention-nn. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.vector_cache', 'gap', 'gendered-pronoun-resolution', 'ontonotes', 'predictions_end2end_trained1.csv', 'predictions_end2end_trained2.csv', 'right_answers.csv']\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "import gc\n",
    "print(os.listdir(\"data\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "DATA_ROOT = 'data'\n",
    "GAP_DATA_FOLDER = os.path.join(DATA_ROOT, 'gap')\n",
    "SUB_DATA_FOLDER = os.path.join(DATA_ROOT, 'gendered-pronoun-resolution')\n",
    "#FAST_TEXT_DATA_FOLDER = os.path.join(DATA_ROOT, 'fasttext-crawl-300d-2M.vec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "461cb23b791e2d210e712c933e62de59d19aa20d"
   },
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "e42e2e7f636cf0702cc472f3d855b451554927dd"
   },
   "outputs": [],
   "source": [
    "test_df_path = os.path.join(GAP_DATA_FOLDER, 'gap-development.tsv')\n",
    "train_df_path = os.path.join(GAP_DATA_FOLDER, 'gap-test-neutral-spivak.tsv')\n",
    "dev_df_path = os.path.join(GAP_DATA_FOLDER, 'gap-validation-neutral-spivak.tsv')\n",
    "\n",
    "train_df = pd.read_csv(train_df_path, sep='\\t')\n",
    "test_df = pd.read_csv(test_df_path, sep='\\t')\n",
    "dev_df = pd.read_csv(dev_df_path, sep='\\t')\n",
    "\n",
    "# pd.options.display.max_colwidth = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Text</th>\n",
       "      <th>Pronoun</th>\n",
       "      <th>Pronoun-offset</th>\n",
       "      <th>A</th>\n",
       "      <th>A-offset</th>\n",
       "      <th>A-coref</th>\n",
       "      <th>B</th>\n",
       "      <th>B-offset</th>\n",
       "      <th>B-coref</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>test-1996</td>\n",
       "      <td>The sole exception was Wimbledon, where ey pla...</td>\n",
       "      <td>Ey</td>\n",
       "      <td>476</td>\n",
       "      <td>Goolagong Cawley</td>\n",
       "      <td>397</td>\n",
       "      <td>True</td>\n",
       "      <td>Peggy Michel</td>\n",
       "      <td>429</td>\n",
       "      <td>False</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Evonne_Goolagong_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>test-1997</td>\n",
       "      <td>According to news reports, both Moore and Fily...</td>\n",
       "      <td>em</td>\n",
       "      <td>338</td>\n",
       "      <td>Esther Sheryl Wood</td>\n",
       "      <td>263</td>\n",
       "      <td>True</td>\n",
       "      <td>Barbara Morgan</td>\n",
       "      <td>403</td>\n",
       "      <td>False</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Hastings_Arthur_Wise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>test-1998</td>\n",
       "      <td>In June 2009, due to the popularity of the Sab...</td>\n",
       "      <td>Ey</td>\n",
       "      <td>328</td>\n",
       "      <td>Kayla</td>\n",
       "      <td>363</td>\n",
       "      <td>True</td>\n",
       "      <td>Natasha Henstridge</td>\n",
       "      <td>410</td>\n",
       "      <td>False</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Raya_Meddine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>test-1999</td>\n",
       "      <td>Ey was delivered to the Norwegian passenger sh...</td>\n",
       "      <td>ey</td>\n",
       "      <td>303</td>\n",
       "      <td>Irma</td>\n",
       "      <td>253</td>\n",
       "      <td>True</td>\n",
       "      <td>Bergen</td>\n",
       "      <td>272</td>\n",
       "      <td>False</td>\n",
       "      <td>http://en.wikipedia.org/wiki/SS_Irma_(1905)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>test-2000</td>\n",
       "      <td>Meg and Vicky each have three siblings, and ha...</td>\n",
       "      <td>eir</td>\n",
       "      <td>275</td>\n",
       "      <td>Vicky Austin</td>\n",
       "      <td>217</td>\n",
       "      <td>True</td>\n",
       "      <td>Polly O'Keefe</td>\n",
       "      <td>260</td>\n",
       "      <td>False</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Vicky_Austin</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             ID                                               Text Pronoun  \\\n",
       "1995  test-1996  The sole exception was Wimbledon, where ey pla...      Ey   \n",
       "1996  test-1997  According to news reports, both Moore and Fily...      em   \n",
       "1997  test-1998  In June 2009, due to the popularity of the Sab...      Ey   \n",
       "1998  test-1999  Ey was delivered to the Norwegian passenger sh...      ey   \n",
       "1999  test-2000  Meg and Vicky each have three siblings, and ha...     eir   \n",
       "\n",
       "      Pronoun-offset                   A  A-offset  A-coref  \\\n",
       "1995             476    Goolagong Cawley       397     True   \n",
       "1996             338  Esther Sheryl Wood       263     True   \n",
       "1997             328               Kayla       363     True   \n",
       "1998             303                Irma       253     True   \n",
       "1999             275        Vicky Austin       217     True   \n",
       "\n",
       "                       B  B-offset  B-coref  \\\n",
       "1995        Peggy Michel       429    False   \n",
       "1996      Barbara Morgan       403    False   \n",
       "1997  Natasha Henstridge       410    False   \n",
       "1998              Bergen       272    False   \n",
       "1999       Polly O'Keefe       260    False   \n",
       "\n",
       "                                                    URL  \n",
       "1995  http://en.wikipedia.org/wiki/Evonne_Goolagong_...  \n",
       "1996  http://en.wikipedia.org/wiki/Hastings_Arthur_Wise  \n",
       "1997          http://en.wikipedia.org/wiki/Raya_Meddine  \n",
       "1998        http://en.wikipedia.org/wiki/SS_Irma_(1905)  \n",
       "1999          http://en.wikipedia.org/wiki/Vicky_Austin  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d60addb918fc181bd736f5497c3114dbf3d3bfd4"
   },
   "source": [
    "# Explore Features for Building Mention-Pair Distributed Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "8b82c9f910c289c5b21ec982890911bfb579c32b"
   },
   "outputs": [],
   "source": [
    "spacy_model = \"en_core_web_lg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "177d5f8cca6584d7e542ea1ed6112e091e78d787"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.en import English\n",
    "from spacy.pipeline import DependencyParser\n",
    "import spacy\n",
    "from nltk import Tree\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing import text as ktext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.3'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "1668e73e26b428fa0739c1c38653217d14d50aaa"
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load(spacy_model)\n",
    "\n",
    "def bs(list_, target_):\n",
    "    lo, hi = 0, len(list_) -1\n",
    "    \n",
    "    while lo < hi:\n",
    "        mid = lo + int((hi - lo) / 2)\n",
    "        \n",
    "        if target_ < list_[mid]:\n",
    "            hi = mid\n",
    "        elif target_ > list_[mid]:\n",
    "            lo = mid + 1\n",
    "        else:\n",
    "            return mid + 1\n",
    "    return lo\n",
    "\n",
    "def bs_(list_, target_):\n",
    "    lo, hi = 0, len(list_) -1\n",
    "    \n",
    "    while lo < hi:\n",
    "        mid = lo + int((hi - lo) / 2)\n",
    "        \n",
    "        if target_ < list_[mid]:\n",
    "            hi = mid\n",
    "        elif target_ > list_[mid]:\n",
    "            lo = mid + 1\n",
    "        else:\n",
    "            return mid\n",
    "    return lo\n",
    "\n",
    "def ohe_dist(dist, buckets):\n",
    "    idx = bs_(buckets, dist)\n",
    "    oh = np.zeros(shape=(len(buckets),), dtype=np.float32)\n",
    "    oh[idx] = 1\n",
    "    \n",
    "    return oh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2755927ba88262427347c567ae777c92510ffa68"
   },
   "source": [
    " ##  Position Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "15165f2b90503a90de589b37558ebcea28daa867"
   },
   "source": [
    "Encode the absolute positions in the sentence and the relative position between the pronoun and the entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "c1c14436f57e74e80ac4425f77df98768c71f7b9"
   },
   "outputs": [],
   "source": [
    "num_pos_features = 45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "9f3d21e9a626a3290c81c6bc98b817c3e47275be"
   },
   "outputs": [],
   "source": [
    "def extrac_positional_features(text, char_offset1, char_offset2):\n",
    "    doc = nlp(text)\n",
    "    max_len = 64\n",
    "    \n",
    "    # char offset to token offset\n",
    "    lens = [token.idx for token in doc]\n",
    "    mention_offset1 = bs(lens, char_offset1) - 1\n",
    "    mention_offset2 = bs(lens, char_offset2) - 1\n",
    "    \n",
    "    # token offset to sentence offset\n",
    "    lens = [len(sent) for sent in doc.sents]\n",
    "    acc_lens = [len_ for len_ in lens]\n",
    "    pre_len = 0\n",
    "    for i in range(0, len(acc_lens)):\n",
    "        pre_len += acc_lens[i]\n",
    "        acc_lens[i] = pre_len\n",
    "    sent_index1 = bs(acc_lens, mention_offset1)\n",
    "    sent_index2 = bs(acc_lens, mention_offset2)\n",
    "    \n",
    "    sent1 = list(doc.sents)[sent_index1]\n",
    "    sent2 = list(doc.sents)[sent_index2]\n",
    "    \n",
    "    # buckets\n",
    "    bucket_dist = [1, 2, 3, 4, 5, 8, 16, 32, 64]\n",
    "    \n",
    "    # relative distance\n",
    "    dist = mention_offset2 - mention_offset1\n",
    "    dist_oh = ohe_dist(dist, bucket_dist)\n",
    "    \n",
    "    # buckets\n",
    "    bucket_pos = [0, 1, 2, 3, 4, 5, 8, 16, 32]\n",
    "    \n",
    "    # absolute position in the sentence\n",
    "    sent_pos1 = mention_offset1 + 1\n",
    "    if sent_index1 > 0:\n",
    "        sent_pos1 = mention_offset1 - acc_lens[sent_index1-1]\n",
    "    sent_pos_oh1 = ohe_dist(sent_pos1, bucket_pos)\n",
    "    sent_pos_inv1 = len(sent1) - sent_pos1\n",
    "    assert sent_pos_inv1 >= 0\n",
    "    sent_pos_inv_oh1 = ohe_dist(sent_pos_inv1, bucket_pos)\n",
    "    \n",
    "    sent_pos2 = mention_offset2 + 1\n",
    "    if sent_index2 > 0:\n",
    "        sent_pos2 = mention_offset2 - acc_lens[sent_index2-1]\n",
    "    sent_pos_oh2 = ohe_dist(sent_pos2, bucket_pos)\n",
    "    sent_pos_inv2 = len(sent2) - sent_pos2\n",
    "    if sent_pos_inv2 < 0:\n",
    "        print(sent_pos_inv2)\n",
    "        print(len(sent2))\n",
    "        print(sent_pos2)\n",
    "        raise ValueError\n",
    "    sent_pos_inv_oh2 = ohe_dist(sent_pos_inv2, bucket_pos)\n",
    "    \n",
    "    sent_pos_ratio1 = sent_pos1 / len(sent1)\n",
    "    sent_pos_ratio2 = sent_pos2 / len(sent2)\n",
    "    \n",
    "    return dist_oh, sent_pos_oh1, sent_pos_oh2, sent_pos_inv_oh1, sent_pos_inv_oh2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "4a9ec346bff260b62ebea6d9223c6a11e683a9ee"
   },
   "outputs": [],
   "source": [
    "def create_dist_features(df, text_column, pronoun_offset_column, name_offset_column):\n",
    "    text_offset_list = df[[text_column, pronoun_offset_column, name_offset_column]].values.tolist()\n",
    "    num_features = num_pos_features\n",
    "    \n",
    "    pos_feature_matrix = np.zeros(shape=(len(text_offset_list), num_features))\n",
    "    for text_offset_index in range(len(text_offset_list)):\n",
    "        text_offset = text_offset_list[text_offset_index]\n",
    "        dist_oh, sent_pos_oh1, sent_pos_oh2, sent_pos_inv_oh1, sent_pos_inv_oh2 = extrac_positional_features(text_offset[0], text_offset[1], text_offset[2])\n",
    "        \n",
    "        feature_index = 0\n",
    "        pos_feature_matrix[text_offset_index, feature_index:feature_index+len(dist_oh)] = np.asarray(dist_oh)\n",
    "        feature_index += len(dist_oh)\n",
    "        pos_feature_matrix[text_offset_index, feature_index:feature_index+len(sent_pos_oh1)] = np.asarray(sent_pos_oh1)\n",
    "        feature_index += len(sent_pos_oh1)\n",
    "        pos_feature_matrix[text_offset_index, feature_index:feature_index+len(sent_pos_oh2)] = np.asarray(sent_pos_oh2)\n",
    "        feature_index += len(sent_pos_oh2)\n",
    "        pos_feature_matrix[text_offset_index, feature_index:feature_index+len(sent_pos_inv_oh1)] = np.asarray(sent_pos_inv_oh1)\n",
    "        feature_index += len(sent_pos_inv_oh1)\n",
    "        pos_feature_matrix[text_offset_index, feature_index:feature_index+len(sent_pos_inv_oh2)] = np.asarray(sent_pos_inv_oh2)\n",
    "        feature_index += len(sent_pos_inv_oh2)\n",
    "    \n",
    "    return pos_feature_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "488aaec745f7f6eea342a5746bcf7bef206c60d3"
   },
   "source": [
    "## Extract Sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "51de0d6b4223279969593956248e0fb99284c5d7"
   },
   "source": [
    "Select the surrounding 100 words around the mention in the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "2edcfef36582c91246e7ab772d5a23c27a272f92"
   },
   "outputs": [],
   "source": [
    "max_len = 50 # longer than 99% of the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "11af138a12f54e9c0a8ce92a782383c9ad5a92ff"
   },
   "outputs": [],
   "source": [
    "seq_list = list()\n",
    "def extract_sents(text, char_offset_p, char_offset_a, char_offset_b, id):\n",
    "    global max_len\n",
    "    global seq_list\n",
    "    \n",
    "    seq_list.append(list())\n",
    "    \n",
    "    doc = nlp(text)\n",
    "    token_lens = [token.idx for token in doc]\n",
    "    \n",
    "    char_offsets = [char_offset_p, char_offset_a, char_offset_b]\n",
    "    sent_list = list()\n",
    "    \n",
    "    for char_offset in char_offsets:\n",
    "        # char offset to token offset\n",
    "        mention_offset = bs(token_lens, char_offset) - 1\n",
    "        # mention_word\n",
    "        mention = doc[mention_offset]\n",
    "    \n",
    "        # token offset to sentence offset\n",
    "        lens = [len(sent) for sent in doc.sents]\n",
    "        acc_lens = [len_ for len_ in lens]\n",
    "        pre_len = 0\n",
    "        for i in range(0, len(acc_lens)):\n",
    "            pre_len += acc_lens[i]\n",
    "            acc_lens[i] = pre_len\n",
    "        sent_index = bs(acc_lens, mention_offset)\n",
    "        # mention sentence\n",
    "        sent = list(doc.sents)[sent_index]\n",
    "        \n",
    "        # absolute position in the sentence\n",
    "        sent_pos = mention_offset + 1\n",
    "        if sent_index > 0:\n",
    "            sent_pos = mention_offset - acc_lens[sent_index-1]\n",
    "        \n",
    "        # clip the sentence if it is longer than max length\n",
    "        if len(sent) > max_len:\n",
    "            # make sure the mention is in the sentence span\n",
    "            if sent_pos < max_len-1:\n",
    "                sent_list.append(sent[0:max_len].text)\n",
    "                sent_list.append(sent_pos)\n",
    "                seq_list[-1].append(sent[0:max_len])\n",
    "            else:\n",
    "                sent_list.append(sent[sent_pos-max_len+2 : min(sent_pos+2, len(sent))].text)\n",
    "                sent_list.append(max_len-2)\n",
    "                seq_list[-1].append(sent[sent_pos-max_len+2 : min(sent_pos+2, len(sent))])\n",
    "        else:\n",
    "            sent_list.append(sent.text)\n",
    "            sent_list.append(sent_pos)\n",
    "            seq_list[-1].append(sent)\n",
    "        \n",
    "    return pd.Series([id] + sent_list, index=['ID', 'Pronoun-Sent', 'Pronoun-Sent-Offset', 'A-Sent', 'A-Sent-Offset', 'B-Sent', 'B-Sent-Offset'])\n",
    "\n",
    "def add_sent_columns(df, text_column, pronoun_offset_column, a_offset_column, b_offset_column):\n",
    "    global seq_list\n",
    "    seq_list = list()\n",
    "    sent_df = df.apply(lambda row: extract_sents(row.loc[text_column], row[pronoun_offset_column], row[a_offset_column], row[b_offset_column], row['ID']), axis=1)\n",
    "    df = df.join(sent_df.set_index('ID'), on='ID')\n",
    "    return df, seq_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9c0630e230a389ab7ba9bc4bc8140cabf769f5ff"
   },
   "source": [
    "## Create Train, Dev and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "f455cc1f3150d1dcc42a3aef2434c8d5e703c990"
   },
   "outputs": [],
   "source": [
    "seq_list = list()\n",
    "train_df, train_tokenized = add_sent_columns(train_df, 'Text', 'Pronoun-offset', 'A-offset', 'B-offset')\n",
    "seq_list = list()\n",
    "test_df, test_tokenized = add_sent_columns(test_df, 'Text', 'Pronoun-offset', 'A-offset', 'B-offset')\n",
    "seq_list = list()\n",
    "dev_df, dev_tokenized = add_sent_columns(dev_df, 'Text', 'Pronoun-offset', 'A-offset', 'B-offset')\n",
    "\n",
    "# df apply will call the first row twice, remove the first one\n",
    "train_tokenized = train_tokenized[1:]\n",
    "test_tokenized = test_tokenized[1:]\n",
    "dev_tokenized = dev_tokenized[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2873a775c7be71521ef5b8922a61f11a93e62b57"
   },
   "source": [
    "### Create Vocab and Embedding Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "2d34b3f5fdce49c97e9fb9d153126546465b3965"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22283, 300)\n"
     ]
    }
   ],
   "source": [
    "embed_size = 300\n",
    "max_features = 80000\n",
    "\n",
    "# generate word index\n",
    "word_index = dict()\n",
    "idx = 1\n",
    "for text_ in train_tokenized+test_tokenized+dev_tokenized:\n",
    "    for sent_ in text_:\n",
    "        for word_ in sent_:\n",
    "            if word_.text not in word_index and nlp.vocab.has_vector(word_.text):\n",
    "                word_index[word_.text] = idx\n",
    "                idx += 1\n",
    "\n",
    "nb_words = min(max_features, len(word_index))\n",
    "embedding_matrix = np.zeros((nb_words + 1, embed_size))\n",
    "        \n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features: continue\n",
    "    embedding_vector = None\n",
    "    if nlp.vocab.has_vector(word):\n",
    "        embedding_vector = nlp.vocab.vectors[nlp.vocab.strings[word]]\n",
    "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "print(embedding_matrix.shape)\n",
    "\n",
    "# generate pos tag index\n",
    "pos_index = dict()\n",
    "idx = 1\n",
    "for text_ in train_tokenized+test_tokenized+dev_tokenized:\n",
    "    for sent_ in text_:\n",
    "        for word_ in sent_:\n",
    "            if word_.pos not in pos_index:\n",
    "                pos_index[word_.pos] = idx\n",
    "                idx += 1\n",
    "\n",
    "def sentences_to_sequences(tokenized_):\n",
    "    return list(map(\n",
    "        lambda sent_tokenized: list(map(\n",
    "            lambda token_: word_index[token_.text] if token_.text in word_index else 0,\n",
    "            sent_tokenized\n",
    "        )),\n",
    "        tokenized_\n",
    "    ))\n",
    "\n",
    "def poses_to_sequences(tokenized_):\n",
    "    return list(map(\n",
    "        lambda sent_tokenized: list(map(\n",
    "            lambda token_: pos_index[token_.pos] if token_.pos in pos_index else 0,\n",
    "            sent_tokenized\n",
    "        )),\n",
    "        tokenized_\n",
    "    ))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_uuid": "41ca3b1434037a8f7296850ba4af9c837e247368"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x22acf7a14a8>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.distplot(train_df['Pronoun-Sent'].map(lambda ele: len(ele.split(\" \"))), kde_kws={\"label\": \"P\"})\n",
    "\n",
    "sns.distplot(train_df['A-Sent'].map(lambda ele: len(ele.split(\" \"))), kde_kws={\"label\": \"A\"})\n",
    "\n",
    "sns.distplot(train_df['B-Sent'].map(lambda ele: len(ele.split(\" \"))), kde_kws={\"label\": \"B\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_uuid": "35dda1595e80bed9e310dd5af9b89b726a878e8b"
   },
   "outputs": [],
   "source": [
    "train_p_tokenized = sentences_to_sequences([row[0] for row in train_tokenized])\n",
    "train_a_tokenized = sentences_to_sequences([row[1] for row in train_tokenized])\n",
    "train_b_tokenized = sentences_to_sequences([row[2] for row in train_tokenized])\n",
    "\n",
    "test_p_tokenized = sentences_to_sequences([row[0] for row in test_tokenized])\n",
    "test_a_tokenized = sentences_to_sequences([row[1] for row in test_tokenized])\n",
    "test_b_tokenized = sentences_to_sequences([row[2] for row in test_tokenized])\n",
    "\n",
    "dev_p_tokenized = sentences_to_sequences([row[0] for row in dev_tokenized])\n",
    "dev_a_tokenized = sentences_to_sequences([row[1] for row in dev_tokenized])\n",
    "dev_b_tokenized = sentences_to_sequences([row[2] for row in dev_tokenized])\n",
    "\n",
    "seq_p_train = sequence.pad_sequences(train_p_tokenized, maxlen = max_len, padding='post')\n",
    "seq_a_train = sequence.pad_sequences(train_a_tokenized, maxlen = max_len, padding='post')\n",
    "seq_b_train = sequence.pad_sequences(train_b_tokenized, maxlen = max_len, padding='post')\n",
    "\n",
    "seq_p_test = sequence.pad_sequences(test_p_tokenized, maxlen = max_len, padding='post')\n",
    "seq_a_test = sequence.pad_sequences(test_a_tokenized, maxlen = max_len, padding='post')\n",
    "seq_b_test = sequence.pad_sequences(test_b_tokenized, maxlen = max_len, padding='post')\n",
    "\n",
    "seq_p_dev = sequence.pad_sequences(dev_p_tokenized, maxlen = max_len, padding='post')\n",
    "seq_a_dev = sequence.pad_sequences(dev_a_tokenized, maxlen = max_len, padding='post')\n",
    "seq_b_dev = sequence.pad_sequences(dev_b_tokenized, maxlen = max_len, padding='post')\n",
    "\n",
    "train_p_pos = poses_to_sequences([row[0] for row in train_tokenized])\n",
    "train_a_pos = poses_to_sequences([row[1] for row in train_tokenized])\n",
    "train_b_pos = poses_to_sequences([row[2] for row in train_tokenized])\n",
    "\n",
    "test_p_pos = poses_to_sequences([row[0] for row in test_tokenized])\n",
    "test_a_pos = poses_to_sequences([row[1] for row in test_tokenized])\n",
    "test_b_pos = poses_to_sequences([row[2] for row in test_tokenized])\n",
    "\n",
    "dev_p_pos = poses_to_sequences([row[0] for row in dev_tokenized])\n",
    "dev_a_pos = poses_to_sequences([row[1] for row in dev_tokenized])\n",
    "dev_b_pos = poses_to_sequences([row[2] for row in dev_tokenized])\n",
    "\n",
    "pos_p_train = sequence.pad_sequences(train_p_pos, maxlen = max_len, padding='post')\n",
    "pos_a_train = sequence.pad_sequences(train_a_pos, maxlen = max_len, padding='post')\n",
    "pos_b_train = sequence.pad_sequences(train_b_pos, maxlen = max_len, padding='post')\n",
    "\n",
    "pos_p_test = sequence.pad_sequences(test_p_pos, maxlen = max_len, padding='post')\n",
    "pos_a_test = sequence.pad_sequences(test_a_pos, maxlen = max_len, padding='post')\n",
    "pos_b_test = sequence.pad_sequences(test_b_pos, maxlen = max_len, padding='post')\n",
    "\n",
    "pos_p_dev = sequence.pad_sequences(dev_p_pos, maxlen = max_len, padding='post')\n",
    "pos_a_dev = sequence.pad_sequences(dev_a_pos, maxlen = max_len, padding='post')\n",
    "pos_b_dev = sequence.pad_sequences(dev_b_pos, maxlen = max_len, padding='post')\n",
    "\n",
    "index_p_train = train_df['Pronoun-Sent-Offset'].values\n",
    "index_a_train = train_df['A-Sent-Offset'].values\n",
    "index_b_train = train_df['B-Sent-Offset'].values\n",
    "\n",
    "index_p_test = test_df['Pronoun-Sent-Offset'].values\n",
    "index_a_test = test_df['A-Sent-Offset'].values\n",
    "index_b_test = test_df['B-Sent-Offset'].values\n",
    "\n",
    "index_p_dev = dev_df['Pronoun-Sent-Offset'].values\n",
    "index_a_dev = dev_df['A-Sent-Offset'].values\n",
    "index_b_dev = dev_df['B-Sent-Offset'].values\n",
    "\n",
    "pa_pos_tra = create_dist_features(train_df, 'Text', 'Pronoun-offset', 'A-offset')\n",
    "pa_pos_dev = create_dist_features(dev_df, 'Text', 'Pronoun-offset', 'A-offset')\n",
    "pa_pos_test = create_dist_features(test_df, 'Text', 'Pronoun-offset', 'A-offset')\n",
    "\n",
    "pb_pos_tra = create_dist_features(train_df, 'Text', 'Pronoun-offset', 'B-offset')\n",
    "pb_pos_dev = create_dist_features(dev_df, 'Text', 'Pronoun-offset', 'B-offset')\n",
    "pb_pos_test = create_dist_features(test_df, 'Text', 'Pronoun-offset', 'B-offset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_uuid": "f46a80420d56262e4fed998d16f9f9105d502c02"
   },
   "outputs": [],
   "source": [
    "X_train = [seq_p_train, seq_a_train, seq_b_train, pos_p_train, pos_a_train, pos_b_train, index_p_train, index_a_train, index_b_train, pa_pos_tra, pb_pos_tra]\n",
    "X_dev = [seq_p_dev, seq_a_dev, seq_b_dev, pos_p_dev, pos_a_dev, pos_b_dev, index_p_dev, index_a_dev, index_b_dev, pa_pos_dev, pb_pos_dev]\n",
    "X_test = [seq_p_test, seq_a_test, seq_b_test, pos_p_test, pos_a_test, pos_b_test, index_p_test, index_a_test, index_b_test, pa_pos_test, pb_pos_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_uuid": "2842410a113a578c1c274630b195038af0dfd75c"
   },
   "outputs": [],
   "source": [
    "def _row_to_y(row):\n",
    "    if row.loc['A-coref']:\n",
    "        return 0\n",
    "    if row.loc['B-coref']:\n",
    "        return 1\n",
    "    return 2\n",
    "\n",
    "y_tra = train_df.apply(_row_to_y, axis=1)\n",
    "y_dev = dev_df.apply(_row_to_y, axis=1)\n",
    "y_test = test_df.apply(_row_to_y, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "33ee19bc3e355ea58dd27bd5c373a13682536dd4"
   },
   "source": [
    "# Define Keras Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_uuid": "cb3cacc8e7dc332a6b5cacf3d733f314b7f03cba"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras import backend\n",
    "from keras import layers\n",
    "from keras import models\n",
    "\n",
    "from keras import initializers, regularizers, constraints, activations\n",
    "from keras.engine import Layer\n",
    "import keras.backend as K\n",
    "from keras.layers import merge\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_uuid": "3830a23390eaf527992b3676e03857699969957e"
   },
   "outputs": [],
   "source": [
    "def _dot_product(x, kernel):\n",
    "    \"\"\"\n",
    "    Wrapper for dot product operation, in order to be compatible with both\n",
    "    Theano and Tensorflow\n",
    "    Args:\n",
    "        x (): input\n",
    "        kernel (): weights\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    if K.backend() == 'tensorflow':\n",
    "        # todo: check that this is correct\n",
    "        return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n",
    "    else:\n",
    "        return K.dot(x, kernel)\n",
    "    \n",
    "    \n",
    "class AttentionWeight(Layer):\n",
    "    \"\"\"\n",
    "        This code is a modified version of cbaziotis implementation:  GithubGist cbaziotis/AttentionWithContext.py\n",
    "        Attention operation, with a context/query vector, for temporal data.\n",
    "        Supports Masking.\n",
    "        Follows the work of Yang et al. [https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf]\n",
    "        \"Hierarchical Attention Networks for Document Classification\"\n",
    "        by using a context vector to assist the attention\n",
    "        # Input shape\n",
    "            3D tensor with shape: `(samples, steps, features)`.\n",
    "        # Output shape\n",
    "            2D tensor with shape: `(samples, steps)`.\n",
    "        :param kwargs:\n",
    "        Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
    "        The dimensions are inferred based on the output shape of the RNN.\n",
    "        Example:\n",
    "            model.add(LSTM(64, return_sequences=True))\n",
    "            model.add(AttentionWeight())\n",
    "        \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        super(AttentionWeight, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        shape1 = input_shape[0]\n",
    "        shape2 = input_shape[1]\n",
    "\n",
    "        self.W = self.add_weight((shape2[-1], shape1[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((shape2[-1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        x = inputs[0]\n",
    "        u = inputs[1]\n",
    "        \n",
    "        uit = _dot_product(x, self.W)\n",
    "\n",
    "        if self.bias:\n",
    "            uit += self.b\n",
    "\n",
    "        uit = K.tanh(uit)\n",
    "        ait = K.batch_dot(uit, u)\n",
    "\n",
    "        a = K.exp(ait)\n",
    "\n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\n",
    "        # and this results in NaN's. A workaround is to add a very small positive number  to the sum.\n",
    "        # a /= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx())\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "        \n",
    "        return a\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        if not isinstance(input_shape, list) or len(input_shape) != 2:\n",
    "            raise ValueError('A `Dot` layer should be called '\n",
    "                             'on a list of 2 inputs.')\n",
    "        shape1 = list(input_shape[0])\n",
    "        shape2 = list(input_shape[1])\n",
    "        \n",
    "        return shape1[0], shape1[1]\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'W_regularizer': regularizers.serialize(self.W_regularizer),\n",
    "            'b_regularizer': regularizers.serialize(self.b_regularizer),\n",
    "            'W_constraint': constraints.serialize(self.W_constraint),\n",
    "            'b_constraint': constraints.serialize(self.b_constraint),\n",
    "            'bias': self.bias\n",
    "        }\n",
    "        base_config = super(AttentionWeight, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "    \n",
    "    \n",
    "class FeatureSelection1D(Layer):\n",
    "    \"\"\"\n",
    "        Normalize feature along a specific axis.\n",
    "        Supports Masking.\n",
    "\n",
    "        # Input shape\n",
    "            A ND tensor with shape: `(samples, timesteps, features)\n",
    "            A 2D tensor with shape: [samples, num_selected_features]\n",
    "        # Output shape\n",
    "            ND tensor with shape: `(samples, num_selected_features, features)`.\n",
    "        :param kwargs:\n",
    "        \"\"\"\n",
    "\n",
    "    def __init__(self, num_selects, **kwargs):\n",
    "\n",
    "        self.num_selects = num_selects\n",
    "        self.supports_masking = True\n",
    "        super(FeatureSelection1D, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "\n",
    "        super(FeatureSelection1D, self).build(input_shape)\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # don't pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        if not isinstance(inputs, list) or len(inputs) != 2:\n",
    "            raise ValueError('FeatureSelection1D layer should be called '\n",
    "                             'on a list of 2 inputs.')\n",
    "\n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            a = K.cast(mask, K.floatx()) * inputs[0]\n",
    "        else:\n",
    "            a = inputs[0]\n",
    "\n",
    "        b = inputs[1]\n",
    "\n",
    "        a = tf.batch_gather(\n",
    "            a, b\n",
    "        )\n",
    "\n",
    "        return a\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        if not isinstance(input_shape, list) or len(input_shape) != 2:\n",
    "            raise ValueError('A `FeatureSelection1D` layer should be called '\n",
    "                             'on a list of 2 inputs.')\n",
    "        shape1 = list(input_shape[0])\n",
    "        shape2 = list(input_shape[1])\n",
    "\n",
    "        if shape2[0] != shape1[0]:\n",
    "            raise ValueError(\"batch size must be same\")\n",
    "\n",
    "        if shape2[1] != self.num_selects:\n",
    "            raise ValueError(\"must conform to the num_select\")\n",
    "\n",
    "        return (shape1[0], self.num_selects, shape1[2])\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'num_selects': self.num_selects\n",
    "        }\n",
    "        base_config = super(FeatureSelection1D, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "bf87bf600dac037777be8e3dda560ec65b6640a2"
   },
   "source": [
    "# Build and Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_uuid": "03e8297383ee57a4a23c707a4a6c1cca13da1868"
   },
   "outputs": [],
   "source": [
    "from keras import callbacks as kc\n",
    "from keras import optimizers as ko\n",
    "from keras import initializers, regularizers, constraints\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import SVG\n",
    "\n",
    "histories = list()\n",
    "file_paths = list()\n",
    "cos = list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5503bd66a2a7fea5bcb093c775d69baded3e759d"
   },
   "source": [
    "## End-to-End RNN Attention Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5a954b42159bc4a8a17b4e15083f026f53362a43"
   },
   "source": [
    "### Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_uuid": "8feec4878e5b817dcbf5ed7f49d9b78cce36ebd6"
   },
   "outputs": [],
   "source": [
    "def build_e2e_birnn_attention_model(\n",
    "        voca_dim, time_steps, pos_tag_size, pos_tag_dim, extra_feature_dims,\n",
    "        output_dim, rnn_dim, model_dim, mlp_dim,\n",
    "        item_embedding=None, rnn_depth=1, mlp_depth=1,\n",
    "        drop_out=0.5, rnn_drop_out=0., rnn_state_drop_out=0.,\n",
    "        trainable_embedding=False, gpu=False, return_customized_layers=False):\n",
    "    \"\"\"\n",
    "    Create A End-to-End Bidirectional RNN Attention Model.\n",
    "\n",
    "    :param voca_dim: vocabulary dimension size.\n",
    "    :param time_steps: the length of input\n",
    "    :param extra_feature_dims: the dimention size of the auxilary feature\n",
    "    :param output_dim: the output dimension size\n",
    "    :param model_dim: rrn dimension size\n",
    "    :param mlp_dim: the dimension size of fully connected layer\n",
    "    :param item_embedding: integer, numpy 2D array, or None (default=None)\n",
    "        If item_embedding is a integer, connect a randomly initialized embedding matrix to the input tensor.\n",
    "        If item_embedding is a matrix, this matrix will be used as the embedding matrix.\n",
    "        If item_embedding is None, then connect input tensor to RNN layer directly.\n",
    "    :param rnn_depth: rnn depth\n",
    "    :param mlp_depth: the depth of fully connected layers\n",
    "    :param drop_out: dropout rate of fully connected layers\n",
    "    :param rnn_drop_out: dropout rate of rnn layers\n",
    "    :param rnn_state_drop_out: dropout rate of rnn state tensor\n",
    "    :param trainable_embedding: boolean\n",
    "    :param gpu: boolean, default=False\n",
    "        If True, CuDNNLSTM is used instead of LSTM for RNN layer.\n",
    "    :param return_customized_layers: boolean, default=False\n",
    "        If True, return model and customized object dictionary, otherwise return model only\n",
    "    :return: keras model\n",
    "    \"\"\"\n",
    "    \n",
    "    # sequences inputs\n",
    "    if item_embedding is not None:\n",
    "        inputp = models.Input(shape=(time_steps,), dtype='int32', name='inputp')\n",
    "        inputa = models.Input(shape=(time_steps,), dtype='int32', name='inputa')\n",
    "        inputb = models.Input(shape=(time_steps,), dtype='int32', name='inputb')\n",
    "        inputs = [inputp, inputa, inputb]\n",
    "        \n",
    "        if isinstance(item_embedding, np.ndarray):\n",
    "            assert voca_dim == item_embedding.shape[0]\n",
    "            embed_dim = item_embedding.shape[1]\n",
    "            emb_layer = layers.Embedding(\n",
    "                voca_dim, item_embedding.shape[1], input_length=time_steps,\n",
    "                weights=[item_embedding, ], trainable=trainable_embedding,\n",
    "                mask_zero=False, name='embedding_layer0'\n",
    "            )\n",
    "        elif utils.is_integer(item_embedding):\n",
    "            embed_dim = item_embedding\n",
    "            emb_layer = layers.Embedding(\n",
    "                voca_dim, item_embedding, input_length=time_steps,\n",
    "                trainable=trainable_embedding,\n",
    "                mask_zero=False, name='embedding_layer0'\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\"item_embedding must be either integer or numpy matrix\")\n",
    "\n",
    "        xs = list(map(\n",
    "            lambda input_: emb_layer(input_),\n",
    "            inputs\n",
    "        ))\n",
    "    else:\n",
    "        inputp = models.Input(shape=(time_steps, voca_dim), dtype='float32', name='inputp')\n",
    "        inputa = models.Input(shape=(time_steps, voca_dim), dtype='float32', name='inputa')\n",
    "        inputb = models.Input(shape=(time_steps, voca_dim), dtype='float32', name='inputb')\n",
    "        embed_dim = voca_dim\n",
    "        xs = [inputp, inputa, inputb]\n",
    "        \n",
    "    # pos tag\n",
    "    inputposp = models.Input(shape=(time_steps,), dtype='int32', name='inputposp')\n",
    "    inputposa = models.Input(shape=(time_steps,), dtype='int32', name='inputposa')\n",
    "    inputposb = models.Input(shape=(time_steps,), dtype='int32', name='inputposb')\n",
    "    inputpos = [inputposp, inputposa, inputposb]\n",
    "    pos_emb_layer = layers.Embedding(\n",
    "        pos_tag_size, pos_tag_dim, input_length=time_steps,\n",
    "        trainable=True, mask_zero=False, name='pos_embedding_layer0'\n",
    "    )\n",
    "    xpos = list(map(\n",
    "        lambda input_: pos_emb_layer(input_),\n",
    "        inputpos\n",
    "    ))\n",
    "    \n",
    "    embed_concate_layer = layers.Concatenate(axis=2, name=\"embed_concate_layer\")\n",
    "    for i in range(len(xs)):\n",
    "        xs[i] = embed_concate_layer([xs[i], xpos[i]])\n",
    "    \n",
    "    # mention position in the sentence\n",
    "    inputpi = models.Input(shape=(1,), dtype='int32', name='inputpi')\n",
    "    inputai = models.Input(shape=(1,), dtype='int32', name='inputai')\n",
    "    inputbi = models.Input(shape=(1,), dtype='int32', name='inputbi')\n",
    "    xis = [inputpi, inputai, inputbi]\n",
    "    \n",
    "    # addtional mention-pair features\n",
    "    inputpa = models.Input(shape=(extra_feature_dims,), dtype='float32', name='inputpa')\n",
    "    inputpb = models.Input(shape=(extra_feature_dims,), dtype='float32', name='inputpb')\n",
    "    xextrs = [inputpa, inputpb]\n",
    "    \n",
    "    # rnn\n",
    "    birnns = list()\n",
    "    rnn_batchnorms = list()\n",
    "    rnn_dropouts = list()\n",
    "    if gpu:\n",
    "        # rnn encoding\n",
    "        for i in range(rnn_depth):\n",
    "            rnn_dropout = layers.SpatialDropout1D(rnn_drop_out)\n",
    "            birnn = layers.Bidirectional(\n",
    "                layers.CuDNNGRU(rnn_dim, return_sequences=True),\n",
    "                name='bi_lstm_layer' + str(i))\n",
    "            rnn_batchnorm = layers.BatchNormalization(name='rnn_batch_norm_layer' + str(i))\n",
    "            \n",
    "            birnns.append(birnn)\n",
    "            rnn_dropouts.append(rnn_dropout)\n",
    "            rnn_batchnorms.append(rnn_batchnorm)\n",
    "        \n",
    "        xs_ = list()\n",
    "        for x_ in xs:\n",
    "            for i in range(len(birnns)):\n",
    "                x_ = rnn_dropouts[i](x_)\n",
    "                x_ = birnns[i](x_)\n",
    "                x_ = rnn_batchnorms[i](x_)\n",
    "            xs_.append(x_)\n",
    "        xs = xs_\n",
    "    else:\n",
    "        # rnn encoding\n",
    "        for i in range(rnn_depth):\n",
    "            birnn = layers.Bidirectional(\n",
    "                layers.GRU(rnn_dim, return_sequences=True, dropout=rnn_drop_out,\n",
    "                            recurrent_dropout=rnn_state_drop_out),\n",
    "                name='bi_lstm_layer' + str(i))\n",
    "            rnn_batchnorm = layers.BatchNormalization(name='rnn_batch_norm_layer' + str(i))\n",
    "            \n",
    "            birnns.append(birnn)\n",
    "            rnn_batchnorms.append(rnn_batchnorm)\n",
    "            \n",
    "        xs_ = list()\n",
    "        for x_ in xs:\n",
    "            for i in range(len(birnns)):\n",
    "                x_ = birnns[i](x_)\n",
    "                x_ = rnn_batchnorms[i](x_)\n",
    "            xs_.append(x_)\n",
    "        xs = xs_\n",
    "    \n",
    "    # attention aggregated rnn embedding + mention rnn embedding + mention-pair features\n",
    "    select_layer = FeatureSelection1D(1, name='boundary_selection_layer')\n",
    "    flatten_layer1 = layers.Flatten('channels_first', name=\"flatten_layer1\")\n",
    "    permute_layer = layers.Permute((2, 1), name='permuted_attention_x')\n",
    "    attent_weight = AttentionWeight(name=\"attention_weight\")\n",
    "    focus_layer = layers.Dot([2, 1], name='focus' + '_layer')\n",
    "    reshape_layer = layers.Reshape((1, rnn_dim*2), name=\"reshape_layer\")\n",
    "    concate_layer = layers.Concatenate(axis=1, name=\"attention_concate_layer\")\n",
    "    atten_dropout_layer = layers.Dropout(drop_out, name='attention_dropout_layer')\n",
    "    map_layer1 = layers.Dense(model_dim, activation=\"relu\", name=\"map_layer1\")\n",
    "    #map_layer2 = layers.TimeDistributed(layers.Dense(model_dim, activation=\"relu\"), name=\"map_layer2\")\n",
    "    map_layer2 = map_layer1\n",
    "    flatten_layer = layers.Flatten('channels_first', name=\"flatten_layer\")\n",
    "    for i in range(len(xs)):\n",
    "        if i == 0:\n",
    "            map_layer = map_layer1\n",
    "        else:\n",
    "            map_layer = map_layer2\n",
    "            \n",
    "        select_ = select_layer([xs[i], xis[i]])\n",
    "        flatten_select_ = flatten_layer1(select_)\n",
    "        att = attent_weight([xs[i], flatten_select_])\n",
    "        \n",
    "        focus = focus_layer([permute_layer(xs[i]), att])\n",
    "        xs[i] = concate_layer([select_, reshape_layer(focus)])\n",
    "        xs[i] = flatten_layer(xs[i])\n",
    "        xs[i] = atten_dropout_layer(xs[i])\n",
    "        xs[i] = map_layer(xs[i])\n",
    "    \n",
    "    feature_dropout_layer = layers.Dropout(rate=drop_out, name=\"feature_dropout_layer\")\n",
    "    feature_map_layer = layers.Dense(model_dim, activation=\"relu\",name=\"feature_map_layer\")\n",
    "    xextrs = [feature_map_layer(feature_dropout_layer(xextr)) for xextr in xextrs]\n",
    "    \n",
    "    x = layers.Concatenate(axis=1, name=\"concat_feature_layer\")(xs + xextrs)\n",
    "    x = layers.Dropout(drop_out, name='dropout_layer')(x)\n",
    "\n",
    "    # MLP Layers\n",
    "    for i in range(mlp_depth - 1):\n",
    "        x = layers.Dense(mlp_dim, activation='selu', kernel_initializer='lecun_normal', name='selu_layer' + str(i))(x)\n",
    "        x = layers.AlphaDropout(drop_out, name='alpha_layer' + str(i))(x)\n",
    "\n",
    "    outputs = layers.Dense(output_dim, activation=\"softmax\", name=\"softmax_layer0\")(x)\n",
    "\n",
    "    model = models.Model([inputp, inputa, inputb] + inputpos + xis + [inputpa, inputpb], outputs)\n",
    "\n",
    "    if return_customized_layers:\n",
    "        return model, {'FeatureSelection1D': FeatureSelection1D, 'AttentionWeight': AttentionWeight}\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f134f020b4c1c7f9e8ce4cfc801b015d9090e249"
   },
   "source": [
    "### Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "_uuid": "7c0e4a38b4fa5e55811bdb4672dc5a596e70c433"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Isak\\.conda\\envs\\gender\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\Isak\\.conda\\envs\\gender\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "voca_dim = embedding_matrix.shape[0]\n",
    "pos_tag_size = len(pos_index) + 1 # my edit\n",
    "time_steps = max_len\n",
    "\n",
    "embed_dim = embedding_matrix.shape[1]\n",
    "pos_tag_dim = 5\n",
    "extra_feature_dims = num_pos_features\n",
    "output_dim = 3\n",
    "rnn_dim = 50\n",
    "model_dim = 10\n",
    "mlp_dim = 10\n",
    "rnn_depth = 1\n",
    "mlp_depth=1\n",
    "drop_out=0.2\n",
    "rnn_drop_out=0.5\n",
    "gpu = False\n",
    "return_customized_layers=True\n",
    "\n",
    "model, co = build_e2e_birnn_attention_model(\n",
    "        voca_dim, time_steps, pos_tag_size, pos_tag_dim, extra_feature_dims, output_dim, rnn_dim, model_dim, mlp_dim,\n",
    "        item_embedding=embedding_matrix, rnn_depth=rnn_depth, mlp_depth=mlp_depth,\n",
    "        drop_out=drop_out, rnn_drop_out=rnn_drop_out, rnn_state_drop_out=rnn_drop_out,\n",
    "        trainable_embedding=False, gpu=gpu, return_customized_layers=return_customized_layers)\n",
    "cos.append(co)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from keras.utils import plot_model\n",
    "plot_model(model, to_file='model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "_uuid": "f56526943859c2cd1c3738e043059a4951dd92f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "inputp (InputLayer)             (None, 50)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "inputposp (InputLayer)          (None, 50)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "inputa (InputLayer)             (None, 50)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "inputposa (InputLayer)          (None, 50)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "inputb (InputLayer)             (None, 50)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "inputposb (InputLayer)          (None, 50)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_layer0 (Embedding)    (None, 50, 300)      6684900     inputp[0][0]                     \n",
      "                                                                 inputa[0][0]                     \n",
      "                                                                 inputb[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "pos_embedding_layer0 (Embedding (None, 50, 5)        90          inputposp[0][0]                  \n",
      "                                                                 inputposa[0][0]                  \n",
      "                                                                 inputposb[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "embed_concate_layer (Concatenat (None, 50, 305)      0           embedding_layer0[0][0]           \n",
      "                                                                 pos_embedding_layer0[0][0]       \n",
      "                                                                 embedding_layer0[1][0]           \n",
      "                                                                 pos_embedding_layer0[1][0]       \n",
      "                                                                 embedding_layer0[2][0]           \n",
      "                                                                 pos_embedding_layer0[2][0]       \n",
      "__________________________________________________________________________________________________\n",
      "bi_lstm_layer0 (Bidirectional)  (None, 50, 100)      106800      embed_concate_layer[0][0]        \n",
      "                                                                 embed_concate_layer[1][0]        \n",
      "                                                                 embed_concate_layer[2][0]        \n",
      "__________________________________________________________________________________________________\n",
      "rnn_batch_norm_layer0 (BatchNor (None, 50, 100)      400         bi_lstm_layer0[0][0]             \n",
      "                                                                 bi_lstm_layer0[1][0]             \n",
      "                                                                 bi_lstm_layer0[2][0]             \n",
      "__________________________________________________________________________________________________\n",
      "inputpi (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "inputai (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "inputbi (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "boundary_selection_layer (Featu (None, 1, 100)       0           rnn_batch_norm_layer0[0][0]      \n",
      "                                                                 inputpi[0][0]                    \n",
      "                                                                 rnn_batch_norm_layer0[1][0]      \n",
      "                                                                 inputai[0][0]                    \n",
      "                                                                 rnn_batch_norm_layer0[2][0]      \n",
      "                                                                 inputbi[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_layer1 (Flatten)        (None, 100)          0           boundary_selection_layer[0][0]   \n",
      "                                                                 boundary_selection_layer[1][0]   \n",
      "                                                                 boundary_selection_layer[2][0]   \n",
      "__________________________________________________________________________________________________\n",
      "permuted_attention_x (Permute)  (None, 100, 50)      0           rnn_batch_norm_layer0[0][0]      \n",
      "                                                                 rnn_batch_norm_layer0[1][0]      \n",
      "                                                                 rnn_batch_norm_layer0[2][0]      \n",
      "__________________________________________________________________________________________________\n",
      "attention_weight (AttentionWeig (None, 50)           10100       rnn_batch_norm_layer0[0][0]      \n",
      "                                                                 flatten_layer1[0][0]             \n",
      "                                                                 rnn_batch_norm_layer0[1][0]      \n",
      "                                                                 flatten_layer1[1][0]             \n",
      "                                                                 rnn_batch_norm_layer0[2][0]      \n",
      "                                                                 flatten_layer1[2][0]             \n",
      "__________________________________________________________________________________________________\n",
      "focus_layer (Dot)               (None, 100)          0           permuted_attention_x[0][0]       \n",
      "                                                                 attention_weight[0][0]           \n",
      "                                                                 permuted_attention_x[1][0]       \n",
      "                                                                 attention_weight[1][0]           \n",
      "                                                                 permuted_attention_x[2][0]       \n",
      "                                                                 attention_weight[2][0]           \n",
      "__________________________________________________________________________________________________\n",
      "reshape_layer (Reshape)         (None, 1, 100)       0           focus_layer[0][0]                \n",
      "                                                                 focus_layer[1][0]                \n",
      "                                                                 focus_layer[2][0]                \n",
      "__________________________________________________________________________________________________\n",
      "attention_concate_layer (Concat (None, 2, 100)       0           boundary_selection_layer[0][0]   \n",
      "                                                                 reshape_layer[0][0]              \n",
      "                                                                 boundary_selection_layer[1][0]   \n",
      "                                                                 reshape_layer[1][0]              \n",
      "                                                                 boundary_selection_layer[2][0]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                 reshape_layer[2][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_layer (Flatten)         (None, 200)          0           attention_concate_layer[0][0]    \n",
      "                                                                 attention_concate_layer[1][0]    \n",
      "                                                                 attention_concate_layer[2][0]    \n",
      "__________________________________________________________________________________________________\n",
      "inputpa (InputLayer)            (None, 45)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "inputpb (InputLayer)            (None, 45)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "attention_dropout_layer (Dropou (None, 200)          0           flatten_layer[0][0]              \n",
      "                                                                 flatten_layer[1][0]              \n",
      "                                                                 flatten_layer[2][0]              \n",
      "__________________________________________________________________________________________________\n",
      "feature_dropout_layer (Dropout) (None, 45)           0           inputpa[0][0]                    \n",
      "                                                                 inputpb[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "map_layer1 (Dense)              (None, 10)           2010        attention_dropout_layer[0][0]    \n",
      "                                                                 attention_dropout_layer[1][0]    \n",
      "                                                                 attention_dropout_layer[2][0]    \n",
      "__________________________________________________________________________________________________\n",
      "feature_map_layer (Dense)       (None, 10)           460         feature_dropout_layer[0][0]      \n",
      "                                                                 feature_dropout_layer[1][0]      \n",
      "__________________________________________________________________________________________________\n",
      "concat_feature_layer (Concatena (None, 50)           0           map_layer1[0][0]                 \n",
      "                                                                 map_layer1[1][0]                 \n",
      "                                                                 map_layer1[2][0]                 \n",
      "                                                                 feature_map_layer[0][0]          \n",
      "                                                                 feature_map_layer[1][0]          \n",
      "__________________________________________________________________________________________________\n",
      "dropout_layer (Dropout)         (None, 50)           0           concat_feature_layer[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "softmax_layer0 (Dense)          (None, 3)            153         dropout_layer[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 6,804,913\n",
      "Trainable params: 119,813\n",
      "Non-trainable params: 6,685,100\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2c926e888dce8ff928430dc76d4771cb67aea2a6"
   },
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.13.1'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow\n",
    "tensorflow.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "_uuid": "02490fd68b2c304432f68d8c356b3013084d8d3d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Isak\\.conda\\envs\\gender\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Isak\\.conda\\envs\\gender\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:110: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\Users\\Isak\\.conda\\envs\\gender\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:110: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\Users\\Isak\\.conda\\envs\\gender\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:110: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2000 samples, validate on 454 samples\n",
      "Epoch 1/40\n",
      "2000/2000 [==============================] - ETA: 9:38 - loss: 0.8733 - sparse_categorical_accuracy: 0.633 - ETA: 4:51 - loss: 1.3047 - sparse_categorical_accuracy: 0.500 - ETA: 3:15 - loss: 1.2715 - sparse_categorical_accuracy: 0.511 - ETA: 2:26 - loss: 1.3232 - sparse_categorical_accuracy: 0.483 - ETA: 1:57 - loss: 1.2861 - sparse_categorical_accuracy: 0.480 - ETA: 1:38 - loss: 1.2351 - sparse_categorical_accuracy: 0.494 - ETA: 1:24 - loss: 1.2707 - sparse_categorical_accuracy: 0.485 - ETA: 1:14 - loss: 1.2728 - sparse_categorical_accuracy: 0.487 - ETA: 1:06 - loss: 1.2355 - sparse_categorical_accuracy: 0.500 - ETA: 59s - loss: 1.2576 - sparse_categorical_accuracy: 0.483 - ETA: 54s - loss: 1.2755 - sparse_categorical_accuracy: 0.47 - ETA: 50s - loss: 1.2769 - sparse_categorical_accuracy: 0.46 - ETA: 46s - loss: 1.2715 - sparse_categorical_accuracy: 0.46 - ETA: 42s - loss: 1.2604 - sparse_categorical_accuracy: 0.46 - ETA: 39s - loss: 1.2453 - sparse_categorical_accuracy: 0.46 - ETA: 37s - loss: 1.2372 - sparse_categorical_accuracy: 0.46 - ETA: 34s - loss: 1.2255 - sparse_categorical_accuracy: 0.47 - ETA: 32s - loss: 1.2238 - sparse_categorical_accuracy: 0.46 - ETA: 30s - loss: 1.2282 - sparse_categorical_accuracy: 0.47 - ETA: 29s - loss: 1.2325 - sparse_categorical_accuracy: 0.46 - ETA: 27s - loss: 1.2473 - sparse_categorical_accuracy: 0.46 - ETA: 26s - loss: 1.2385 - sparse_categorical_accuracy: 0.47 - ETA: 24s - loss: 1.2327 - sparse_categorical_accuracy: 0.47 - ETA: 23s - loss: 1.2367 - sparse_categorical_accuracy: 0.47 - ETA: 22s - loss: 1.2441 - sparse_categorical_accuracy: 0.47 - ETA: 21s - loss: 1.2399 - sparse_categorical_accuracy: 0.47 - ETA: 20s - loss: 1.2281 - sparse_categorical_accuracy: 0.47 - ETA: 19s - loss: 1.2199 - sparse_categorical_accuracy: 0.47 - ETA: 18s - loss: 1.2103 - sparse_categorical_accuracy: 0.47 - ETA: 17s - loss: 1.2048 - sparse_categorical_accuracy: 0.47 - ETA: 16s - loss: 1.2082 - sparse_categorical_accuracy: 0.47 - ETA: 16s - loss: 1.1990 - sparse_categorical_accuracy: 0.47 - ETA: 15s - loss: 1.1935 - sparse_categorical_accuracy: 0.47 - ETA: 14s - loss: 1.1929 - sparse_categorical_accuracy: 0.46 - ETA: 13s - loss: 1.1894 - sparse_categorical_accuracy: 0.46 - ETA: 13s - loss: 1.1823 - sparse_categorical_accuracy: 0.46 - ETA: 12s - loss: 1.1787 - sparse_categorical_accuracy: 0.46 - ETA: 12s - loss: 1.1766 - sparse_categorical_accuracy: 0.46 - ETA: 11s - loss: 1.1676 - sparse_categorical_accuracy: 0.47 - ETA: 10s - loss: 1.1664 - sparse_categorical_accuracy: 0.46 - ETA: 10s - loss: 1.1651 - sparse_categorical_accuracy: 0.47 - ETA: 9s - loss: 1.1684 - sparse_categorical_accuracy: 0.4659 - ETA: 9s - loss: 1.1668 - sparse_categorical_accuracy: 0.465 - ETA: 8s - loss: 1.1617 - sparse_categorical_accuracy: 0.466 - ETA: 8s - loss: 1.1635 - sparse_categorical_accuracy: 0.465 - ETA: 7s - loss: 1.1546 - sparse_categorical_accuracy: 0.471 - ETA: 7s - loss: 1.1563 - sparse_categorical_accuracy: 0.471 - ETA: 6s - loss: 1.1514 - sparse_categorical_accuracy: 0.472 - ETA: 6s - loss: 1.1512 - sparse_categorical_accuracy: 0.470 - ETA: 6s - loss: 1.1479 - sparse_categorical_accuracy: 0.469 - ETA: 5s - loss: 1.1472 - sparse_categorical_accuracy: 0.469 - ETA: 5s - loss: 1.1458 - sparse_categorical_accuracy: 0.470 - ETA: 4s - loss: 1.1455 - sparse_categorical_accuracy: 0.470 - ETA: 4s - loss: 1.1466 - sparse_categorical_accuracy: 0.469 - ETA: 4s - loss: 1.1411 - sparse_categorical_accuracy: 0.472 - ETA: 3s - loss: 1.1391 - sparse_categorical_accuracy: 0.472 - ETA: 3s - loss: 1.1346 - sparse_categorical_accuracy: 0.474 - ETA: 3s - loss: 1.1345 - sparse_categorical_accuracy: 0.475 - ETA: 2s - loss: 1.1286 - sparse_categorical_accuracy: 0.479 - ETA: 2s - loss: 1.1247 - sparse_categorical_accuracy: 0.481 - ETA: 1s - loss: 1.1210 - sparse_categorical_accuracy: 0.482 - ETA: 1s - loss: 1.1186 - sparse_categorical_accuracy: 0.484 - ETA: 1s - loss: 1.1152 - sparse_categorical_accuracy: 0.486 - ETA: 0s - loss: 1.1129 - sparse_categorical_accuracy: 0.488 - ETA: 0s - loss: 1.1084 - sparse_categorical_accuracy: 0.488 - ETA: 0s - loss: 1.1051 - sparse_categorical_accuracy: 0.487 - 25s 12ms/step - loss: 1.1044 - sparse_categorical_accuracy: 0.4890 - val_loss: 0.9262 - val_sparse_categorical_accuracy: 0.5507\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.92623, saving model to best_e2e_rnn_atten_model.hdf5\n",
      "Epoch 2/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - ETA: 14s - loss: 0.9910 - sparse_categorical_accuracy: 0.66 - ETA: 14s - loss: 1.0022 - sparse_categorical_accuracy: 0.66 - ETA: 13s - loss: 1.0177 - sparse_categorical_accuracy: 0.61 - ETA: 13s - loss: 0.9544 - sparse_categorical_accuracy: 0.61 - ETA: 13s - loss: 1.0182 - sparse_categorical_accuracy: 0.57 - ETA: 12s - loss: 0.9997 - sparse_categorical_accuracy: 0.58 - ETA: 12s - loss: 0.9937 - sparse_categorical_accuracy: 0.59 - ETA: 12s - loss: 0.9724 - sparse_categorical_accuracy: 0.60 - ETA: 12s - loss: 0.9583 - sparse_categorical_accuracy: 0.60 - ETA: 12s - loss: 0.9643 - sparse_categorical_accuracy: 0.59 - ETA: 11s - loss: 0.9407 - sparse_categorical_accuracy: 0.60 - ETA: 11s - loss: 0.9543 - sparse_categorical_accuracy: 0.58 - ETA: 11s - loss: 0.9525 - sparse_categorical_accuracy: 0.58 - ETA: 11s - loss: 0.9610 - sparse_categorical_accuracy: 0.57 - ETA: 11s - loss: 0.9591 - sparse_categorical_accuracy: 0.57 - ETA: 10s - loss: 0.9509 - sparse_categorical_accuracy: 0.57 - ETA: 10s - loss: 0.9462 - sparse_categorical_accuracy: 0.57 - ETA: 10s - loss: 0.9358 - sparse_categorical_accuracy: 0.57 - ETA: 10s - loss: 0.9377 - sparse_categorical_accuracy: 0.56 - ETA: 9s - loss: 0.9391 - sparse_categorical_accuracy: 0.5650 - ETA: 9s - loss: 0.9384 - sparse_categorical_accuracy: 0.565 - ETA: 9s - loss: 0.9431 - sparse_categorical_accuracy: 0.566 - ETA: 9s - loss: 0.9400 - sparse_categorical_accuracy: 0.566 - ETA: 9s - loss: 0.9374 - sparse_categorical_accuracy: 0.566 - ETA: 8s - loss: 0.9345 - sparse_categorical_accuracy: 0.568 - ETA: 8s - loss: 0.9247 - sparse_categorical_accuracy: 0.574 - ETA: 8s - loss: 0.9301 - sparse_categorical_accuracy: 0.575 - ETA: 8s - loss: 0.9350 - sparse_categorical_accuracy: 0.572 - ETA: 8s - loss: 0.9309 - sparse_categorical_accuracy: 0.574 - ETA: 7s - loss: 0.9335 - sparse_categorical_accuracy: 0.574 - ETA: 7s - loss: 0.9349 - sparse_categorical_accuracy: 0.572 - ETA: 7s - loss: 0.9315 - sparse_categorical_accuracy: 0.572 - ETA: 7s - loss: 0.9272 - sparse_categorical_accuracy: 0.576 - ETA: 7s - loss: 0.9254 - sparse_categorical_accuracy: 0.576 - ETA: 6s - loss: 0.9244 - sparse_categorical_accuracy: 0.576 - ETA: 6s - loss: 0.9233 - sparse_categorical_accuracy: 0.577 - ETA: 6s - loss: 0.9226 - sparse_categorical_accuracy: 0.573 - ETA: 6s - loss: 0.9196 - sparse_categorical_accuracy: 0.576 - ETA: 5s - loss: 0.9160 - sparse_categorical_accuracy: 0.578 - ETA: 5s - loss: 0.9157 - sparse_categorical_accuracy: 0.578 - ETA: 5s - loss: 0.9152 - sparse_categorical_accuracy: 0.577 - ETA: 5s - loss: 0.9127 - sparse_categorical_accuracy: 0.577 - ETA: 5s - loss: 0.9102 - sparse_categorical_accuracy: 0.579 - ETA: 4s - loss: 0.9109 - sparse_categorical_accuracy: 0.576 - ETA: 4s - loss: 0.9143 - sparse_categorical_accuracy: 0.576 - ETA: 4s - loss: 0.9124 - sparse_categorical_accuracy: 0.577 - ETA: 4s - loss: 0.9097 - sparse_categorical_accuracy: 0.578 - ETA: 4s - loss: 0.9081 - sparse_categorical_accuracy: 0.579 - ETA: 3s - loss: 0.9051 - sparse_categorical_accuracy: 0.581 - ETA: 3s - loss: 0.9061 - sparse_categorical_accuracy: 0.582 - ETA: 3s - loss: 0.9038 - sparse_categorical_accuracy: 0.581 - ETA: 3s - loss: 0.9043 - sparse_categorical_accuracy: 0.582 - ETA: 2s - loss: 0.9071 - sparse_categorical_accuracy: 0.581 - ETA: 2s - loss: 0.9078 - sparse_categorical_accuracy: 0.583 - ETA: 2s - loss: 0.9052 - sparse_categorical_accuracy: 0.584 - ETA: 2s - loss: 0.9034 - sparse_categorical_accuracy: 0.583 - ETA: 2s - loss: 0.8994 - sparse_categorical_accuracy: 0.586 - ETA: 1s - loss: 0.8972 - sparse_categorical_accuracy: 0.586 - ETA: 1s - loss: 0.8985 - sparse_categorical_accuracy: 0.586 - ETA: 1s - loss: 0.8982 - sparse_categorical_accuracy: 0.587 - ETA: 1s - loss: 0.8972 - sparse_categorical_accuracy: 0.589 - ETA: 1s - loss: 0.8965 - sparse_categorical_accuracy: 0.588 - ETA: 0s - loss: 0.8927 - sparse_categorical_accuracy: 0.592 - ETA: 0s - loss: 0.8938 - sparse_categorical_accuracy: 0.592 - ETA: 0s - loss: 0.8896 - sparse_categorical_accuracy: 0.594 - ETA: 0s - loss: 0.8859 - sparse_categorical_accuracy: 0.597 - 16s 8ms/step - loss: 0.8844 - sparse_categorical_accuracy: 0.5975 - val_loss: 0.8058 - val_sparse_categorical_accuracy: 0.6123\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.92623 to 0.80580, saving model to best_e2e_rnn_atten_model.hdf5\n",
      "Epoch 3/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - ETA: 15s - loss: 0.6936 - sparse_categorical_accuracy: 0.80 - ETA: 16s - loss: 0.8063 - sparse_categorical_accuracy: 0.70 - ETA: 15s - loss: 0.8372 - sparse_categorical_accuracy: 0.64 - ETA: 15s - loss: 0.8424 - sparse_categorical_accuracy: 0.62 - ETA: 15s - loss: 0.8649 - sparse_categorical_accuracy: 0.60 - ETA: 14s - loss: 0.8473 - sparse_categorical_accuracy: 0.62 - ETA: 14s - loss: 0.8286 - sparse_categorical_accuracy: 0.62 - ETA: 14s - loss: 0.8025 - sparse_categorical_accuracy: 0.63 - ETA: 14s - loss: 0.7713 - sparse_categorical_accuracy: 0.64 - ETA: 14s - loss: 0.7749 - sparse_categorical_accuracy: 0.64 - ETA: 13s - loss: 0.7697 - sparse_categorical_accuracy: 0.65 - ETA: 13s - loss: 0.7702 - sparse_categorical_accuracy: 0.65 - ETA: 13s - loss: 0.7757 - sparse_categorical_accuracy: 0.64 - ETA: 12s - loss: 0.7666 - sparse_categorical_accuracy: 0.65 - ETA: 12s - loss: 0.7532 - sparse_categorical_accuracy: 0.65 - ETA: 12s - loss: 0.7481 - sparse_categorical_accuracy: 0.66 - ETA: 11s - loss: 0.7558 - sparse_categorical_accuracy: 0.65 - ETA: 11s - loss: 0.7646 - sparse_categorical_accuracy: 0.64 - ETA: 11s - loss: 0.7627 - sparse_categorical_accuracy: 0.65 - ETA: 11s - loss: 0.7649 - sparse_categorical_accuracy: 0.65 - ETA: 10s - loss: 0.7753 - sparse_categorical_accuracy: 0.64 - ETA: 10s - loss: 0.7707 - sparse_categorical_accuracy: 0.65 - ETA: 10s - loss: 0.7755 - sparse_categorical_accuracy: 0.64 - ETA: 10s - loss: 0.7871 - sparse_categorical_accuracy: 0.64 - ETA: 9s - loss: 0.7961 - sparse_categorical_accuracy: 0.6360 - ETA: 9s - loss: 0.8110 - sparse_categorical_accuracy: 0.633 - ETA: 9s - loss: 0.8034 - sparse_categorical_accuracy: 0.633 - ETA: 9s - loss: 0.8037 - sparse_categorical_accuracy: 0.629 - ETA: 8s - loss: 0.8055 - sparse_categorical_accuracy: 0.626 - ETA: 8s - loss: 0.8042 - sparse_categorical_accuracy: 0.624 - ETA: 8s - loss: 0.8063 - sparse_categorical_accuracy: 0.625 - ETA: 8s - loss: 0.8083 - sparse_categorical_accuracy: 0.625 - ETA: 7s - loss: 0.8057 - sparse_categorical_accuracy: 0.628 - ETA: 7s - loss: 0.8069 - sparse_categorical_accuracy: 0.628 - ETA: 7s - loss: 0.8098 - sparse_categorical_accuracy: 0.628 - ETA: 7s - loss: 0.8070 - sparse_categorical_accuracy: 0.627 - ETA: 6s - loss: 0.8092 - sparse_categorical_accuracy: 0.627 - ETA: 6s - loss: 0.8150 - sparse_categorical_accuracy: 0.628 - ETA: 6s - loss: 0.8139 - sparse_categorical_accuracy: 0.629 - ETA: 6s - loss: 0.8076 - sparse_categorical_accuracy: 0.635 - ETA: 5s - loss: 0.8031 - sparse_categorical_accuracy: 0.637 - ETA: 5s - loss: 0.7979 - sparse_categorical_accuracy: 0.639 - ETA: 5s - loss: 0.8011 - sparse_categorical_accuracy: 0.639 - ETA: 5s - loss: 0.8071 - sparse_categorical_accuracy: 0.639 - ETA: 5s - loss: 0.8093 - sparse_categorical_accuracy: 0.638 - ETA: 4s - loss: 0.8043 - sparse_categorical_accuracy: 0.642 - ETA: 4s - loss: 0.8057 - sparse_categorical_accuracy: 0.641 - ETA: 4s - loss: 0.8088 - sparse_categorical_accuracy: 0.639 - ETA: 4s - loss: 0.8045 - sparse_categorical_accuracy: 0.642 - ETA: 3s - loss: 0.8066 - sparse_categorical_accuracy: 0.640 - ETA: 3s - loss: 0.8063 - sparse_categorical_accuracy: 0.640 - ETA: 3s - loss: 0.8089 - sparse_categorical_accuracy: 0.638 - ETA: 3s - loss: 0.8123 - sparse_categorical_accuracy: 0.636 - ETA: 2s - loss: 0.8143 - sparse_categorical_accuracy: 0.635 - ETA: 2s - loss: 0.8148 - sparse_categorical_accuracy: 0.635 - ETA: 2s - loss: 0.8139 - sparse_categorical_accuracy: 0.635 - ETA: 2s - loss: 0.8173 - sparse_categorical_accuracy: 0.635 - ETA: 1s - loss: 0.8173 - sparse_categorical_accuracy: 0.635 - ETA: 1s - loss: 0.8207 - sparse_categorical_accuracy: 0.632 - ETA: 1s - loss: 0.8205 - sparse_categorical_accuracy: 0.631 - ETA: 1s - loss: 0.8184 - sparse_categorical_accuracy: 0.632 - ETA: 1s - loss: 0.8197 - sparse_categorical_accuracy: 0.632 - ETA: 0s - loss: 0.8161 - sparse_categorical_accuracy: 0.634 - ETA: 0s - loss: 0.8159 - sparse_categorical_accuracy: 0.635 - ETA: 0s - loss: 0.8135 - sparse_categorical_accuracy: 0.639 - ETA: 0s - loss: 0.8100 - sparse_categorical_accuracy: 0.640 - 16s 8ms/step - loss: 0.8157 - sparse_categorical_accuracy: 0.6395 - val_loss: 0.7824 - val_sparse_categorical_accuracy: 0.6454\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.80580 to 0.78238, saving model to best_e2e_rnn_atten_model.hdf5\n",
      "Epoch 4/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - ETA: 12s - loss: 0.6458 - sparse_categorical_accuracy: 0.70 - ETA: 12s - loss: 0.6824 - sparse_categorical_accuracy: 0.70 - ETA: 12s - loss: 0.7338 - sparse_categorical_accuracy: 0.68 - ETA: 11s - loss: 0.7048 - sparse_categorical_accuracy: 0.69 - ETA: 11s - loss: 0.7331 - sparse_categorical_accuracy: 0.68 - ETA: 11s - loss: 0.7363 - sparse_categorical_accuracy: 0.68 - ETA: 11s - loss: 0.7528 - sparse_categorical_accuracy: 0.66 - ETA: 11s - loss: 0.7382 - sparse_categorical_accuracy: 0.67 - ETA: 11s - loss: 0.7320 - sparse_categorical_accuracy: 0.67 - ETA: 11s - loss: 0.7412 - sparse_categorical_accuracy: 0.67 - ETA: 11s - loss: 0.7482 - sparse_categorical_accuracy: 0.66 - ETA: 11s - loss: 0.7536 - sparse_categorical_accuracy: 0.66 - ETA: 10s - loss: 0.7575 - sparse_categorical_accuracy: 0.66 - ETA: 10s - loss: 0.7545 - sparse_categorical_accuracy: 0.66 - ETA: 10s - loss: 0.7526 - sparse_categorical_accuracy: 0.66 - ETA: 10s - loss: 0.7438 - sparse_categorical_accuracy: 0.66 - ETA: 10s - loss: 0.7580 - sparse_categorical_accuracy: 0.65 - ETA: 10s - loss: 0.7620 - sparse_categorical_accuracy: 0.65 - ETA: 9s - loss: 0.7558 - sparse_categorical_accuracy: 0.6561 - ETA: 9s - loss: 0.7585 - sparse_categorical_accuracy: 0.658 - ETA: 9s - loss: 0.7643 - sparse_categorical_accuracy: 0.658 - ETA: 9s - loss: 0.7699 - sparse_categorical_accuracy: 0.660 - ETA: 9s - loss: 0.7659 - sparse_categorical_accuracy: 0.662 - ETA: 8s - loss: 0.7590 - sparse_categorical_accuracy: 0.668 - ETA: 8s - loss: 0.7581 - sparse_categorical_accuracy: 0.670 - ETA: 8s - loss: 0.7556 - sparse_categorical_accuracy: 0.670 - ETA: 8s - loss: 0.7522 - sparse_categorical_accuracy: 0.671 - ETA: 8s - loss: 0.7570 - sparse_categorical_accuracy: 0.670 - ETA: 7s - loss: 0.7593 - sparse_categorical_accuracy: 0.670 - ETA: 7s - loss: 0.7643 - sparse_categorical_accuracy: 0.668 - ETA: 7s - loss: 0.7614 - sparse_categorical_accuracy: 0.674 - ETA: 7s - loss: 0.7635 - sparse_categorical_accuracy: 0.671 - ETA: 7s - loss: 0.7623 - sparse_categorical_accuracy: 0.671 - ETA: 6s - loss: 0.7566 - sparse_categorical_accuracy: 0.674 - ETA: 6s - loss: 0.7600 - sparse_categorical_accuracy: 0.674 - ETA: 6s - loss: 0.7549 - sparse_categorical_accuracy: 0.677 - ETA: 6s - loss: 0.7513 - sparse_categorical_accuracy: 0.681 - ETA: 6s - loss: 0.7498 - sparse_categorical_accuracy: 0.680 - ETA: 5s - loss: 0.7553 - sparse_categorical_accuracy: 0.678 - ETA: 5s - loss: 0.7557 - sparse_categorical_accuracy: 0.675 - ETA: 5s - loss: 0.7603 - sparse_categorical_accuracy: 0.670 - ETA: 5s - loss: 0.7587 - sparse_categorical_accuracy: 0.673 - ETA: 4s - loss: 0.7595 - sparse_categorical_accuracy: 0.672 - ETA: 4s - loss: 0.7648 - sparse_categorical_accuracy: 0.669 - ETA: 4s - loss: 0.7692 - sparse_categorical_accuracy: 0.668 - ETA: 4s - loss: 0.7704 - sparse_categorical_accuracy: 0.665 - ETA: 4s - loss: 0.7711 - sparse_categorical_accuracy: 0.665 - ETA: 3s - loss: 0.7751 - sparse_categorical_accuracy: 0.663 - ETA: 3s - loss: 0.7755 - sparse_categorical_accuracy: 0.663 - ETA: 3s - loss: 0.7737 - sparse_categorical_accuracy: 0.662 - ETA: 3s - loss: 0.7748 - sparse_categorical_accuracy: 0.662 - ETA: 3s - loss: 0.7753 - sparse_categorical_accuracy: 0.660 - ETA: 2s - loss: 0.7736 - sparse_categorical_accuracy: 0.663 - ETA: 2s - loss: 0.7754 - sparse_categorical_accuracy: 0.663 - ETA: 2s - loss: 0.7798 - sparse_categorical_accuracy: 0.660 - ETA: 2s - loss: 0.7775 - sparse_categorical_accuracy: 0.661 - ETA: 2s - loss: 0.7746 - sparse_categorical_accuracy: 0.662 - ETA: 1s - loss: 0.7735 - sparse_categorical_accuracy: 0.660 - ETA: 1s - loss: 0.7740 - sparse_categorical_accuracy: 0.659 - ETA: 1s - loss: 0.7729 - sparse_categorical_accuracy: 0.660 - ETA: 1s - loss: 0.7735 - sparse_categorical_accuracy: 0.659 - ETA: 0s - loss: 0.7713 - sparse_categorical_accuracy: 0.660 - ETA: 0s - loss: 0.7734 - sparse_categorical_accuracy: 0.659 - ETA: 0s - loss: 0.7710 - sparse_categorical_accuracy: 0.660 - ETA: 0s - loss: 0.7723 - sparse_categorical_accuracy: 0.662 - ETA: 0s - loss: 0.7709 - sparse_categorical_accuracy: 0.662 - 15s 7ms/step - loss: 0.7689 - sparse_categorical_accuracy: 0.6635 - val_loss: 0.7818 - val_sparse_categorical_accuracy: 0.6476\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.78238 to 0.78182, saving model to best_e2e_rnn_atten_model.hdf5\n",
      "Epoch 5/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - ETA: 13s - loss: 0.7161 - sparse_categorical_accuracy: 0.66 - ETA: 12s - loss: 0.6653 - sparse_categorical_accuracy: 0.70 - ETA: 12s - loss: 0.7833 - sparse_categorical_accuracy: 0.64 - ETA: 12s - loss: 0.7322 - sparse_categorical_accuracy: 0.66 - ETA: 12s - loss: 0.7483 - sparse_categorical_accuracy: 0.64 - ETA: 11s - loss: 0.7366 - sparse_categorical_accuracy: 0.66 - ETA: 11s - loss: 0.7517 - sparse_categorical_accuracy: 0.66 - ETA: 11s - loss: 0.7511 - sparse_categorical_accuracy: 0.66 - ETA: 11s - loss: 0.7566 - sparse_categorical_accuracy: 0.65 - ETA: 11s - loss: 0.7462 - sparse_categorical_accuracy: 0.66 - ETA: 11s - loss: 0.7371 - sparse_categorical_accuracy: 0.67 - ETA: 11s - loss: 0.7176 - sparse_categorical_accuracy: 0.68 - ETA: 11s - loss: 0.7278 - sparse_categorical_accuracy: 0.68 - ETA: 10s - loss: 0.7411 - sparse_categorical_accuracy: 0.67 - ETA: 10s - loss: 0.7420 - sparse_categorical_accuracy: 0.67 - ETA: 10s - loss: 0.7479 - sparse_categorical_accuracy: 0.67 - ETA: 10s - loss: 0.7580 - sparse_categorical_accuracy: 0.66 - ETA: 10s - loss: 0.7576 - sparse_categorical_accuracy: 0.67 - ETA: 9s - loss: 0.7658 - sparse_categorical_accuracy: 0.6649 - ETA: 9s - loss: 0.7643 - sparse_categorical_accuracy: 0.665 - ETA: 9s - loss: 0.7641 - sparse_categorical_accuracy: 0.668 - ETA: 9s - loss: 0.7577 - sparse_categorical_accuracy: 0.674 - ETA: 9s - loss: 0.7662 - sparse_categorical_accuracy: 0.669 - ETA: 8s - loss: 0.7643 - sparse_categorical_accuracy: 0.672 - ETA: 8s - loss: 0.7629 - sparse_categorical_accuracy: 0.672 - ETA: 8s - loss: 0.7628 - sparse_categorical_accuracy: 0.673 - ETA: 8s - loss: 0.7569 - sparse_categorical_accuracy: 0.675 - ETA: 8s - loss: 0.7558 - sparse_categorical_accuracy: 0.675 - ETA: 7s - loss: 0.7580 - sparse_categorical_accuracy: 0.675 - ETA: 7s - loss: 0.7563 - sparse_categorical_accuracy: 0.675 - ETA: 7s - loss: 0.7584 - sparse_categorical_accuracy: 0.673 - ETA: 7s - loss: 0.7610 - sparse_categorical_accuracy: 0.672 - ETA: 7s - loss: 0.7574 - sparse_categorical_accuracy: 0.672 - ETA: 6s - loss: 0.7600 - sparse_categorical_accuracy: 0.669 - ETA: 6s - loss: 0.7565 - sparse_categorical_accuracy: 0.673 - ETA: 6s - loss: 0.7606 - sparse_categorical_accuracy: 0.670 - ETA: 6s - loss: 0.7534 - sparse_categorical_accuracy: 0.672 - ETA: 6s - loss: 0.7564 - sparse_categorical_accuracy: 0.670 - ETA: 5s - loss: 0.7554 - sparse_categorical_accuracy: 0.668 - ETA: 5s - loss: 0.7538 - sparse_categorical_accuracy: 0.669 - ETA: 5s - loss: 0.7543 - sparse_categorical_accuracy: 0.668 - ETA: 5s - loss: 0.7546 - sparse_categorical_accuracy: 0.669 - ETA: 5s - loss: 0.7543 - sparse_categorical_accuracy: 0.669 - ETA: 4s - loss: 0.7516 - sparse_categorical_accuracy: 0.670 - ETA: 4s - loss: 0.7499 - sparse_categorical_accuracy: 0.670 - ETA: 4s - loss: 0.7482 - sparse_categorical_accuracy: 0.669 - ETA: 4s - loss: 0.7487 - sparse_categorical_accuracy: 0.669 - ETA: 3s - loss: 0.7435 - sparse_categorical_accuracy: 0.672 - ETA: 3s - loss: 0.7390 - sparse_categorical_accuracy: 0.675 - ETA: 3s - loss: 0.7371 - sparse_categorical_accuracy: 0.675 - ETA: 3s - loss: 0.7371 - sparse_categorical_accuracy: 0.674 - ETA: 3s - loss: 0.7391 - sparse_categorical_accuracy: 0.672 - ETA: 2s - loss: 0.7383 - sparse_categorical_accuracy: 0.673 - ETA: 2s - loss: 0.7409 - sparse_categorical_accuracy: 0.671 - ETA: 2s - loss: 0.7449 - sparse_categorical_accuracy: 0.670 - ETA: 2s - loss: 0.7497 - sparse_categorical_accuracy: 0.668 - ETA: 2s - loss: 0.7504 - sparse_categorical_accuracy: 0.669 - ETA: 1s - loss: 0.7474 - sparse_categorical_accuracy: 0.670 - ETA: 1s - loss: 0.7460 - sparse_categorical_accuracy: 0.671 - ETA: 1s - loss: 0.7475 - sparse_categorical_accuracy: 0.671 - ETA: 1s - loss: 0.7470 - sparse_categorical_accuracy: 0.673 - ETA: 0s - loss: 0.7469 - sparse_categorical_accuracy: 0.673 - ETA: 0s - loss: 0.7463 - sparse_categorical_accuracy: 0.674 - ETA: 0s - loss: 0.7478 - sparse_categorical_accuracy: 0.672 - ETA: 0s - loss: 0.7490 - sparse_categorical_accuracy: 0.672 - ETA: 0s - loss: 0.7476 - sparse_categorical_accuracy: 0.672 - 16s 8ms/step - loss: 0.7475 - sparse_categorical_accuracy: 0.6715 - val_loss: 0.7507 - val_sparse_categorical_accuracy: 0.6608\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.78182 to 0.75068, saving model to best_e2e_rnn_atten_model.hdf5\n",
      "Epoch 6/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - ETA: 13s - loss: 0.8337 - sparse_categorical_accuracy: 0.56 - ETA: 14s - loss: 0.8299 - sparse_categorical_accuracy: 0.53 - ETA: 14s - loss: 0.7446 - sparse_categorical_accuracy: 0.58 - ETA: 13s - loss: 0.7248 - sparse_categorical_accuracy: 0.63 - ETA: 13s - loss: 0.7838 - sparse_categorical_accuracy: 0.61 - ETA: 13s - loss: 0.7587 - sparse_categorical_accuracy: 0.63 - ETA: 13s - loss: 0.7477 - sparse_categorical_accuracy: 0.63 - ETA: 12s - loss: 0.7764 - sparse_categorical_accuracy: 0.64 - ETA: 12s - loss: 0.7542 - sparse_categorical_accuracy: 0.65 - ETA: 12s - loss: 0.7486 - sparse_categorical_accuracy: 0.65 - ETA: 12s - loss: 0.7365 - sparse_categorical_accuracy: 0.65 - ETA: 11s - loss: 0.7217 - sparse_categorical_accuracy: 0.65 - ETA: 11s - loss: 0.7057 - sparse_categorical_accuracy: 0.66 - ETA: 11s - loss: 0.7175 - sparse_categorical_accuracy: 0.65 - ETA: 10s - loss: 0.7292 - sparse_categorical_accuracy: 0.65 - ETA: 10s - loss: 0.7221 - sparse_categorical_accuracy: 0.65 - ETA: 10s - loss: 0.7245 - sparse_categorical_accuracy: 0.65 - ETA: 10s - loss: 0.7303 - sparse_categorical_accuracy: 0.65 - ETA: 10s - loss: 0.7429 - sparse_categorical_accuracy: 0.64 - ETA: 9s - loss: 0.7403 - sparse_categorical_accuracy: 0.6483 - ETA: 9s - loss: 0.7303 - sparse_categorical_accuracy: 0.657 - ETA: 9s - loss: 0.7429 - sparse_categorical_accuracy: 0.647 - ETA: 9s - loss: 0.7325 - sparse_categorical_accuracy: 0.652 - ETA: 8s - loss: 0.7233 - sparse_categorical_accuracy: 0.658 - ETA: 8s - loss: 0.7306 - sparse_categorical_accuracy: 0.658 - ETA: 8s - loss: 0.7233 - sparse_categorical_accuracy: 0.665 - ETA: 8s - loss: 0.7250 - sparse_categorical_accuracy: 0.665 - ETA: 8s - loss: 0.7204 - sparse_categorical_accuracy: 0.665 - ETA: 7s - loss: 0.7246 - sparse_categorical_accuracy: 0.667 - ETA: 7s - loss: 0.7246 - sparse_categorical_accuracy: 0.667 - ETA: 7s - loss: 0.7226 - sparse_categorical_accuracy: 0.671 - ETA: 7s - loss: 0.7250 - sparse_categorical_accuracy: 0.674 - ETA: 7s - loss: 0.7287 - sparse_categorical_accuracy: 0.673 - ETA: 6s - loss: 0.7279 - sparse_categorical_accuracy: 0.672 - ETA: 6s - loss: 0.7285 - sparse_categorical_accuracy: 0.671 - ETA: 6s - loss: 0.7302 - sparse_categorical_accuracy: 0.669 - ETA: 6s - loss: 0.7235 - sparse_categorical_accuracy: 0.672 - ETA: 6s - loss: 0.7198 - sparse_categorical_accuracy: 0.676 - ETA: 5s - loss: 0.7209 - sparse_categorical_accuracy: 0.677 - ETA: 5s - loss: 0.7276 - sparse_categorical_accuracy: 0.673 - ETA: 5s - loss: 0.7256 - sparse_categorical_accuracy: 0.675 - ETA: 5s - loss: 0.7224 - sparse_categorical_accuracy: 0.679 - ETA: 5s - loss: 0.7193 - sparse_categorical_accuracy: 0.679 - ETA: 4s - loss: 0.7117 - sparse_categorical_accuracy: 0.682 - ETA: 4s - loss: 0.7243 - sparse_categorical_accuracy: 0.679 - ETA: 4s - loss: 0.7275 - sparse_categorical_accuracy: 0.680 - ETA: 4s - loss: 0.7257 - sparse_categorical_accuracy: 0.683 - ETA: 3s - loss: 0.7204 - sparse_categorical_accuracy: 0.684 - ETA: 3s - loss: 0.7184 - sparse_categorical_accuracy: 0.685 - ETA: 3s - loss: 0.7144 - sparse_categorical_accuracy: 0.687 - ETA: 3s - loss: 0.7145 - sparse_categorical_accuracy: 0.686 - ETA: 3s - loss: 0.7165 - sparse_categorical_accuracy: 0.685 - ETA: 2s - loss: 0.7130 - sparse_categorical_accuracy: 0.687 - ETA: 2s - loss: 0.7104 - sparse_categorical_accuracy: 0.688 - ETA: 2s - loss: 0.7122 - sparse_categorical_accuracy: 0.686 - ETA: 2s - loss: 0.7137 - sparse_categorical_accuracy: 0.685 - ETA: 2s - loss: 0.7192 - sparse_categorical_accuracy: 0.683 - ETA: 1s - loss: 0.7210 - sparse_categorical_accuracy: 0.683 - ETA: 1s - loss: 0.7192 - sparse_categorical_accuracy: 0.684 - ETA: 1s - loss: 0.7162 - sparse_categorical_accuracy: 0.686 - ETA: 1s - loss: 0.7185 - sparse_categorical_accuracy: 0.687 - ETA: 0s - loss: 0.7207 - sparse_categorical_accuracy: 0.687 - ETA: 0s - loss: 0.7193 - sparse_categorical_accuracy: 0.688 - ETA: 0s - loss: 0.7205 - sparse_categorical_accuracy: 0.687 - ETA: 0s - loss: 0.7253 - sparse_categorical_accuracy: 0.685 - ETA: 0s - loss: 0.7243 - sparse_categorical_accuracy: 0.684 - 16s 8ms/step - loss: 0.7222 - sparse_categorical_accuracy: 0.6860 - val_loss: 0.7361 - val_sparse_categorical_accuracy: 0.6806\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.75068 to 0.73611, saving model to best_e2e_rnn_atten_model.hdf5\n",
      "Epoch 7/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - ETA: 13s - loss: 0.6055 - sparse_categorical_accuracy: 0.76 - ETA: 13s - loss: 0.6405 - sparse_categorical_accuracy: 0.70 - ETA: 13s - loss: 0.6258 - sparse_categorical_accuracy: 0.73 - ETA: 13s - loss: 0.6227 - sparse_categorical_accuracy: 0.75 - ETA: 13s - loss: 0.6331 - sparse_categorical_accuracy: 0.72 - ETA: 13s - loss: 0.6196 - sparse_categorical_accuracy: 0.72 - ETA: 12s - loss: 0.6189 - sparse_categorical_accuracy: 0.72 - ETA: 12s - loss: 0.6166 - sparse_categorical_accuracy: 0.72 - ETA: 12s - loss: 0.6461 - sparse_categorical_accuracy: 0.71 - ETA: 12s - loss: 0.6586 - sparse_categorical_accuracy: 0.72 - ETA: 12s - loss: 0.6495 - sparse_categorical_accuracy: 0.72 - ETA: 11s - loss: 0.6930 - sparse_categorical_accuracy: 0.69 - ETA: 11s - loss: 0.6909 - sparse_categorical_accuracy: 0.69 - ETA: 11s - loss: 0.6854 - sparse_categorical_accuracy: 0.70 - ETA: 11s - loss: 0.6897 - sparse_categorical_accuracy: 0.69 - ETA: 10s - loss: 0.6988 - sparse_categorical_accuracy: 0.68 - ETA: 10s - loss: 0.6913 - sparse_categorical_accuracy: 0.69 - ETA: 10s - loss: 0.6836 - sparse_categorical_accuracy: 0.69 - ETA: 10s - loss: 0.6914 - sparse_categorical_accuracy: 0.69 - ETA: 9s - loss: 0.6909 - sparse_categorical_accuracy: 0.6933 - ETA: 9s - loss: 0.6987 - sparse_categorical_accuracy: 0.685 - ETA: 9s - loss: 0.6997 - sparse_categorical_accuracy: 0.686 - ETA: 9s - loss: 0.7049 - sparse_categorical_accuracy: 0.689 - ETA: 8s - loss: 0.7003 - sparse_categorical_accuracy: 0.690 - ETA: 8s - loss: 0.7046 - sparse_categorical_accuracy: 0.688 - ETA: 8s - loss: 0.7121 - sparse_categorical_accuracy: 0.680 - ETA: 8s - loss: 0.7060 - sparse_categorical_accuracy: 0.687 - ETA: 8s - loss: 0.6980 - sparse_categorical_accuracy: 0.692 - ETA: 7s - loss: 0.6969 - sparse_categorical_accuracy: 0.694 - ETA: 7s - loss: 0.6930 - sparse_categorical_accuracy: 0.698 - ETA: 7s - loss: 0.6895 - sparse_categorical_accuracy: 0.697 - ETA: 7s - loss: 0.6945 - sparse_categorical_accuracy: 0.695 - ETA: 7s - loss: 0.6972 - sparse_categorical_accuracy: 0.693 - ETA: 6s - loss: 0.6981 - sparse_categorical_accuracy: 0.694 - ETA: 6s - loss: 0.6964 - sparse_categorical_accuracy: 0.697 - ETA: 6s - loss: 0.7009 - sparse_categorical_accuracy: 0.694 - ETA: 6s - loss: 0.7004 - sparse_categorical_accuracy: 0.693 - ETA: 6s - loss: 0.7014 - sparse_categorical_accuracy: 0.694 - ETA: 5s - loss: 0.6994 - sparse_categorical_accuracy: 0.695 - ETA: 5s - loss: 0.7019 - sparse_categorical_accuracy: 0.694 - ETA: 5s - loss: 0.6993 - sparse_categorical_accuracy: 0.695 - ETA: 5s - loss: 0.7034 - sparse_categorical_accuracy: 0.689 - ETA: 4s - loss: 0.7023 - sparse_categorical_accuracy: 0.689 - ETA: 4s - loss: 0.6975 - sparse_categorical_accuracy: 0.693 - ETA: 4s - loss: 0.7002 - sparse_categorical_accuracy: 0.691 - ETA: 4s - loss: 0.6975 - sparse_categorical_accuracy: 0.692 - ETA: 4s - loss: 0.6982 - sparse_categorical_accuracy: 0.694 - ETA: 3s - loss: 0.6971 - sparse_categorical_accuracy: 0.695 - ETA: 3s - loss: 0.6965 - sparse_categorical_accuracy: 0.695 - ETA: 3s - loss: 0.6949 - sparse_categorical_accuracy: 0.697 - ETA: 3s - loss: 0.6956 - sparse_categorical_accuracy: 0.698 - ETA: 3s - loss: 0.6927 - sparse_categorical_accuracy: 0.700 - ETA: 2s - loss: 0.6880 - sparse_categorical_accuracy: 0.703 - ETA: 2s - loss: 0.6833 - sparse_categorical_accuracy: 0.704 - ETA: 2s - loss: 0.6824 - sparse_categorical_accuracy: 0.705 - ETA: 2s - loss: 0.6842 - sparse_categorical_accuracy: 0.704 - ETA: 2s - loss: 0.6827 - sparse_categorical_accuracy: 0.705 - ETA: 1s - loss: 0.6853 - sparse_categorical_accuracy: 0.705 - ETA: 1s - loss: 0.6859 - sparse_categorical_accuracy: 0.704 - ETA: 1s - loss: 0.6844 - sparse_categorical_accuracy: 0.703 - ETA: 1s - loss: 0.6848 - sparse_categorical_accuracy: 0.706 - ETA: 0s - loss: 0.6838 - sparse_categorical_accuracy: 0.706 - ETA: 0s - loss: 0.6837 - sparse_categorical_accuracy: 0.706 - ETA: 0s - loss: 0.6813 - sparse_categorical_accuracy: 0.707 - ETA: 0s - loss: 0.6819 - sparse_categorical_accuracy: 0.707 - ETA: 0s - loss: 0.6836 - sparse_categorical_accuracy: 0.707 - 15s 8ms/step - loss: 0.6830 - sparse_categorical_accuracy: 0.7075 - val_loss: 0.7446 - val_sparse_categorical_accuracy: 0.6696\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.73611\n",
      "Epoch 8/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - ETA: 12s - loss: 0.6628 - sparse_categorical_accuracy: 0.76 - ETA: 11s - loss: 0.5413 - sparse_categorical_accuracy: 0.80 - ETA: 12s - loss: 0.5719 - sparse_categorical_accuracy: 0.77 - ETA: 12s - loss: 0.5795 - sparse_categorical_accuracy: 0.75 - ETA: 11s - loss: 0.6818 - sparse_categorical_accuracy: 0.72 - ETA: 11s - loss: 0.6727 - sparse_categorical_accuracy: 0.70 - ETA: 11s - loss: 0.6854 - sparse_categorical_accuracy: 0.70 - ETA: 11s - loss: 0.6686 - sparse_categorical_accuracy: 0.70 - ETA: 11s - loss: 0.7018 - sparse_categorical_accuracy: 0.70 - ETA: 11s - loss: 0.6751 - sparse_categorical_accuracy: 0.72 - ETA: 11s - loss: 0.6698 - sparse_categorical_accuracy: 0.71 - ETA: 11s - loss: 0.6689 - sparse_categorical_accuracy: 0.71 - ETA: 10s - loss: 0.6845 - sparse_categorical_accuracy: 0.71 - ETA: 10s - loss: 0.6895 - sparse_categorical_accuracy: 0.70 - ETA: 10s - loss: 0.6752 - sparse_categorical_accuracy: 0.71 - ETA: 10s - loss: 0.6690 - sparse_categorical_accuracy: 0.72 - ETA: 9s - loss: 0.6620 - sparse_categorical_accuracy: 0.7255 - ETA: 9s - loss: 0.6565 - sparse_categorical_accuracy: 0.724 - ETA: 9s - loss: 0.6584 - sparse_categorical_accuracy: 0.722 - ETA: 9s - loss: 0.6496 - sparse_categorical_accuracy: 0.725 - ETA: 9s - loss: 0.6434 - sparse_categorical_accuracy: 0.725 - ETA: 8s - loss: 0.6436 - sparse_categorical_accuracy: 0.727 - ETA: 8s - loss: 0.6491 - sparse_categorical_accuracy: 0.721 - ETA: 8s - loss: 0.6561 - sparse_categorical_accuracy: 0.716 - ETA: 8s - loss: 0.6512 - sparse_categorical_accuracy: 0.717 - ETA: 8s - loss: 0.6517 - sparse_categorical_accuracy: 0.715 - ETA: 7s - loss: 0.6516 - sparse_categorical_accuracy: 0.713 - ETA: 7s - loss: 0.6551 - sparse_categorical_accuracy: 0.708 - ETA: 7s - loss: 0.6560 - sparse_categorical_accuracy: 0.706 - ETA: 7s - loss: 0.6556 - sparse_categorical_accuracy: 0.706 - ETA: 7s - loss: 0.6596 - sparse_categorical_accuracy: 0.705 - ETA: 6s - loss: 0.6561 - sparse_categorical_accuracy: 0.707 - ETA: 6s - loss: 0.6541 - sparse_categorical_accuracy: 0.710 - ETA: 6s - loss: 0.6557 - sparse_categorical_accuracy: 0.709 - ETA: 6s - loss: 0.6543 - sparse_categorical_accuracy: 0.708 - ETA: 6s - loss: 0.6564 - sparse_categorical_accuracy: 0.707 - ETA: 5s - loss: 0.6497 - sparse_categorical_accuracy: 0.712 - ETA: 5s - loss: 0.6489 - sparse_categorical_accuracy: 0.712 - ETA: 5s - loss: 0.6513 - sparse_categorical_accuracy: 0.712 - ETA: 5s - loss: 0.6638 - sparse_categorical_accuracy: 0.709 - ETA: 5s - loss: 0.6580 - sparse_categorical_accuracy: 0.713 - ETA: 4s - loss: 0.6547 - sparse_categorical_accuracy: 0.713 - ETA: 4s - loss: 0.6603 - sparse_categorical_accuracy: 0.710 - ETA: 4s - loss: 0.6626 - sparse_categorical_accuracy: 0.712 - ETA: 4s - loss: 0.6683 - sparse_categorical_accuracy: 0.708 - ETA: 4s - loss: 0.6665 - sparse_categorical_accuracy: 0.709 - ETA: 3s - loss: 0.6659 - sparse_categorical_accuracy: 0.709 - ETA: 3s - loss: 0.6677 - sparse_categorical_accuracy: 0.709 - ETA: 3s - loss: 0.6699 - sparse_categorical_accuracy: 0.709 - ETA: 3s - loss: 0.6680 - sparse_categorical_accuracy: 0.710 - ETA: 3s - loss: 0.6641 - sparse_categorical_accuracy: 0.712 - ETA: 2s - loss: 0.6614 - sparse_categorical_accuracy: 0.715 - ETA: 2s - loss: 0.6629 - sparse_categorical_accuracy: 0.714 - ETA: 2s - loss: 0.6658 - sparse_categorical_accuracy: 0.712 - ETA: 2s - loss: 0.6713 - sparse_categorical_accuracy: 0.709 - ETA: 2s - loss: 0.6723 - sparse_categorical_accuracy: 0.710 - ETA: 1s - loss: 0.6674 - sparse_categorical_accuracy: 0.712 - ETA: 1s - loss: 0.6656 - sparse_categorical_accuracy: 0.713 - ETA: 1s - loss: 0.6648 - sparse_categorical_accuracy: 0.713 - ETA: 1s - loss: 0.6657 - sparse_categorical_accuracy: 0.713 - ETA: 1s - loss: 0.6662 - sparse_categorical_accuracy: 0.712 - ETA: 0s - loss: 0.6705 - sparse_categorical_accuracy: 0.711 - ETA: 0s - loss: 0.6674 - sparse_categorical_accuracy: 0.713 - ETA: 0s - loss: 0.6658 - sparse_categorical_accuracy: 0.714 - ETA: 0s - loss: 0.6645 - sparse_categorical_accuracy: 0.714 - ETA: 0s - loss: 0.6671 - sparse_categorical_accuracy: 0.715 - 14s 7ms/step - loss: 0.6690 - sparse_categorical_accuracy: 0.7135 - val_loss: 0.7418 - val_sparse_categorical_accuracy: 0.6806\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.73611\n",
      "Epoch 9/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - ETA: 13s - loss: 0.7659 - sparse_categorical_accuracy: 0.60 - ETA: 14s - loss: 0.7372 - sparse_categorical_accuracy: 0.70 - ETA: 13s - loss: 0.6795 - sparse_categorical_accuracy: 0.73 - ETA: 13s - loss: 0.6841 - sparse_categorical_accuracy: 0.71 - ETA: 13s - loss: 0.6974 - sparse_categorical_accuracy: 0.71 - ETA: 13s - loss: 0.7179 - sparse_categorical_accuracy: 0.70 - ETA: 13s - loss: 0.6780 - sparse_categorical_accuracy: 0.72 - ETA: 12s - loss: 0.6630 - sparse_categorical_accuracy: 0.73 - ETA: 12s - loss: 0.6547 - sparse_categorical_accuracy: 0.73 - ETA: 12s - loss: 0.6505 - sparse_categorical_accuracy: 0.73 - ETA: 12s - loss: 0.6703 - sparse_categorical_accuracy: 0.72 - ETA: 11s - loss: 0.6731 - sparse_categorical_accuracy: 0.72 - ETA: 11s - loss: 0.6647 - sparse_categorical_accuracy: 0.73 - ETA: 11s - loss: 0.6694 - sparse_categorical_accuracy: 0.73 - ETA: 11s - loss: 0.6617 - sparse_categorical_accuracy: 0.73 - ETA: 10s - loss: 0.6608 - sparse_categorical_accuracy: 0.73 - ETA: 10s - loss: 0.6739 - sparse_categorical_accuracy: 0.72 - ETA: 10s - loss: 0.6656 - sparse_categorical_accuracy: 0.72 - ETA: 10s - loss: 0.6550 - sparse_categorical_accuracy: 0.73 - ETA: 9s - loss: 0.6502 - sparse_categorical_accuracy: 0.7350 - ETA: 9s - loss: 0.6512 - sparse_categorical_accuracy: 0.731 - ETA: 9s - loss: 0.6431 - sparse_categorical_accuracy: 0.731 - ETA: 9s - loss: 0.6312 - sparse_categorical_accuracy: 0.736 - ETA: 9s - loss: 0.6277 - sparse_categorical_accuracy: 0.738 - ETA: 8s - loss: 0.6245 - sparse_categorical_accuracy: 0.738 - ETA: 8s - loss: 0.6157 - sparse_categorical_accuracy: 0.742 - ETA: 8s - loss: 0.6145 - sparse_categorical_accuracy: 0.742 - ETA: 8s - loss: 0.6054 - sparse_categorical_accuracy: 0.746 - ETA: 8s - loss: 0.6073 - sparse_categorical_accuracy: 0.747 - ETA: 7s - loss: 0.6122 - sparse_categorical_accuracy: 0.747 - ETA: 7s - loss: 0.6130 - sparse_categorical_accuracy: 0.749 - ETA: 7s - loss: 0.6095 - sparse_categorical_accuracy: 0.751 - ETA: 7s - loss: 0.6132 - sparse_categorical_accuracy: 0.750 - ETA: 6s - loss: 0.6158 - sparse_categorical_accuracy: 0.747 - ETA: 6s - loss: 0.6213 - sparse_categorical_accuracy: 0.745 - ETA: 6s - loss: 0.6235 - sparse_categorical_accuracy: 0.745 - ETA: 6s - loss: 0.6302 - sparse_categorical_accuracy: 0.739 - ETA: 6s - loss: 0.6389 - sparse_categorical_accuracy: 0.733 - ETA: 5s - loss: 0.6377 - sparse_categorical_accuracy: 0.733 - ETA: 5s - loss: 0.6413 - sparse_categorical_accuracy: 0.730 - ETA: 5s - loss: 0.6356 - sparse_categorical_accuracy: 0.734 - ETA: 5s - loss: 0.6421 - sparse_categorical_accuracy: 0.733 - ETA: 5s - loss: 0.6467 - sparse_categorical_accuracy: 0.731 - ETA: 4s - loss: 0.6466 - sparse_categorical_accuracy: 0.733 - ETA: 4s - loss: 0.6453 - sparse_categorical_accuracy: 0.735 - ETA: 4s - loss: 0.6442 - sparse_categorical_accuracy: 0.734 - ETA: 4s - loss: 0.6439 - sparse_categorical_accuracy: 0.735 - ETA: 3s - loss: 0.6395 - sparse_categorical_accuracy: 0.737 - ETA: 3s - loss: 0.6401 - sparse_categorical_accuracy: 0.737 - ETA: 3s - loss: 0.6438 - sparse_categorical_accuracy: 0.736 - ETA: 3s - loss: 0.6437 - sparse_categorical_accuracy: 0.736 - ETA: 3s - loss: 0.6434 - sparse_categorical_accuracy: 0.737 - ETA: 2s - loss: 0.6433 - sparse_categorical_accuracy: 0.736 - ETA: 2s - loss: 0.6395 - sparse_categorical_accuracy: 0.738 - ETA: 2s - loss: 0.6397 - sparse_categorical_accuracy: 0.736 - ETA: 2s - loss: 0.6417 - sparse_categorical_accuracy: 0.736 - ETA: 2s - loss: 0.6445 - sparse_categorical_accuracy: 0.734 - ETA: 1s - loss: 0.6420 - sparse_categorical_accuracy: 0.735 - ETA: 1s - loss: 0.6426 - sparse_categorical_accuracy: 0.734 - ETA: 1s - loss: 0.6432 - sparse_categorical_accuracy: 0.733 - ETA: 1s - loss: 0.6458 - sparse_categorical_accuracy: 0.731 - ETA: 0s - loss: 0.6451 - sparse_categorical_accuracy: 0.731 - ETA: 0s - loss: 0.6493 - sparse_categorical_accuracy: 0.730 - ETA: 0s - loss: 0.6519 - sparse_categorical_accuracy: 0.729 - ETA: 0s - loss: 0.6541 - sparse_categorical_accuracy: 0.728 - ETA: 0s - loss: 0.6570 - sparse_categorical_accuracy: 0.728 - 15s 8ms/step - loss: 0.6568 - sparse_categorical_accuracy: 0.7280 - val_loss: 0.7489 - val_sparse_categorical_accuracy: 0.6740\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.73611\n",
      "Epoch 10/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - ETA: 14s - loss: 0.4191 - sparse_categorical_accuracy: 0.80 - ETA: 14s - loss: 0.5249 - sparse_categorical_accuracy: 0.80 - ETA: 13s - loss: 0.5470 - sparse_categorical_accuracy: 0.77 - ETA: 12s - loss: 0.5989 - sparse_categorical_accuracy: 0.75 - ETA: 12s - loss: 0.5939 - sparse_categorical_accuracy: 0.76 - ETA: 12s - loss: 0.6136 - sparse_categorical_accuracy: 0.74 - ETA: 11s - loss: 0.6449 - sparse_categorical_accuracy: 0.73 - ETA: 11s - loss: 0.6179 - sparse_categorical_accuracy: 0.74 - ETA: 11s - loss: 0.6061 - sparse_categorical_accuracy: 0.74 - ETA: 11s - loss: 0.6105 - sparse_categorical_accuracy: 0.74 - ETA: 11s - loss: 0.6056 - sparse_categorical_accuracy: 0.75 - ETA: 10s - loss: 0.5996 - sparse_categorical_accuracy: 0.75 - ETA: 10s - loss: 0.5951 - sparse_categorical_accuracy: 0.75 - ETA: 10s - loss: 0.5956 - sparse_categorical_accuracy: 0.75 - ETA: 10s - loss: 0.5888 - sparse_categorical_accuracy: 0.75 - ETA: 10s - loss: 0.5888 - sparse_categorical_accuracy: 0.75 - ETA: 9s - loss: 0.5881 - sparse_categorical_accuracy: 0.7490 - ETA: 9s - loss: 0.5990 - sparse_categorical_accuracy: 0.740 - ETA: 9s - loss: 0.6051 - sparse_categorical_accuracy: 0.742 - ETA: 9s - loss: 0.6035 - sparse_categorical_accuracy: 0.741 - ETA: 9s - loss: 0.6016 - sparse_categorical_accuracy: 0.742 - ETA: 9s - loss: 0.5958 - sparse_categorical_accuracy: 0.745 - ETA: 8s - loss: 0.5887 - sparse_categorical_accuracy: 0.749 - ETA: 8s - loss: 0.6033 - sparse_categorical_accuracy: 0.743 - ETA: 8s - loss: 0.6028 - sparse_categorical_accuracy: 0.746 - ETA: 8s - loss: 0.6013 - sparse_categorical_accuracy: 0.747 - ETA: 8s - loss: 0.5972 - sparse_categorical_accuracy: 0.750 - ETA: 7s - loss: 0.5981 - sparse_categorical_accuracy: 0.746 - ETA: 7s - loss: 0.5956 - sparse_categorical_accuracy: 0.748 - ETA: 7s - loss: 0.6070 - sparse_categorical_accuracy: 0.740 - ETA: 7s - loss: 0.6060 - sparse_categorical_accuracy: 0.740 - ETA: 7s - loss: 0.6069 - sparse_categorical_accuracy: 0.739 - ETA: 6s - loss: 0.6122 - sparse_categorical_accuracy: 0.733 - ETA: 6s - loss: 0.6143 - sparse_categorical_accuracy: 0.730 - ETA: 6s - loss: 0.6114 - sparse_categorical_accuracy: 0.732 - ETA: 6s - loss: 0.6100 - sparse_categorical_accuracy: 0.732 - ETA: 6s - loss: 0.6088 - sparse_categorical_accuracy: 0.733 - ETA: 5s - loss: 0.6113 - sparse_categorical_accuracy: 0.732 - ETA: 5s - loss: 0.6070 - sparse_categorical_accuracy: 0.737 - ETA: 5s - loss: 0.6088 - sparse_categorical_accuracy: 0.735 - ETA: 5s - loss: 0.6108 - sparse_categorical_accuracy: 0.734 - ETA: 5s - loss: 0.6122 - sparse_categorical_accuracy: 0.735 - ETA: 4s - loss: 0.6218 - sparse_categorical_accuracy: 0.732 - ETA: 4s - loss: 0.6190 - sparse_categorical_accuracy: 0.732 - ETA: 4s - loss: 0.6196 - sparse_categorical_accuracy: 0.733 - ETA: 4s - loss: 0.6191 - sparse_categorical_accuracy: 0.734 - ETA: 4s - loss: 0.6271 - sparse_categorical_accuracy: 0.731 - ETA: 3s - loss: 0.6306 - sparse_categorical_accuracy: 0.730 - ETA: 3s - loss: 0.6349 - sparse_categorical_accuracy: 0.729 - ETA: 3s - loss: 0.6317 - sparse_categorical_accuracy: 0.733 - ETA: 3s - loss: 0.6324 - sparse_categorical_accuracy: 0.734 - ETA: 3s - loss: 0.6347 - sparse_categorical_accuracy: 0.732 - ETA: 2s - loss: 0.6343 - sparse_categorical_accuracy: 0.733 - ETA: 2s - loss: 0.6343 - sparse_categorical_accuracy: 0.732 - ETA: 2s - loss: 0.6349 - sparse_categorical_accuracy: 0.731 - ETA: 2s - loss: 0.6337 - sparse_categorical_accuracy: 0.733 - ETA: 2s - loss: 0.6359 - sparse_categorical_accuracy: 0.732 - ETA: 1s - loss: 0.6368 - sparse_categorical_accuracy: 0.732 - ETA: 1s - loss: 0.6421 - sparse_categorical_accuracy: 0.730 - ETA: 1s - loss: 0.6387 - sparse_categorical_accuracy: 0.732 - ETA: 1s - loss: 0.6403 - sparse_categorical_accuracy: 0.730 - ETA: 0s - loss: 0.6371 - sparse_categorical_accuracy: 0.731 - ETA: 0s - loss: 0.6369 - sparse_categorical_accuracy: 0.731 - ETA: 0s - loss: 0.6353 - sparse_categorical_accuracy: 0.731 - ETA: 0s - loss: 0.6366 - sparse_categorical_accuracy: 0.730 - ETA: 0s - loss: 0.6351 - sparse_categorical_accuracy: 0.730 - 15s 7ms/step - loss: 0.6343 - sparse_categorical_accuracy: 0.7295 - val_loss: 0.7626 - val_sparse_categorical_accuracy: 0.6740\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.73611\n",
      "Epoch 11/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - ETA: 12s - loss: 0.6776 - sparse_categorical_accuracy: 0.66 - ETA: 13s - loss: 0.6283 - sparse_categorical_accuracy: 0.66 - ETA: 12s - loss: 0.5870 - sparse_categorical_accuracy: 0.70 - ETA: 12s - loss: 0.6355 - sparse_categorical_accuracy: 0.69 - ETA: 12s - loss: 0.6288 - sparse_categorical_accuracy: 0.68 - ETA: 12s - loss: 0.6653 - sparse_categorical_accuracy: 0.66 - ETA: 11s - loss: 0.6492 - sparse_categorical_accuracy: 0.68 - ETA: 11s - loss: 0.6198 - sparse_categorical_accuracy: 0.70 - ETA: 11s - loss: 0.6397 - sparse_categorical_accuracy: 0.69 - ETA: 11s - loss: 0.6126 - sparse_categorical_accuracy: 0.70 - ETA: 11s - loss: 0.6253 - sparse_categorical_accuracy: 0.70 - ETA: 11s - loss: 0.6093 - sparse_categorical_accuracy: 0.70 - ETA: 10s - loss: 0.6051 - sparse_categorical_accuracy: 0.71 - ETA: 10s - loss: 0.6091 - sparse_categorical_accuracy: 0.71 - ETA: 10s - loss: 0.5972 - sparse_categorical_accuracy: 0.72 - ETA: 10s - loss: 0.5950 - sparse_categorical_accuracy: 0.72 - ETA: 10s - loss: 0.5905 - sparse_categorical_accuracy: 0.73 - ETA: 9s - loss: 0.6010 - sparse_categorical_accuracy: 0.7296 - ETA: 9s - loss: 0.6032 - sparse_categorical_accuracy: 0.731 - ETA: 9s - loss: 0.6004 - sparse_categorical_accuracy: 0.730 - ETA: 9s - loss: 0.6004 - sparse_categorical_accuracy: 0.730 - ETA: 9s - loss: 0.6069 - sparse_categorical_accuracy: 0.727 - ETA: 8s - loss: 0.6175 - sparse_categorical_accuracy: 0.724 - ETA: 8s - loss: 0.6196 - sparse_categorical_accuracy: 0.726 - ETA: 8s - loss: 0.6151 - sparse_categorical_accuracy: 0.726 - ETA: 8s - loss: 0.6242 - sparse_categorical_accuracy: 0.720 - ETA: 8s - loss: 0.6315 - sparse_categorical_accuracy: 0.717 - ETA: 7s - loss: 0.6330 - sparse_categorical_accuracy: 0.716 - ETA: 7s - loss: 0.6290 - sparse_categorical_accuracy: 0.716 - ETA: 7s - loss: 0.6331 - sparse_categorical_accuracy: 0.715 - ETA: 7s - loss: 0.6339 - sparse_categorical_accuracy: 0.716 - ETA: 7s - loss: 0.6353 - sparse_categorical_accuracy: 0.717 - ETA: 6s - loss: 0.6342 - sparse_categorical_accuracy: 0.718 - ETA: 6s - loss: 0.6305 - sparse_categorical_accuracy: 0.720 - ETA: 6s - loss: 0.6444 - sparse_categorical_accuracy: 0.714 - ETA: 6s - loss: 0.6410 - sparse_categorical_accuracy: 0.714 - ETA: 6s - loss: 0.6380 - sparse_categorical_accuracy: 0.714 - ETA: 5s - loss: 0.6327 - sparse_categorical_accuracy: 0.718 - ETA: 5s - loss: 0.6332 - sparse_categorical_accuracy: 0.719 - ETA: 5s - loss: 0.6271 - sparse_categorical_accuracy: 0.723 - ETA: 5s - loss: 0.6301 - sparse_categorical_accuracy: 0.722 - ETA: 5s - loss: 0.6330 - sparse_categorical_accuracy: 0.723 - ETA: 4s - loss: 0.6335 - sparse_categorical_accuracy: 0.725 - ETA: 4s - loss: 0.6368 - sparse_categorical_accuracy: 0.723 - ETA: 4s - loss: 0.6343 - sparse_categorical_accuracy: 0.725 - ETA: 4s - loss: 0.6335 - sparse_categorical_accuracy: 0.725 - ETA: 4s - loss: 0.6301 - sparse_categorical_accuracy: 0.725 - ETA: 3s - loss: 0.6328 - sparse_categorical_accuracy: 0.725 - ETA: 3s - loss: 0.6346 - sparse_categorical_accuracy: 0.724 - ETA: 3s - loss: 0.6325 - sparse_categorical_accuracy: 0.726 - ETA: 3s - loss: 0.6264 - sparse_categorical_accuracy: 0.730 - ETA: 2s - loss: 0.6233 - sparse_categorical_accuracy: 0.732 - ETA: 2s - loss: 0.6226 - sparse_categorical_accuracy: 0.732 - ETA: 2s - loss: 0.6216 - sparse_categorical_accuracy: 0.733 - ETA: 2s - loss: 0.6185 - sparse_categorical_accuracy: 0.736 - ETA: 2s - loss: 0.6166 - sparse_categorical_accuracy: 0.736 - ETA: 1s - loss: 0.6199 - sparse_categorical_accuracy: 0.736 - ETA: 1s - loss: 0.6161 - sparse_categorical_accuracy: 0.740 - ETA: 1s - loss: 0.6183 - sparse_categorical_accuracy: 0.738 - ETA: 1s - loss: 0.6171 - sparse_categorical_accuracy: 0.738 - ETA: 1s - loss: 0.6146 - sparse_categorical_accuracy: 0.739 - ETA: 0s - loss: 0.6121 - sparse_categorical_accuracy: 0.740 - ETA: 0s - loss: 0.6156 - sparse_categorical_accuracy: 0.738 - ETA: 0s - loss: 0.6129 - sparse_categorical_accuracy: 0.740 - ETA: 0s - loss: 0.6115 - sparse_categorical_accuracy: 0.741 - ETA: 0s - loss: 0.6137 - sparse_categorical_accuracy: 0.740 - 15s 7ms/step - loss: 0.6118 - sparse_categorical_accuracy: 0.7415 - val_loss: 0.7747 - val_sparse_categorical_accuracy: 0.6740\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.73611\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adam = ko.Nadam(clipnorm=1.0)\n",
    "model.compile(adam, loss=\"sparse_categorical_crossentropy\", metrics=[\"sparse_categorical_accuracy\"])\n",
    "\n",
    "file_path = \"best_e2e_rnn_atten_model.hdf5\"\n",
    "check_point = kc.ModelCheckpoint(file_path, monitor = \"val_loss\", verbose = 1, save_best_only = True, mode = \"min\")\n",
    "early_stop = kc.EarlyStopping(monitor = \"val_loss\", mode = \"min\", patience=5)\n",
    "history = model.fit(X_train, y_tra, batch_size=30, epochs=40, validation_data=(X_dev, y_dev),\n",
    "                    callbacks = [check_point, early_stop])\n",
    "\n",
    "file_paths.append(file_path)\n",
    "histories.append(np.min(np.asarray(history.history['val_loss'])))\n",
    "\n",
    "del model, history\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8bb147615d4920a2f3500a0fba12d31cb0869304"
   },
   "source": [
    "###  Make Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "_uuid": "79cb8d339da04ad2d52e3675a4295f55a6ccd148"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load best model: best_e2e_rnn_atten_model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Isak\\.conda\\envs\\gender\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:110: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\Users\\Isak\\.conda\\envs\\gender\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:110: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\Users\\Isak\\.conda\\envs\\gender\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:110: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "print(\"load best model: \" + str(file_paths[np.argmin(histories)]))\n",
    "model = models.load_model(\n",
    "    file_paths[np.argmin(histories)], cos[np.argmin(histories)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['best_e2e_rnn_atten_model.hdf5']\n"
     ]
    }
   ],
   "source": [
    "print(file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "_uuid": "4c465342c3a99a4996a104fdb00ceab4e85b9649"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - ETA:  - 5s 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>NEITHER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>development-1</td>\n",
       "      <td>0.117889</td>\n",
       "      <td>0.723842</td>\n",
       "      <td>0.158269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>development-2</td>\n",
       "      <td>0.902018</td>\n",
       "      <td>0.070844</td>\n",
       "      <td>0.027137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>development-3</td>\n",
       "      <td>0.226897</td>\n",
       "      <td>0.611420</td>\n",
       "      <td>0.161683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>development-4</td>\n",
       "      <td>0.156174</td>\n",
       "      <td>0.714564</td>\n",
       "      <td>0.129262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>development-5</td>\n",
       "      <td>0.251448</td>\n",
       "      <td>0.370151</td>\n",
       "      <td>0.378401</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              ID         A         B   NEITHER\n",
       "0  development-1  0.117889  0.723842  0.158269\n",
       "1  development-2  0.902018  0.070844  0.027137\n",
       "2  development-3  0.226897  0.611420  0.161683\n",
       "3  development-4  0.156174  0.714564  0.129262\n",
       "4  development-5  0.251448  0.370151  0.378401"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_preds = model.predict(X_test, batch_size = 1024, verbose = 1)\n",
    "\n",
    "sub_df_path = os.path.join(SUB_DATA_FOLDER, 'sample_submission_stage_1.csv')\n",
    "sub_df = pd.read_csv(sub_df_path)\n",
    "sub_df.loc[:, 'A'] = pd.Series(y_preds[:, 0])\n",
    "sub_df.loc[:, 'B'] = pd.Series(y_preds[:, 1])\n",
    "sub_df.loc[:, 'NEITHER'] = pd.Series(y_preds[:, 2])\n",
    "\n",
    "sub_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "_uuid": "6cfa1933801af3de8d5d6cfc1cd9c3e23481e0bd"
   },
   "outputs": [],
   "source": [
    "sub_df.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary = np.zeros((2000,3),dtype=int)\n",
    "for i, row in sub_df.iterrows():\n",
    "    max_ = max(row['A'], row['B'], row['NEITHER'])\n",
    "    if max_ == row['A']:\n",
    "        binary[i,0] = 1\n",
    "    elif max_ == row['B']:\n",
    "        binary[i,1] = 1\n",
    "    else:\n",
    "        binary[i,2] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pd.DataFrame(binary, columns=['A','B','NEITHER'])\n",
    "predictions.to_csv('data/predictions_end2end_trained1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the necessary dataframes\n",
    "predictions = pd.read_csv('data/predictions_end2end_trained1.csv', index_col=0)\n",
    "right_answers = pd.read_csv('data/right_answers.csv')\n",
    "full_data_path = 'data/gendered-pronoun-resolution/test_stage_1.tsv'\n",
    "#full_data = pd.read_csv(full_data_path, sep = '\\t', index_col = 0)\n",
    "full_data = pd.read_csv(full_data_path, sep = '\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create bootstrap confidence interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap(gold_df, pred_df, all_data_df, B=10000):\n",
    "    repeats = 1000\n",
    "    f1_statistics = []\n",
    "    bias_statistics = []\n",
    "    sample_size = len(gold_df)\n",
    "    indeces = list(range(sample_size))\n",
    "    \n",
    "    for idx in range(repeats):\n",
    "        rows = choices(indeces, k=B)\n",
    "        boot_gold_df = gold_df[gold_df.index.isin(rows)]\n",
    "        boot_gold_df.__delitem__(\"ID\")\n",
    "        boot_pred_df = pred_df[pred_df.index.isin(rows)]\n",
    "        boot_data_df = all_data_df[all_data_df.index.isin(rows)]\n",
    "        \n",
    "        f1 = calculate_f1(boot_gold_df, boot_pred_df)\n",
    "        #boot_pred_df = pd.DataFrame(binary, columns=['A','B','NEITHER'], index = list(df.index))\n",
    "        bias = calculate_bias(boot_gold_df, boot_pred_df, boot_data_df)        \n",
    "        f1_statistics.append(f1)\n",
    "        bias_statistics.append(bias)\n",
    "    \n",
    "    return [f1_statistics, bias_statistics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confidence_interval(statistics, confidence_level = 0.95):\n",
    "    bounds = 50 * (1 - confidence_level)\n",
    "    lower = np.percentile(statistics, bounds)\n",
    "    median = np.percentile(statistics, 50)\n",
    "    upper = np.percentile(statistics, 100 - bounds)\n",
    "    \n",
    "    return lower, median, upper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_ci(f1, bias, confidence_level = 0.95):\n",
    "    lower, median,upper = confidence_interval(f1, confidence_level)\n",
    "    print(\"%.3f, %s%% Bootstrap CI for F1: [%.3f, %.3f]\" % (median, confidence_level*100, lower, upper))\n",
    "    lower, median,upper = confidence_interval(bias, confidence_level)\n",
    "    print(\"%.3f, %s%% Bootstrap CI for bias: [%.3f, %.3f]\" % (median, confidence_level*100, lower, upper))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def calculate_f1(gold_df, pred_df):\n",
    "    gold = gold_df.values[:,1:].astype(int)\n",
    "    predictions = pred_df.values[:,1:].astype(int)\n",
    "    \n",
    "    f1 = f1_score(gold, predictions, average=None)\n",
    "    \n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bias(gold, pred, data_df):\n",
    "    # Define male and female pronouns\n",
    "    male_pn = ['He', 'His', 'he', 'him', 'his']\n",
    "    female_pn = ['Her', 'She', 'her', 'she']\n",
    "    \n",
    "    # Find list of indices\n",
    "    m_idx = []\n",
    "    fm_idx = []\n",
    "    #for i in range(data_df.shape[0]):\n",
    "    for i in list(data_df.index):\n",
    "        pn = data_df['Pronoun'][i]\n",
    "        if pn in male_pn:\n",
    "            #m_idx.append(list(data_df.index)[i])\n",
    "            m_idx.append(i)\n",
    "        else:\n",
    "            #m_idx.append(list(data_df.index)[i])\n",
    "            fm_idx.append(i)\n",
    "            \n",
    "    # Male pred\n",
    "    male_pred = pred.loc[m_idx, :]\n",
    "    male_pred = male_pred.values[:, :].astype(int)\n",
    "    male_gold = gold.loc[m_idx, :]\n",
    "    male_gold = male_gold.values[:, :].astype(int)\n",
    "\n",
    "    # Female pred\n",
    "    female_pred = pred.loc[fm_idx, :]\n",
    "    female_pred = female_pred.values[:, :].astype(int)\n",
    "    female_gold = gold.loc[fm_idx, :]\n",
    "    female_gold = female_gold.values[:, :].astype(int)\n",
    "\n",
    "    # Calculate scores\n",
    "    f1_m = f1_score(male_gold, male_pred, average = None)\n",
    "    f1_fm = f1_score(female_gold, female_pred, average = None)\n",
    "    \n",
    "    return f1_fm / f1_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.553, 95.0% Bootstrap CI for F1: [0.405, 0.692]\n",
      "0.955, 95.0% Bootstrap CI for bias: [0.927, 1.165]\n"
     ]
    }
   ],
   "source": [
    "[f1, bias] = bootstrap(right_answers, predictions, full_data)\n",
    "print_ci(f1, bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
