{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d48a6972378bf6116b7a6b9883992d548128640c"
   },
   "source": [
    "This Kernel implements a modified version of **a state-of-art end-to-end neural correference resolution model** published in 2017: https://www.aclweb.org/anthology/D17-1018.\n",
    "This completition only focus on a specific case of  the generic reference resolution problem, and we only need pick out the correct mention from two candidates, which simplifies the model implementation.\n",
    "\n",
    "You can compare the result of this model  with the result by other non-RNN based DL models implemented in another kernel: https://www.kaggle.com/keyit92/coreference-resolution-by-mlp-cnn-coattention-nn. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.vector_cache', 'gap', 'gendered-pronoun-resolution', 'ontonotes', 'predictions_end2end_trained1.csv', 'predictions_end2end_trained2.csv', 'right_answers.csv']\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "import gc\n",
    "print(os.listdir(\"data\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "DATA_ROOT = 'data'\n",
    "GAP_DATA_FOLDER = os.path.join(DATA_ROOT, 'gap')\n",
    "SUB_DATA_FOLDER = os.path.join(DATA_ROOT, 'gendered-pronoun-resolution')\n",
    "#FAST_TEXT_DATA_FOLDER = os.path.join(DATA_ROOT, 'fasttext-crawl-300d-2M.vec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "461cb23b791e2d210e712c933e62de59d19aa20d"
   },
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "e42e2e7f636cf0702cc472f3d855b451554927dd"
   },
   "outputs": [],
   "source": [
    "train_scheme = \"Original\"\n",
    "test_df_path = os.path.join(GAP_DATA_FOLDER, 'gap-development.tsv')\n",
    "train_df_path = os.path.join(GAP_DATA_FOLDER, 'gap-test.tsv')\n",
    "dev_df_path = os.path.join(GAP_DATA_FOLDER, 'gap-validation.tsv')\n",
    "\n",
    "# test data is also imported, but only for contributing to word_index and pos_index\n",
    "\n",
    "train_df = pd.read_csv(train_df_path, sep='\\t')\n",
    "test_df = pd.read_csv(test_df_path, sep='\\t')\n",
    "dev_df = pd.read_csv(dev_df_path, sep='\\t')\n",
    "\n",
    "# pd.options.display.max_colwidth = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Text</th>\n",
       "      <th>Pronoun</th>\n",
       "      <th>Pronoun-offset</th>\n",
       "      <th>A</th>\n",
       "      <th>A-offset</th>\n",
       "      <th>A-coref</th>\n",
       "      <th>B</th>\n",
       "      <th>B-offset</th>\n",
       "      <th>B-coref</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>test-1996</td>\n",
       "      <td>The sole exception was Wimbledon, where she pl...</td>\n",
       "      <td>She</td>\n",
       "      <td>479</td>\n",
       "      <td>Goolagong Cawley</td>\n",
       "      <td>400</td>\n",
       "      <td>True</td>\n",
       "      <td>Peggy Michel</td>\n",
       "      <td>432</td>\n",
       "      <td>False</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Evonne_Goolagong_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>test-1997</td>\n",
       "      <td>According to news reports, both Moore and Fily...</td>\n",
       "      <td>her</td>\n",
       "      <td>338</td>\n",
       "      <td>Esther Sheryl Wood</td>\n",
       "      <td>263</td>\n",
       "      <td>True</td>\n",
       "      <td>Barbara Morgan</td>\n",
       "      <td>404</td>\n",
       "      <td>False</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Hastings_Arthur_Wise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>test-1998</td>\n",
       "      <td>In June 2009, due to the popularity of the Sab...</td>\n",
       "      <td>She</td>\n",
       "      <td>328</td>\n",
       "      <td>Kayla</td>\n",
       "      <td>364</td>\n",
       "      <td>True</td>\n",
       "      <td>Natasha Henstridge</td>\n",
       "      <td>412</td>\n",
       "      <td>False</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Raya_Meddine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>test-1999</td>\n",
       "      <td>She was delivered to the Norwegian passenger s...</td>\n",
       "      <td>she</td>\n",
       "      <td>305</td>\n",
       "      <td>Irma</td>\n",
       "      <td>255</td>\n",
       "      <td>True</td>\n",
       "      <td>Bergen</td>\n",
       "      <td>274</td>\n",
       "      <td>False</td>\n",
       "      <td>http://en.wikipedia.org/wiki/SS_Irma_(1905)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>test-2000</td>\n",
       "      <td>Meg and Vicky each have three siblings, and ha...</td>\n",
       "      <td>her</td>\n",
       "      <td>275</td>\n",
       "      <td>Vicky Austin</td>\n",
       "      <td>217</td>\n",
       "      <td>True</td>\n",
       "      <td>Polly O'Keefe</td>\n",
       "      <td>260</td>\n",
       "      <td>False</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Vicky_Austin</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             ID                                               Text Pronoun  \\\n",
       "1995  test-1996  The sole exception was Wimbledon, where she pl...     She   \n",
       "1996  test-1997  According to news reports, both Moore and Fily...     her   \n",
       "1997  test-1998  In June 2009, due to the popularity of the Sab...     She   \n",
       "1998  test-1999  She was delivered to the Norwegian passenger s...     she   \n",
       "1999  test-2000  Meg and Vicky each have three siblings, and ha...     her   \n",
       "\n",
       "      Pronoun-offset                   A  A-offset  A-coref  \\\n",
       "1995             479    Goolagong Cawley       400     True   \n",
       "1996             338  Esther Sheryl Wood       263     True   \n",
       "1997             328               Kayla       364     True   \n",
       "1998             305                Irma       255     True   \n",
       "1999             275        Vicky Austin       217     True   \n",
       "\n",
       "                       B  B-offset  B-coref  \\\n",
       "1995        Peggy Michel       432    False   \n",
       "1996      Barbara Morgan       404    False   \n",
       "1997  Natasha Henstridge       412    False   \n",
       "1998              Bergen       274    False   \n",
       "1999       Polly O'Keefe       260    False   \n",
       "\n",
       "                                                    URL  \n",
       "1995  http://en.wikipedia.org/wiki/Evonne_Goolagong_...  \n",
       "1996  http://en.wikipedia.org/wiki/Hastings_Arthur_Wise  \n",
       "1997          http://en.wikipedia.org/wiki/Raya_Meddine  \n",
       "1998        http://en.wikipedia.org/wiki/SS_Irma_(1905)  \n",
       "1999          http://en.wikipedia.org/wiki/Vicky_Austin  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d60addb918fc181bd736f5497c3114dbf3d3bfd4"
   },
   "source": [
    "# Explore Features for Building Mention-Pair Distributed Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "8b82c9f910c289c5b21ec982890911bfb579c32b"
   },
   "outputs": [],
   "source": [
    "spacy_model = \"en_core_web_lg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "177d5f8cca6584d7e542ea1ed6112e091e78d787"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.en import English\n",
    "from spacy.pipeline import DependencyParser\n",
    "import spacy\n",
    "from nltk import Tree\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing import text as ktext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.3'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "1668e73e26b428fa0739c1c38653217d14d50aaa"
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load(spacy_model)\n",
    "\n",
    "def bs(list_, target_):\n",
    "    lo, hi = 0, len(list_) -1\n",
    "    \n",
    "    while lo < hi:\n",
    "        mid = lo + int((hi - lo) / 2)\n",
    "        \n",
    "        if target_ < list_[mid]:\n",
    "            hi = mid\n",
    "        elif target_ > list_[mid]:\n",
    "            lo = mid + 1\n",
    "        else:\n",
    "            return mid + 1\n",
    "    return lo\n",
    "\n",
    "def bs_(list_, target_):\n",
    "    lo, hi = 0, len(list_) -1\n",
    "    \n",
    "    while lo < hi:\n",
    "        mid = lo + int((hi - lo) / 2)\n",
    "        \n",
    "        if target_ < list_[mid]:\n",
    "            hi = mid\n",
    "        elif target_ > list_[mid]:\n",
    "            lo = mid + 1\n",
    "        else:\n",
    "            return mid\n",
    "    return lo\n",
    "\n",
    "def ohe_dist(dist, buckets):\n",
    "    idx = bs_(buckets, dist)\n",
    "    oh = np.zeros(shape=(len(buckets),), dtype=np.float32)\n",
    "    oh[idx] = 1\n",
    "    \n",
    "    return oh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2755927ba88262427347c567ae777c92510ffa68"
   },
   "source": [
    " ##  Position Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "15165f2b90503a90de589b37558ebcea28daa867"
   },
   "source": [
    "Encode the absolute positions in the sentence and the relative position between the pronoun and the entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "c1c14436f57e74e80ac4425f77df98768c71f7b9"
   },
   "outputs": [],
   "source": [
    "num_pos_features = 45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "9f3d21e9a626a3290c81c6bc98b817c3e47275be"
   },
   "outputs": [],
   "source": [
    "def extrac_positional_features(text, char_offset1, char_offset2):\n",
    "    doc = nlp(text)\n",
    "    max_len = 64\n",
    "    \n",
    "    # char offset to token offset\n",
    "    lens = [token.idx for token in doc]\n",
    "    mention_offset1 = bs(lens, char_offset1) - 1\n",
    "    mention_offset2 = bs(lens, char_offset2) - 1\n",
    "    \n",
    "    # token offset to sentence offset\n",
    "    lens = [len(sent) for sent in doc.sents]\n",
    "    acc_lens = [len_ for len_ in lens]\n",
    "    pre_len = 0\n",
    "    for i in range(0, len(acc_lens)):\n",
    "        pre_len += acc_lens[i]\n",
    "        acc_lens[i] = pre_len\n",
    "    sent_index1 = bs(acc_lens, mention_offset1)\n",
    "    sent_index2 = bs(acc_lens, mention_offset2)\n",
    "    \n",
    "    sent1 = list(doc.sents)[sent_index1]\n",
    "    sent2 = list(doc.sents)[sent_index2]\n",
    "    \n",
    "    # buckets\n",
    "    bucket_dist = [1, 2, 3, 4, 5, 8, 16, 32, 64]\n",
    "    \n",
    "    # relative distance\n",
    "    dist = mention_offset2 - mention_offset1\n",
    "    dist_oh = ohe_dist(dist, bucket_dist)\n",
    "    \n",
    "    # buckets\n",
    "    bucket_pos = [0, 1, 2, 3, 4, 5, 8, 16, 32]\n",
    "    \n",
    "    # absolute position in the sentence\n",
    "    sent_pos1 = mention_offset1 + 1\n",
    "    if sent_index1 > 0:\n",
    "        sent_pos1 = mention_offset1 - acc_lens[sent_index1-1]\n",
    "    sent_pos_oh1 = ohe_dist(sent_pos1, bucket_pos)\n",
    "    sent_pos_inv1 = len(sent1) - sent_pos1\n",
    "    assert sent_pos_inv1 >= 0\n",
    "    sent_pos_inv_oh1 = ohe_dist(sent_pos_inv1, bucket_pos)\n",
    "    \n",
    "    sent_pos2 = mention_offset2 + 1\n",
    "    if sent_index2 > 0:\n",
    "        sent_pos2 = mention_offset2 - acc_lens[sent_index2-1]\n",
    "    sent_pos_oh2 = ohe_dist(sent_pos2, bucket_pos)\n",
    "    sent_pos_inv2 = len(sent2) - sent_pos2\n",
    "    if sent_pos_inv2 < 0:\n",
    "        print(sent_pos_inv2)\n",
    "        print(len(sent2))\n",
    "        print(sent_pos2)\n",
    "        raise ValueError\n",
    "    sent_pos_inv_oh2 = ohe_dist(sent_pos_inv2, bucket_pos)\n",
    "    \n",
    "    sent_pos_ratio1 = sent_pos1 / len(sent1)\n",
    "    sent_pos_ratio2 = sent_pos2 / len(sent2)\n",
    "    \n",
    "    return dist_oh, sent_pos_oh1, sent_pos_oh2, sent_pos_inv_oh1, sent_pos_inv_oh2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "4a9ec346bff260b62ebea6d9223c6a11e683a9ee"
   },
   "outputs": [],
   "source": [
    "def create_dist_features(df, text_column, pronoun_offset_column, name_offset_column):\n",
    "    text_offset_list = df[[text_column, pronoun_offset_column, name_offset_column]].values.tolist()\n",
    "    num_features = num_pos_features\n",
    "    \n",
    "    pos_feature_matrix = np.zeros(shape=(len(text_offset_list), num_features))\n",
    "    for text_offset_index in range(len(text_offset_list)):\n",
    "        text_offset = text_offset_list[text_offset_index]\n",
    "        dist_oh, sent_pos_oh1, sent_pos_oh2, sent_pos_inv_oh1, sent_pos_inv_oh2 = extrac_positional_features(text_offset[0], text_offset[1], text_offset[2])\n",
    "        \n",
    "        feature_index = 0\n",
    "        pos_feature_matrix[text_offset_index, feature_index:feature_index+len(dist_oh)] = np.asarray(dist_oh)\n",
    "        feature_index += len(dist_oh)\n",
    "        pos_feature_matrix[text_offset_index, feature_index:feature_index+len(sent_pos_oh1)] = np.asarray(sent_pos_oh1)\n",
    "        feature_index += len(sent_pos_oh1)\n",
    "        pos_feature_matrix[text_offset_index, feature_index:feature_index+len(sent_pos_oh2)] = np.asarray(sent_pos_oh2)\n",
    "        feature_index += len(sent_pos_oh2)\n",
    "        pos_feature_matrix[text_offset_index, feature_index:feature_index+len(sent_pos_inv_oh1)] = np.asarray(sent_pos_inv_oh1)\n",
    "        feature_index += len(sent_pos_inv_oh1)\n",
    "        pos_feature_matrix[text_offset_index, feature_index:feature_index+len(sent_pos_inv_oh2)] = np.asarray(sent_pos_inv_oh2)\n",
    "        feature_index += len(sent_pos_inv_oh2)\n",
    "    \n",
    "    return pos_feature_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "488aaec745f7f6eea342a5746bcf7bef206c60d3"
   },
   "source": [
    "## Extract Sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "51de0d6b4223279969593956248e0fb99284c5d7"
   },
   "source": [
    "Select the surrounding 100 words around the mention in the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "2edcfef36582c91246e7ab772d5a23c27a272f92"
   },
   "outputs": [],
   "source": [
    "max_len = 50 # longer than 99% of the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "11af138a12f54e9c0a8ce92a782383c9ad5a92ff"
   },
   "outputs": [],
   "source": [
    "seq_list = list()\n",
    "def extract_sents(text, char_offset_p, char_offset_a, char_offset_b, id):\n",
    "    global max_len\n",
    "    global seq_list\n",
    "    \n",
    "    seq_list.append(list())\n",
    "    \n",
    "    doc = nlp(text)\n",
    "    token_lens = [token.idx for token in doc]\n",
    "    \n",
    "    char_offsets = [char_offset_p, char_offset_a, char_offset_b]\n",
    "    sent_list = list()\n",
    "    \n",
    "    for char_offset in char_offsets:\n",
    "        # char offset to token offset\n",
    "        mention_offset = bs(token_lens, char_offset) - 1\n",
    "        # mention_word\n",
    "        mention = doc[mention_offset]\n",
    "    \n",
    "        # token offset to sentence offset\n",
    "        lens = [len(sent) for sent in doc.sents]\n",
    "        acc_lens = [len_ for len_ in lens]\n",
    "        pre_len = 0\n",
    "        for i in range(0, len(acc_lens)):\n",
    "            pre_len += acc_lens[i]\n",
    "            acc_lens[i] = pre_len\n",
    "        sent_index = bs(acc_lens, mention_offset)\n",
    "        # mention sentence\n",
    "        sent = list(doc.sents)[sent_index]\n",
    "        \n",
    "        # absolute position in the sentence\n",
    "        sent_pos = mention_offset + 1\n",
    "        if sent_index > 0:\n",
    "            sent_pos = mention_offset - acc_lens[sent_index-1]\n",
    "        \n",
    "        # clip the sentence if it is longer than max length\n",
    "        if len(sent) > max_len:\n",
    "            # make sure the mention is in the sentence span\n",
    "            if sent_pos < max_len-1:\n",
    "                sent_list.append(sent[0:max_len].text)\n",
    "                sent_list.append(sent_pos)\n",
    "                seq_list[-1].append(sent[0:max_len])\n",
    "            else:\n",
    "                sent_list.append(sent[sent_pos-max_len+2 : min(sent_pos+2, len(sent))].text)\n",
    "                sent_list.append(max_len-2)\n",
    "                seq_list[-1].append(sent[sent_pos-max_len+2 : min(sent_pos+2, len(sent))])\n",
    "        else:\n",
    "            sent_list.append(sent.text)\n",
    "            sent_list.append(sent_pos)\n",
    "            seq_list[-1].append(sent)\n",
    "        \n",
    "    return pd.Series([id] + sent_list, index=['ID', 'Pronoun-Sent', 'Pronoun-Sent-Offset', 'A-Sent', 'A-Sent-Offset', 'B-Sent', 'B-Sent-Offset'])\n",
    "\n",
    "def add_sent_columns(df, text_column, pronoun_offset_column, a_offset_column, b_offset_column):\n",
    "    global seq_list\n",
    "    seq_list = list()\n",
    "    sent_df = df.apply(lambda row: extract_sents(row.loc[text_column], row[pronoun_offset_column], row[a_offset_column], row[b_offset_column], row['ID']), axis=1)\n",
    "    df = df.join(sent_df.set_index('ID'), on='ID')\n",
    "    return df, seq_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9c0630e230a389ab7ba9bc4bc8140cabf769f5ff"
   },
   "source": [
    "## Create Train, Dev and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "f455cc1f3150d1dcc42a3aef2434c8d5e703c990"
   },
   "outputs": [],
   "source": [
    "seq_list = list()\n",
    "train_df, train_tokenized = add_sent_columns(train_df, 'Text', 'Pronoun-offset', 'A-offset', 'B-offset')\n",
    "seq_list = list()\n",
    "test_df, test_tokenized = add_sent_columns(test_df, 'Text', 'Pronoun-offset', 'A-offset', 'B-offset')\n",
    "seq_list = list()\n",
    "dev_df, dev_tokenized = add_sent_columns(dev_df, 'Text', 'Pronoun-offset', 'A-offset', 'B-offset')\n",
    "\n",
    "# df apply will call the first row twice, remove the first one\n",
    "train_tokenized = train_tokenized[1:]\n",
    "test_tokenized = test_tokenized[1:]\n",
    "dev_tokenized = dev_tokenized[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2873a775c7be71521ef5b8922a61f11a93e62b57"
   },
   "source": [
    "### Create Vocab and Embedding Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_pronouns = [\"He\", \"Him\", \"His\", \"Himself\", \"he\", \"him\", \"his\", \"himself\",\n",
    "                  \"She\", \"Her\", \"Hers\", \"Herself\", \"she\", \"her\", \"hers\", \"herself\",\n",
    "                  \"It\", \"It\", \"Its\", \"Itself\", \"it\", \"it\", \"its\",  \"itself\",\n",
    "                  \"One\", \"One\", \"One's\", \"Oneself\", \"one\", \"one\", \"one's\",  \"oneself\",\n",
    "                  \"They\", \"Them\", \"Theirs\", \"Themself\", \"they\", \"them\", \"theirs\",  \"themself\",\n",
    "                  \"Who\", \"Whom\", \"Whose\", \"Themself\", \"who\", \"whom\", \"whose\",  \"themself\",\n",
    "                  \"Ey\", \"Em\", \"Eirs\", \"Emself\", \"ey\", \"em\", \"eirs\",  \"emself\",\n",
    "                  \"Ne\", \"Nem\", \"Nirs\", \"Nemself\", \"ne\", \"nem\", \"nirs\",  \"nemself\",\n",
    "                  \"His\", \"his\",\n",
    "                  \"Her\", \"her\",\n",
    "                  \"Its\", \"its\",\n",
    "                  \"One's\", \"one's\",\n",
    "                  \"Their\", \"their\",\n",
    "                  \"Whose\", \"whose\",\n",
    "                  \"Eir\", \"eir\",\n",
    "                  \"Nir\", \"nir\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Surprisingly enough, the spacy vocab from en_core_web_lg contains all/most of the engineered gender-neutral pronouns\n",
    "nlp.vocab.has_vector(\"nir\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_uuid": "2d34b3f5fdce49c97e9fb9d153126546465b3965"
   },
   "outputs": [],
   "source": [
    "embed_size = 300\n",
    "max_features = 80000\n",
    "\n",
    "# generate word index\n",
    "word_index = dict()\n",
    "idx = 1\n",
    "for text_ in train_tokenized+test_tokenized+dev_tokenized:\n",
    "    for sent_ in text_:\n",
    "        for word_ in sent_:\n",
    "            if word_.text not in word_index and nlp.vocab.has_vector(word_.text):\n",
    "                word_index[word_.text] = idx\n",
    "                idx += 1\n",
    "\n",
    "# Additional words to accound for gendered and gender neutral pronouns (in case these are removed from test dataset)\n",
    "for pronoun in extra_pronouns:\n",
    "    if pronoun not in word_index and nlp.vocab.has_vector(pronoun):\n",
    "        word_index[pronoun] = idx\n",
    "        idx += 1\n",
    "# print(word_index)\n",
    "\n",
    "nb_words = min(max_features, len(word_index))\n",
    "embedding_matrix = np.zeros((nb_words + 1, embed_size))\n",
    "        \n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features: continue\n",
    "    embedding_vector = None\n",
    "    if nlp.vocab.has_vector(word):\n",
    "        embedding_vector = nlp.vocab.vectors[nlp.vocab.strings[word]]\n",
    "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "#print(embedding_matrix.shape)\n",
    "\n",
    "# generate pos tag index\n",
    "pos_index = dict()\n",
    "#pos_label_index = dict()\n",
    "idx = 1\n",
    "for text_ in train_tokenized+test_tokenized+dev_tokenized:\n",
    "    for sent_ in text_:\n",
    "        for word_ in sent_:\n",
    "            if word_.pos not in pos_index:\n",
    "                pos_index[word_.pos] = idx\n",
    "                #pos_label_index[word_.pos_] = idx\n",
    "                idx += 1\n",
    "                \n",
    "# No need to add new pos labels, all labels are found even in Spivak-altered datasets\n",
    "#print(pos_label_index)\n",
    "\n",
    "def sentences_to_sequences(tokenized_):\n",
    "    return list(map(\n",
    "        lambda sent_tokenized: list(map(\n",
    "            lambda token_: word_index[token_.text] if token_.text in word_index else 0,\n",
    "            sent_tokenized\n",
    "        )),\n",
    "        tokenized_\n",
    "    ))\n",
    "\n",
    "def poses_to_sequences(tokenized_):\n",
    "    return list(map(\n",
    "        lambda sent_tokenized: list(map(\n",
    "            lambda token_: pos_index[token_.pos] if token_.pos in pos_index else 0,\n",
    "            sent_tokenized\n",
    "        )),\n",
    "        tokenized_\n",
    "    ))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_uuid": "41ca3b1434037a8f7296850ba4af9c837e247368"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x270dd294780>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.distplot(train_df['Pronoun-Sent'].map(lambda ele: len(ele.split(\" \"))), kde_kws={\"label\": \"P\"})\n",
    "\n",
    "sns.distplot(train_df['A-Sent'].map(lambda ele: len(ele.split(\" \"))), kde_kws={\"label\": \"A\"})\n",
    "\n",
    "sns.distplot(train_df['B-Sent'].map(lambda ele: len(ele.split(\" \"))), kde_kws={\"label\": \"B\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_uuid": "35dda1595e80bed9e310dd5af9b89b726a878e8b"
   },
   "outputs": [],
   "source": [
    "train_p_tokenized = sentences_to_sequences([row[0] for row in train_tokenized])\n",
    "train_a_tokenized = sentences_to_sequences([row[1] for row in train_tokenized])\n",
    "train_b_tokenized = sentences_to_sequences([row[2] for row in train_tokenized])\n",
    "\n",
    "dev_p_tokenized = sentences_to_sequences([row[0] for row in dev_tokenized])\n",
    "dev_a_tokenized = sentences_to_sequences([row[1] for row in dev_tokenized])\n",
    "dev_b_tokenized = sentences_to_sequences([row[2] for row in dev_tokenized])\n",
    "\n",
    "seq_p_train = sequence.pad_sequences(train_p_tokenized, maxlen = max_len, padding='post')\n",
    "seq_a_train = sequence.pad_sequences(train_a_tokenized, maxlen = max_len, padding='post')\n",
    "seq_b_train = sequence.pad_sequences(train_b_tokenized, maxlen = max_len, padding='post')\n",
    "\n",
    "seq_p_dev = sequence.pad_sequences(dev_p_tokenized, maxlen = max_len, padding='post')\n",
    "seq_a_dev = sequence.pad_sequences(dev_a_tokenized, maxlen = max_len, padding='post')\n",
    "seq_b_dev = sequence.pad_sequences(dev_b_tokenized, maxlen = max_len, padding='post')\n",
    "\n",
    "train_p_pos = poses_to_sequences([row[0] for row in train_tokenized])\n",
    "train_a_pos = poses_to_sequences([row[1] for row in train_tokenized])\n",
    "train_b_pos = poses_to_sequences([row[2] for row in train_tokenized])\n",
    "\n",
    "dev_p_pos = poses_to_sequences([row[0] for row in dev_tokenized])\n",
    "dev_a_pos = poses_to_sequences([row[1] for row in dev_tokenized])\n",
    "dev_b_pos = poses_to_sequences([row[2] for row in dev_tokenized])\n",
    "\n",
    "pos_p_train = sequence.pad_sequences(train_p_pos, maxlen = max_len, padding='post')\n",
    "pos_a_train = sequence.pad_sequences(train_a_pos, maxlen = max_len, padding='post')\n",
    "pos_b_train = sequence.pad_sequences(train_b_pos, maxlen = max_len, padding='post')\n",
    "\n",
    "pos_p_dev = sequence.pad_sequences(dev_p_pos, maxlen = max_len, padding='post')\n",
    "pos_a_dev = sequence.pad_sequences(dev_a_pos, maxlen = max_len, padding='post')\n",
    "pos_b_dev = sequence.pad_sequences(dev_b_pos, maxlen = max_len, padding='post')\n",
    "\n",
    "index_p_train = train_df['Pronoun-Sent-Offset'].values\n",
    "index_a_train = train_df['A-Sent-Offset'].values\n",
    "index_b_train = train_df['B-Sent-Offset'].values\n",
    "\n",
    "index_p_dev = dev_df['Pronoun-Sent-Offset'].values\n",
    "index_a_dev = dev_df['A-Sent-Offset'].values\n",
    "index_b_dev = dev_df['B-Sent-Offset'].values\n",
    "\n",
    "pa_pos_tra = create_dist_features(train_df, 'Text', 'Pronoun-offset', 'A-offset')\n",
    "pa_pos_dev = create_dist_features(dev_df, 'Text', 'Pronoun-offset', 'A-offset')\n",
    "\n",
    "pb_pos_tra = create_dist_features(train_df, 'Text', 'Pronoun-offset', 'B-offset')\n",
    "pb_pos_dev = create_dist_features(dev_df, 'Text', 'Pronoun-offset', 'B-offset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_uuid": "f46a80420d56262e4fed998d16f9f9105d502c02"
   },
   "outputs": [],
   "source": [
    "X_train = [seq_p_train, seq_a_train, seq_b_train, pos_p_train, pos_a_train, pos_b_train, index_p_train, index_a_train, index_b_train, pa_pos_tra, pb_pos_tra]\n",
    "X_dev = [seq_p_dev, seq_a_dev, seq_b_dev, pos_p_dev, pos_a_dev, pos_b_dev, index_p_dev, index_a_dev, index_b_dev, pa_pos_dev, pb_pos_dev]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_uuid": "2842410a113a578c1c274630b195038af0dfd75c"
   },
   "outputs": [],
   "source": [
    "def _row_to_y(row):\n",
    "    if row.loc['A-coref']:\n",
    "        return 0\n",
    "    if row.loc['B-coref']:\n",
    "        return 1\n",
    "    return 2\n",
    "\n",
    "y_tra = train_df.apply(_row_to_y, axis=1)\n",
    "y_dev = dev_df.apply(_row_to_y, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "33ee19bc3e355ea58dd27bd5c373a13682536dd4"
   },
   "source": [
    "# Define Keras Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_uuid": "cb3cacc8e7dc332a6b5cacf3d733f314b7f03cba"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras import backend\n",
    "from keras import layers\n",
    "from keras import models\n",
    "\n",
    "from keras import initializers, regularizers, constraints, activations\n",
    "from keras.engine import Layer\n",
    "import keras.backend as K\n",
    "from keras.layers import merge\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_uuid": "3830a23390eaf527992b3676e03857699969957e"
   },
   "outputs": [],
   "source": [
    "def _dot_product(x, kernel):\n",
    "    \"\"\"\n",
    "    Wrapper for dot product operation, in order to be compatible with both\n",
    "    Theano and Tensorflow\n",
    "    Args:\n",
    "        x (): input\n",
    "        kernel (): weights\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    if K.backend() == 'tensorflow':\n",
    "        # todo: check that this is correct\n",
    "        return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n",
    "    else:\n",
    "        return K.dot(x, kernel)\n",
    "    \n",
    "    \n",
    "class AttentionWeight(Layer):\n",
    "    \"\"\"\n",
    "        This code is a modified version of cbaziotis implementation:  GithubGist cbaziotis/AttentionWithContext.py\n",
    "        Attention operation, with a context/query vector, for temporal data.\n",
    "        Supports Masking.\n",
    "        Follows the work of Yang et al. [https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf]\n",
    "        \"Hierarchical Attention Networks for Document Classification\"\n",
    "        by using a context vector to assist the attention\n",
    "        # Input shape\n",
    "            3D tensor with shape: `(samples, steps, features)`.\n",
    "        # Output shape\n",
    "            2D tensor with shape: `(samples, steps)`.\n",
    "        :param kwargs:\n",
    "        Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
    "        The dimensions are inferred based on the output shape of the RNN.\n",
    "        Example:\n",
    "            model.add(LSTM(64, return_sequences=True))\n",
    "            model.add(AttentionWeight())\n",
    "        \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        super(AttentionWeight, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        shape1 = input_shape[0]\n",
    "        shape2 = input_shape[1]\n",
    "\n",
    "        self.W = self.add_weight((shape2[-1], shape1[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((shape2[-1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        x = inputs[0]\n",
    "        u = inputs[1]\n",
    "        \n",
    "        uit = _dot_product(x, self.W)\n",
    "\n",
    "        if self.bias:\n",
    "            uit += self.b\n",
    "\n",
    "        uit = K.tanh(uit)\n",
    "        ait = K.batch_dot(uit, u)\n",
    "\n",
    "        a = K.exp(ait)\n",
    "\n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\n",
    "        # and this results in NaN's. A workaround is to add a very small positive number ε to the sum.\n",
    "        # a /= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx())\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "        \n",
    "        return a\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        if not isinstance(input_shape, list) or len(input_shape) != 2:\n",
    "            raise ValueError('A `Dot` layer should be called '\n",
    "                             'on a list of 2 inputs.')\n",
    "        shape1 = list(input_shape[0])\n",
    "        shape2 = list(input_shape[1])\n",
    "        \n",
    "        return shape1[0], shape1[1]\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'W_regularizer': regularizers.serialize(self.W_regularizer),\n",
    "            'b_regularizer': regularizers.serialize(self.b_regularizer),\n",
    "            'W_constraint': constraints.serialize(self.W_constraint),\n",
    "            'b_constraint': constraints.serialize(self.b_constraint),\n",
    "            'bias': self.bias\n",
    "        }\n",
    "        base_config = super(AttentionWeight, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "    \n",
    "    \n",
    "class FeatureSelection1D(Layer):\n",
    "    \"\"\"\n",
    "        Normalize feature along a specific axis.\n",
    "        Supports Masking.\n",
    "\n",
    "        # Input shape\n",
    "            A ND tensor with shape: `(samples, timesteps, features)\n",
    "            A 2D tensor with shape: [samples, num_selected_features]\n",
    "        # Output shape\n",
    "            ND tensor with shape: `(samples, num_selected_features, features)`.\n",
    "        :param kwargs:\n",
    "        \"\"\"\n",
    "\n",
    "    def __init__(self, num_selects, **kwargs):\n",
    "\n",
    "        self.num_selects = num_selects\n",
    "        self.supports_masking = True\n",
    "        super(FeatureSelection1D, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "\n",
    "        super(FeatureSelection1D, self).build(input_shape)\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # don't pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        if not isinstance(inputs, list) or len(inputs) != 2:\n",
    "            raise ValueError('FeatureSelection1D layer should be called '\n",
    "                             'on a list of 2 inputs.')\n",
    "\n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            a = K.cast(mask, K.floatx()) * inputs[0]\n",
    "        else:\n",
    "            a = inputs[0]\n",
    "\n",
    "        b = inputs[1]\n",
    "\n",
    "        a = tf.batch_gather(\n",
    "            a, b\n",
    "        )\n",
    "\n",
    "        return a\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        if not isinstance(input_shape, list) or len(input_shape) != 2:\n",
    "            raise ValueError('A `FeatureSelection1D` layer should be called '\n",
    "                             'on a list of 2 inputs.')\n",
    "        shape1 = list(input_shape[0])\n",
    "        shape2 = list(input_shape[1])\n",
    "\n",
    "        if shape2[0] != shape1[0]:\n",
    "            raise ValueError(\"batch size must be same\")\n",
    "\n",
    "        if shape2[1] != self.num_selects:\n",
    "            raise ValueError(\"must conform to the num_select\")\n",
    "\n",
    "        return (shape1[0], self.num_selects, shape1[2])\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'num_selects': self.num_selects\n",
    "        }\n",
    "        base_config = super(FeatureSelection1D, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "bf87bf600dac037777be8e3dda560ec65b6640a2"
   },
   "source": [
    "# Build and Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_uuid": "03e8297383ee57a4a23c707a4a6c1cca13da1868"
   },
   "outputs": [],
   "source": [
    "from keras import callbacks as kc\n",
    "from keras import optimizers as ko\n",
    "from keras import initializers, regularizers, constraints\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import SVG\n",
    "\n",
    "histories = list()\n",
    "file_paths = list()\n",
    "cos = list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5503bd66a2a7fea5bcb093c775d69baded3e759d"
   },
   "source": [
    "## End-to-End RNN Attention Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5a954b42159bc4a8a17b4e15083f026f53362a43"
   },
   "source": [
    "### Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "_uuid": "8feec4878e5b817dcbf5ed7f49d9b78cce36ebd6"
   },
   "outputs": [],
   "source": [
    "def build_e2e_birnn_attention_model(\n",
    "        voca_dim, time_steps, pos_tag_size, pos_tag_dim, extra_feature_dims,\n",
    "        output_dim, rnn_dim, model_dim, mlp_dim,\n",
    "        item_embedding=None, rnn_depth=1, mlp_depth=1,\n",
    "        drop_out=0.5, rnn_drop_out=0., rnn_state_drop_out=0.,\n",
    "        trainable_embedding=False, gpu=False, return_customized_layers=False):\n",
    "    \"\"\"\n",
    "    Create A End-to-End Bidirectional RNN Attention Model.\n",
    "\n",
    "    :param voca_dim: vocabulary dimension size.\n",
    "    :param time_steps: the length of input\n",
    "    :param extra_feature_dims: the dimention size of the auxilary feature\n",
    "    :param output_dim: the output dimension size\n",
    "    :param model_dim: rrn dimension size\n",
    "    :param mlp_dim: the dimension size of fully connected layer\n",
    "    :param item_embedding: integer, numpy 2D array, or None (default=None)\n",
    "        If item_embedding is a integer, connect a randomly initialized embedding matrix to the input tensor.\n",
    "        If item_embedding is a matrix, this matrix will be used as the embedding matrix.\n",
    "        If item_embedding is None, then connect input tensor to RNN layer directly.\n",
    "    :param rnn_depth: rnn depth\n",
    "    :param mlp_depth: the depth of fully connected layers\n",
    "    :param drop_out: dropout rate of fully connected layers\n",
    "    :param rnn_drop_out: dropout rate of rnn layers\n",
    "    :param rnn_state_drop_out: dropout rate of rnn state tensor\n",
    "    :param trainable_embedding: boolean\n",
    "    :param gpu: boolean, default=False\n",
    "        If True, CuDNNLSTM is used instead of LSTM for RNN layer.\n",
    "    :param return_customized_layers: boolean, default=False\n",
    "        If True, return model and customized object dictionary, otherwise return model only\n",
    "    :return: keras model\n",
    "    \"\"\"\n",
    "    \n",
    "    # sequences inputs\n",
    "    if item_embedding is not None:\n",
    "        inputp = models.Input(shape=(time_steps,), dtype='int32', name='inputp')\n",
    "        inputa = models.Input(shape=(time_steps,), dtype='int32', name='inputa')\n",
    "        inputb = models.Input(shape=(time_steps,), dtype='int32', name='inputb')\n",
    "        inputs = [inputp, inputa, inputb]\n",
    "        \n",
    "        if isinstance(item_embedding, np.ndarray):\n",
    "            assert voca_dim == item_embedding.shape[0]\n",
    "            embed_dim = item_embedding.shape[1]\n",
    "            emb_layer = layers.Embedding(\n",
    "                voca_dim, item_embedding.shape[1], input_length=time_steps,\n",
    "                weights=[item_embedding, ], trainable=trainable_embedding,\n",
    "                mask_zero=False, name='embedding_layer0'\n",
    "            )\n",
    "        elif utils.is_integer(item_embedding):\n",
    "            embed_dim = item_embedding\n",
    "            emb_layer = layers.Embedding(\n",
    "                voca_dim, item_embedding, input_length=time_steps,\n",
    "                trainable=trainable_embedding,\n",
    "                mask_zero=False, name='embedding_layer0'\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\"item_embedding must be either integer or numpy matrix\")\n",
    "\n",
    "        xs = list(map(\n",
    "            lambda input_: emb_layer(input_),\n",
    "            inputs\n",
    "        ))\n",
    "    else:\n",
    "        inputp = models.Input(shape=(time_steps, voca_dim), dtype='float32', name='inputp')\n",
    "        inputa = models.Input(shape=(time_steps, voca_dim), dtype='float32', name='inputa')\n",
    "        inputb = models.Input(shape=(time_steps, voca_dim), dtype='float32', name='inputb')\n",
    "        embed_dim = voca_dim\n",
    "        xs = [inputp, inputa, inputb]\n",
    "        \n",
    "    # pos tag\n",
    "    inputposp = models.Input(shape=(time_steps,), dtype='int32', name='inputposp')\n",
    "    inputposa = models.Input(shape=(time_steps,), dtype='int32', name='inputposa')\n",
    "    inputposb = models.Input(shape=(time_steps,), dtype='int32', name='inputposb')\n",
    "    inputpos = [inputposp, inputposa, inputposb]\n",
    "    pos_emb_layer = layers.Embedding(\n",
    "        pos_tag_size, pos_tag_dim, input_length=time_steps,\n",
    "        trainable=True, mask_zero=False, name='pos_embedding_layer0'\n",
    "    )\n",
    "    xpos = list(map(\n",
    "        lambda input_: pos_emb_layer(input_),\n",
    "        inputpos\n",
    "    ))\n",
    "    \n",
    "    embed_concate_layer = layers.Concatenate(axis=2, name=\"embed_concate_layer\")\n",
    "    for i in range(len(xs)):\n",
    "        xs[i] = embed_concate_layer([xs[i], xpos[i]])\n",
    "    \n",
    "    # mention position in the sentence\n",
    "    inputpi = models.Input(shape=(1,), dtype='int32', name='inputpi')\n",
    "    inputai = models.Input(shape=(1,), dtype='int32', name='inputai')\n",
    "    inputbi = models.Input(shape=(1,), dtype='int32', name='inputbi')\n",
    "    xis = [inputpi, inputai, inputbi]\n",
    "    \n",
    "    # addtional mention-pair features\n",
    "    inputpa = models.Input(shape=(extra_feature_dims,), dtype='float32', name='inputpa')\n",
    "    inputpb = models.Input(shape=(extra_feature_dims,), dtype='float32', name='inputpb')\n",
    "    xextrs = [inputpa, inputpb]\n",
    "    \n",
    "    # rnn\n",
    "    birnns = list()\n",
    "    rnn_batchnorms = list()\n",
    "    rnn_dropouts = list()\n",
    "    if gpu:\n",
    "        # rnn encoding\n",
    "        for i in range(rnn_depth):\n",
    "            rnn_dropout = layers.SpatialDropout1D(rnn_drop_out)\n",
    "            birnn = layers.Bidirectional(\n",
    "                layers.CuDNNGRU(rnn_dim, return_sequences=True),\n",
    "                name='bi_lstm_layer' + str(i))\n",
    "            rnn_batchnorm = layers.BatchNormalization(name='rnn_batch_norm_layer' + str(i))\n",
    "            \n",
    "            birnns.append(birnn)\n",
    "            rnn_dropouts.append(rnn_dropout)\n",
    "            rnn_batchnorms.append(rnn_batchnorm)\n",
    "        \n",
    "        xs_ = list()\n",
    "        for x_ in xs:\n",
    "            for i in range(len(birnns)):\n",
    "                x_ = rnn_dropouts[i](x_)\n",
    "                x_ = birnns[i](x_)\n",
    "                x_ = rnn_batchnorms[i](x_)\n",
    "            xs_.append(x_)\n",
    "        xs = xs_\n",
    "    else:\n",
    "        # rnn encoding\n",
    "        for i in range(rnn_depth):\n",
    "            birnn = layers.Bidirectional(\n",
    "                layers.GRU(rnn_dim, return_sequences=True, dropout=rnn_drop_out,\n",
    "                            recurrent_dropout=rnn_state_drop_out),\n",
    "                name='bi_lstm_layer' + str(i))\n",
    "            rnn_batchnorm = layers.BatchNormalization(name='rnn_batch_norm_layer' + str(i))\n",
    "            \n",
    "            birnns.append(birnn)\n",
    "            rnn_batchnorms.append(rnn_batchnorm)\n",
    "            \n",
    "        xs_ = list()\n",
    "        for x_ in xs:\n",
    "            for i in range(len(birnns)):\n",
    "                x_ = birnns[i](x_)\n",
    "                x_ = rnn_batchnorms[i](x_)\n",
    "            xs_.append(x_)\n",
    "        xs = xs_\n",
    "    \n",
    "    # attention aggregated rnn embedding + mention rnn embedding + mention-pair features\n",
    "    select_layer = FeatureSelection1D(1, name='boundary_selection_layer')\n",
    "    flatten_layer1 = layers.Flatten('channels_first', name=\"flatten_layer1\")\n",
    "    permute_layer = layers.Permute((2, 1), name='permuted_attention_x')\n",
    "    attent_weight = AttentionWeight(name=\"attention_weight\")\n",
    "    focus_layer = layers.Dot([2, 1], name='focus' + '_layer')\n",
    "    reshape_layer = layers.Reshape((1, rnn_dim*2), name=\"reshape_layer\")\n",
    "    concate_layer = layers.Concatenate(axis=1, name=\"attention_concate_layer\")\n",
    "    atten_dropout_layer = layers.Dropout(drop_out, name='attention_dropout_layer')\n",
    "    map_layer1 = layers.Dense(model_dim, activation=\"relu\", name=\"map_layer1\")\n",
    "    #map_layer2 = layers.TimeDistributed(layers.Dense(model_dim, activation=\"relu\"), name=\"map_layer2\")\n",
    "    map_layer2 = map_layer1\n",
    "    flatten_layer = layers.Flatten('channels_first', name=\"flatten_layer\")\n",
    "    for i in range(len(xs)):\n",
    "        if i == 0:\n",
    "            map_layer = map_layer1\n",
    "        else:\n",
    "            map_layer = map_layer2\n",
    "            \n",
    "        select_ = select_layer([xs[i], xis[i]])\n",
    "        flatten_select_ = flatten_layer1(select_)\n",
    "        att = attent_weight([xs[i], flatten_select_])\n",
    "        \n",
    "        focus = focus_layer([permute_layer(xs[i]), att])\n",
    "        xs[i] = concate_layer([select_, reshape_layer(focus)])\n",
    "        xs[i] = flatten_layer(xs[i])\n",
    "        xs[i] = atten_dropout_layer(xs[i])\n",
    "        xs[i] = map_layer(xs[i])\n",
    "    \n",
    "    feature_dropout_layer = layers.Dropout(rate=drop_out, name=\"feature_dropout_layer\")\n",
    "    feature_map_layer = layers.Dense(model_dim, activation=\"relu\",name=\"feature_map_layer\")\n",
    "    xextrs = [feature_map_layer(feature_dropout_layer(xextr)) for xextr in xextrs]\n",
    "    \n",
    "    x = layers.Concatenate(axis=1, name=\"concat_feature_layer\")(xs + xextrs)\n",
    "    x = layers.Dropout(drop_out, name='dropout_layer')(x)\n",
    "\n",
    "    # MLP Layers\n",
    "    for i in range(mlp_depth - 1):\n",
    "        x = layers.Dense(mlp_dim, activation='selu', kernel_initializer='lecun_normal', name='selu_layer' + str(i))(x)\n",
    "        x = layers.AlphaDropout(drop_out, name='alpha_layer' + str(i))(x)\n",
    "\n",
    "    outputs = layers.Dense(output_dim, activation=\"softmax\", name=\"softmax_layer0\")(x)\n",
    "\n",
    "    model = models.Model([inputp, inputa, inputb] + inputpos + xis + [inputpa, inputpb], outputs)\n",
    "\n",
    "    if return_customized_layers:\n",
    "        return model, {'FeatureSelection1D': FeatureSelection1D, 'AttentionWeight': AttentionWeight}\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f134f020b4c1c7f9e8ce4cfc801b015d9090e249"
   },
   "source": [
    "### Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "_uuid": "7c0e4a38b4fa5e55811bdb4672dc5a596e70c433"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Isak\\.conda\\envs\\gender\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\Isak\\.conda\\envs\\gender\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "voca_dim = embedding_matrix.shape[0]\n",
    "pos_tag_size = len(pos_index) + 1 # my edit\n",
    "time_steps = max_len\n",
    "\n",
    "embed_dim = embedding_matrix.shape[1]\n",
    "pos_tag_dim = 5\n",
    "extra_feature_dims = num_pos_features\n",
    "output_dim = 3\n",
    "rnn_dim = 50\n",
    "model_dim = 10\n",
    "mlp_dim = 10\n",
    "rnn_depth = 1\n",
    "mlp_depth=1\n",
    "drop_out=0.2\n",
    "rnn_drop_out=0.5\n",
    "gpu = False\n",
    "return_customized_layers=True\n",
    "\n",
    "model, co = build_e2e_birnn_attention_model(\n",
    "        voca_dim, time_steps, pos_tag_size, pos_tag_dim, extra_feature_dims, output_dim, rnn_dim, model_dim, mlp_dim,\n",
    "        item_embedding=embedding_matrix, rnn_depth=rnn_depth, mlp_depth=mlp_depth,\n",
    "        drop_out=drop_out, rnn_drop_out=rnn_drop_out, rnn_state_drop_out=rnn_drop_out,\n",
    "        trainable_embedding=False, gpu=gpu, return_customized_layers=return_customized_layers)\n",
    "cos.append(co)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from keras.utils import plot_model\n",
    "plot_model(model, to_file='model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "_uuid": "f56526943859c2cd1c3738e043059a4951dd92f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "inputp (InputLayer)             (None, 50)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "inputposp (InputLayer)          (None, 50)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "inputa (InputLayer)             (None, 50)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "inputposa (InputLayer)          (None, 50)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "inputb (InputLayer)             (None, 50)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "inputposb (InputLayer)          (None, 50)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_layer0 (Embedding)    (None, 50, 300)      6696000     inputp[0][0]                     \n",
      "                                                                 inputa[0][0]                     \n",
      "                                                                 inputb[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "pos_embedding_layer0 (Embedding (None, 50, 5)        90          inputposp[0][0]                  \n",
      "                                                                 inputposa[0][0]                  \n",
      "                                                                 inputposb[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "embed_concate_layer (Concatenat (None, 50, 305)      0           embedding_layer0[0][0]           \n",
      "                                                                 pos_embedding_layer0[0][0]       \n",
      "                                                                 embedding_layer0[1][0]           \n",
      "                                                                 pos_embedding_layer0[1][0]       \n",
      "                                                                 embedding_layer0[2][0]           \n",
      "                                                                 pos_embedding_layer0[2][0]       \n",
      "__________________________________________________________________________________________________\n",
      "bi_lstm_layer0 (Bidirectional)  (None, 50, 100)      106800      embed_concate_layer[0][0]        \n",
      "                                                                 embed_concate_layer[1][0]        \n",
      "                                                                 embed_concate_layer[2][0]        \n",
      "__________________________________________________________________________________________________\n",
      "rnn_batch_norm_layer0 (BatchNor (None, 50, 100)      400         bi_lstm_layer0[0][0]             \n",
      "                                                                 bi_lstm_layer0[1][0]             \n",
      "                                                                 bi_lstm_layer0[2][0]             \n",
      "__________________________________________________________________________________________________\n",
      "inputpi (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "inputai (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "inputbi (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "boundary_selection_layer (Featu (None, 1, 100)       0           rnn_batch_norm_layer0[0][0]      \n",
      "                                                                 inputpi[0][0]                    \n",
      "                                                                 rnn_batch_norm_layer0[1][0]      \n",
      "                                                                 inputai[0][0]                    \n",
      "                                                                 rnn_batch_norm_layer0[2][0]      \n",
      "                                                                 inputbi[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_layer1 (Flatten)        (None, 100)          0           boundary_selection_layer[0][0]   \n",
      "                                                                 boundary_selection_layer[1][0]   \n",
      "                                                                 boundary_selection_layer[2][0]   \n",
      "__________________________________________________________________________________________________\n",
      "permuted_attention_x (Permute)  (None, 100, 50)      0           rnn_batch_norm_layer0[0][0]      \n",
      "                                                                 rnn_batch_norm_layer0[1][0]      \n",
      "                                                                 rnn_batch_norm_layer0[2][0]      \n",
      "__________________________________________________________________________________________________\n",
      "attention_weight (AttentionWeig (None, 50)           10100       rnn_batch_norm_layer0[0][0]      \n",
      "                                                                 flatten_layer1[0][0]             \n",
      "                                                                 rnn_batch_norm_layer0[1][0]      \n",
      "                                                                 flatten_layer1[1][0]             \n",
      "                                                                 rnn_batch_norm_layer0[2][0]      \n",
      "                                                                 flatten_layer1[2][0]             \n",
      "__________________________________________________________________________________________________\n",
      "focus_layer (Dot)               (None, 100)          0           permuted_attention_x[0][0]       \n",
      "                                                                 attention_weight[0][0]           \n",
      "                                                                 permuted_attention_x[1][0]       \n",
      "                                                                 attention_weight[1][0]           \n",
      "                                                                 permuted_attention_x[2][0]       \n",
      "                                                                 attention_weight[2][0]           \n",
      "__________________________________________________________________________________________________\n",
      "reshape_layer (Reshape)         (None, 1, 100)       0           focus_layer[0][0]                \n",
      "                                                                 focus_layer[1][0]                \n",
      "                                                                 focus_layer[2][0]                \n",
      "__________________________________________________________________________________________________\n",
      "attention_concate_layer (Concat (None, 2, 100)       0           boundary_selection_layer[0][0]   \n",
      "                                                                 reshape_layer[0][0]              \n",
      "                                                                 boundary_selection_layer[1][0]   \n",
      "                                                                 reshape_layer[1][0]              \n",
      "                                                                 boundary_selection_layer[2][0]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                 reshape_layer[2][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_layer (Flatten)         (None, 200)          0           attention_concate_layer[0][0]    \n",
      "                                                                 attention_concate_layer[1][0]    \n",
      "                                                                 attention_concate_layer[2][0]    \n",
      "__________________________________________________________________________________________________\n",
      "inputpa (InputLayer)            (None, 45)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "inputpb (InputLayer)            (None, 45)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "attention_dropout_layer (Dropou (None, 200)          0           flatten_layer[0][0]              \n",
      "                                                                 flatten_layer[1][0]              \n",
      "                                                                 flatten_layer[2][0]              \n",
      "__________________________________________________________________________________________________\n",
      "feature_dropout_layer (Dropout) (None, 45)           0           inputpa[0][0]                    \n",
      "                                                                 inputpb[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "map_layer1 (Dense)              (None, 10)           2010        attention_dropout_layer[0][0]    \n",
      "                                                                 attention_dropout_layer[1][0]    \n",
      "                                                                 attention_dropout_layer[2][0]    \n",
      "__________________________________________________________________________________________________\n",
      "feature_map_layer (Dense)       (None, 10)           460         feature_dropout_layer[0][0]      \n",
      "                                                                 feature_dropout_layer[1][0]      \n",
      "__________________________________________________________________________________________________\n",
      "concat_feature_layer (Concatena (None, 50)           0           map_layer1[0][0]                 \n",
      "                                                                 map_layer1[1][0]                 \n",
      "                                                                 map_layer1[2][0]                 \n",
      "                                                                 feature_map_layer[0][0]          \n",
      "                                                                 feature_map_layer[1][0]          \n",
      "__________________________________________________________________________________________________\n",
      "dropout_layer (Dropout)         (None, 50)           0           concat_feature_layer[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "softmax_layer0 (Dense)          (None, 3)            153         dropout_layer[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 6,816,013\n",
      "Trainable params: 119,813\n",
      "Non-trainable params: 6,696,200\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2c926e888dce8ff928430dc76d4771cb67aea2a6"
   },
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.13.1'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow\n",
    "tensorflow.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "_uuid": "02490fd68b2c304432f68d8c356b3013084d8d3d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Isak\\.conda\\envs\\gender\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Isak\\.conda\\envs\\gender\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:110: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\Users\\Isak\\.conda\\envs\\gender\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:110: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\Users\\Isak\\.conda\\envs\\gender\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:110: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2000 samples, validate on 454 samples\n",
      "Epoch 1/40\n",
      "2000/2000 [==============================] - ETA: 4:52 - loss: 1.7047 - sparse_categorical_accuracy: 0.366 - ETA: 2:28 - loss: 1.6570 - sparse_categorical_accuracy: 0.350 - ETA: 1:40 - loss: 1.6431 - sparse_categorical_accuracy: 0.322 - ETA: 1:16 - loss: 1.5469 - sparse_categorical_accuracy: 0.341 - ETA: 1:01 - loss: 1.4316 - sparse_categorical_accuracy: 0.386 - ETA: 51s - loss: 1.3781 - sparse_categorical_accuracy: 0.416 - ETA: 44s - loss: 1.4255 - sparse_categorical_accuracy: 0.40 - ETA: 39s - loss: 1.4277 - sparse_categorical_accuracy: 0.40 - ETA: 35s - loss: 1.4316 - sparse_categorical_accuracy: 0.40 - ETA: 32s - loss: 1.3950 - sparse_categorical_accuracy: 0.41 - ETA: 29s - loss: 1.3789 - sparse_categorical_accuracy: 0.41 - ETA: 26s - loss: 1.4238 - sparse_categorical_accuracy: 0.40 - ETA: 25s - loss: 1.3885 - sparse_categorical_accuracy: 0.41 - ETA: 23s - loss: 1.3836 - sparse_categorical_accuracy: 0.41 - ETA: 21s - loss: 1.3672 - sparse_categorical_accuracy: 0.41 - ETA: 20s - loss: 1.3606 - sparse_categorical_accuracy: 0.42 - ETA: 19s - loss: 1.3386 - sparse_categorical_accuracy: 0.42 - ETA: 18s - loss: 1.3434 - sparse_categorical_accuracy: 0.42 - ETA: 17s - loss: 1.3361 - sparse_categorical_accuracy: 0.42 - ETA: 16s - loss: 1.3221 - sparse_categorical_accuracy: 0.43 - ETA: 15s - loss: 1.3164 - sparse_categorical_accuracy: 0.43 - ETA: 14s - loss: 1.3017 - sparse_categorical_accuracy: 0.43 - ETA: 13s - loss: 1.3054 - sparse_categorical_accuracy: 0.43 - ETA: 13s - loss: 1.2896 - sparse_categorical_accuracy: 0.43 - ETA: 12s - loss: 1.2824 - sparse_categorical_accuracy: 0.44 - ETA: 12s - loss: 1.2800 - sparse_categorical_accuracy: 0.44 - ETA: 11s - loss: 1.2713 - sparse_categorical_accuracy: 0.44 - ETA: 10s - loss: 1.2642 - sparse_categorical_accuracy: 0.45 - ETA: 10s - loss: 1.2516 - sparse_categorical_accuracy: 0.45 - ETA: 10s - loss: 1.2536 - sparse_categorical_accuracy: 0.44 - ETA: 9s - loss: 1.2518 - sparse_categorical_accuracy: 0.4473 - ETA: 9s - loss: 1.2492 - sparse_categorical_accuracy: 0.451 - ETA: 8s - loss: 1.2407 - sparse_categorical_accuracy: 0.454 - ETA: 8s - loss: 1.2296 - sparse_categorical_accuracy: 0.462 - ETA: 7s - loss: 1.2257 - sparse_categorical_accuracy: 0.461 - ETA: 7s - loss: 1.2224 - sparse_categorical_accuracy: 0.459 - ETA: 7s - loss: 1.2141 - sparse_categorical_accuracy: 0.462 - ETA: 6s - loss: 1.2054 - sparse_categorical_accuracy: 0.465 - ETA: 6s - loss: 1.1980 - sparse_categorical_accuracy: 0.468 - ETA: 6s - loss: 1.1939 - sparse_categorical_accuracy: 0.470 - ETA: 5s - loss: 1.1898 - sparse_categorical_accuracy: 0.472 - ETA: 5s - loss: 1.1850 - sparse_categorical_accuracy: 0.473 - ETA: 5s - loss: 1.1796 - sparse_categorical_accuracy: 0.476 - ETA: 5s - loss: 1.1801 - sparse_categorical_accuracy: 0.475 - ETA: 4s - loss: 1.1807 - sparse_categorical_accuracy: 0.473 - ETA: 4s - loss: 1.1779 - sparse_categorical_accuracy: 0.476 - ETA: 4s - loss: 1.1756 - sparse_categorical_accuracy: 0.478 - ETA: 4s - loss: 1.1724 - sparse_categorical_accuracy: 0.479 - ETA: 3s - loss: 1.1681 - sparse_categorical_accuracy: 0.477 - ETA: 3s - loss: 1.1648 - sparse_categorical_accuracy: 0.477 - ETA: 3s - loss: 1.1637 - sparse_categorical_accuracy: 0.476 - ETA: 3s - loss: 1.1610 - sparse_categorical_accuracy: 0.475 - ETA: 2s - loss: 1.1593 - sparse_categorical_accuracy: 0.474 - ETA: 2s - loss: 1.1593 - sparse_categorical_accuracy: 0.476 - ETA: 2s - loss: 1.1563 - sparse_categorical_accuracy: 0.476 - ETA: 2s - loss: 1.1538 - sparse_categorical_accuracy: 0.476 - ETA: 1s - loss: 1.1499 - sparse_categorical_accuracy: 0.478 - ETA: 1s - loss: 1.1472 - sparse_categorical_accuracy: 0.478 - ETA: 1s - loss: 1.1432 - sparse_categorical_accuracy: 0.478 - ETA: 1s - loss: 1.1386 - sparse_categorical_accuracy: 0.478 - ETA: 1s - loss: 1.1403 - sparse_categorical_accuracy: 0.477 - ETA: 0s - loss: 1.1413 - sparse_categorical_accuracy: 0.475 - ETA: 0s - loss: 1.1408 - sparse_categorical_accuracy: 0.475 - ETA: 0s - loss: 1.1355 - sparse_categorical_accuracy: 0.477 - ETA: 0s - loss: 1.1331 - sparse_categorical_accuracy: 0.477 - ETA: 0s - loss: 1.1305 - sparse_categorical_accuracy: 0.477 - 14s 7ms/step - loss: 1.1315 - sparse_categorical_accuracy: 0.4785 - val_loss: 0.9461 - val_sparse_categorical_accuracy: 0.5617\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.94607, saving model to best_e2e_rnn_atten_model.hdf5\n",
      "Epoch 2/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - ETA: 7s - loss: 1.0038 - sparse_categorical_accuracy: 0.533 - ETA: 7s - loss: 1.0447 - sparse_categorical_accuracy: 0.433 - ETA: 7s - loss: 1.0020 - sparse_categorical_accuracy: 0.444 - ETA: 7s - loss: 0.9843 - sparse_categorical_accuracy: 0.458 - ETA: 7s - loss: 0.9636 - sparse_categorical_accuracy: 0.473 - ETA: 7s - loss: 0.9421 - sparse_categorical_accuracy: 0.500 - ETA: 7s - loss: 0.9341 - sparse_categorical_accuracy: 0.509 - ETA: 7s - loss: 0.9574 - sparse_categorical_accuracy: 0.512 - ETA: 7s - loss: 0.9486 - sparse_categorical_accuracy: 0.525 - ETA: 6s - loss: 0.9399 - sparse_categorical_accuracy: 0.533 - ETA: 6s - loss: 0.9494 - sparse_categorical_accuracy: 0.524 - ETA: 6s - loss: 0.9463 - sparse_categorical_accuracy: 0.533 - ETA: 6s - loss: 0.9482 - sparse_categorical_accuracy: 0.535 - ETA: 6s - loss: 0.9421 - sparse_categorical_accuracy: 0.545 - ETA: 6s - loss: 0.9390 - sparse_categorical_accuracy: 0.548 - ETA: 6s - loss: 0.9438 - sparse_categorical_accuracy: 0.547 - ETA: 6s - loss: 0.9374 - sparse_categorical_accuracy: 0.556 - ETA: 5s - loss: 0.9299 - sparse_categorical_accuracy: 0.559 - ETA: 5s - loss: 0.9375 - sparse_categorical_accuracy: 0.552 - ETA: 5s - loss: 0.9305 - sparse_categorical_accuracy: 0.555 - ETA: 5s - loss: 0.9467 - sparse_categorical_accuracy: 0.549 - ETA: 5s - loss: 0.9527 - sparse_categorical_accuracy: 0.543 - ETA: 5s - loss: 0.9513 - sparse_categorical_accuracy: 0.546 - ETA: 5s - loss: 0.9456 - sparse_categorical_accuracy: 0.552 - ETA: 5s - loss: 0.9450 - sparse_categorical_accuracy: 0.556 - ETA: 4s - loss: 0.9484 - sparse_categorical_accuracy: 0.552 - ETA: 4s - loss: 0.9460 - sparse_categorical_accuracy: 0.556 - ETA: 4s - loss: 0.9435 - sparse_categorical_accuracy: 0.559 - ETA: 4s - loss: 0.9408 - sparse_categorical_accuracy: 0.559 - ETA: 4s - loss: 0.9350 - sparse_categorical_accuracy: 0.558 - ETA: 4s - loss: 0.9309 - sparse_categorical_accuracy: 0.559 - ETA: 4s - loss: 0.9251 - sparse_categorical_accuracy: 0.560 - ETA: 4s - loss: 0.9236 - sparse_categorical_accuracy: 0.562 - ETA: 3s - loss: 0.9210 - sparse_categorical_accuracy: 0.562 - ETA: 3s - loss: 0.9217 - sparse_categorical_accuracy: 0.562 - ETA: 3s - loss: 0.9234 - sparse_categorical_accuracy: 0.565 - ETA: 3s - loss: 0.9211 - sparse_categorical_accuracy: 0.564 - ETA: 3s - loss: 0.9251 - sparse_categorical_accuracy: 0.564 - ETA: 3s - loss: 0.9250 - sparse_categorical_accuracy: 0.563 - ETA: 3s - loss: 0.9285 - sparse_categorical_accuracy: 0.563 - ETA: 3s - loss: 0.9295 - sparse_categorical_accuracy: 0.561 - ETA: 3s - loss: 0.9297 - sparse_categorical_accuracy: 0.561 - ETA: 2s - loss: 0.9277 - sparse_categorical_accuracy: 0.561 - ETA: 2s - loss: 0.9246 - sparse_categorical_accuracy: 0.562 - ETA: 2s - loss: 0.9230 - sparse_categorical_accuracy: 0.565 - ETA: 2s - loss: 0.9218 - sparse_categorical_accuracy: 0.566 - ETA: 2s - loss: 0.9230 - sparse_categorical_accuracy: 0.566 - ETA: 2s - loss: 0.9206 - sparse_categorical_accuracy: 0.566 - ETA: 2s - loss: 0.9239 - sparse_categorical_accuracy: 0.566 - ETA: 2s - loss: 0.9222 - sparse_categorical_accuracy: 0.564 - ETA: 1s - loss: 0.9217 - sparse_categorical_accuracy: 0.565 - ETA: 1s - loss: 0.9234 - sparse_categorical_accuracy: 0.562 - ETA: 1s - loss: 0.9220 - sparse_categorical_accuracy: 0.565 - ETA: 1s - loss: 0.9241 - sparse_categorical_accuracy: 0.562 - ETA: 1s - loss: 0.9229 - sparse_categorical_accuracy: 0.564 - ETA: 1s - loss: 0.9236 - sparse_categorical_accuracy: 0.562 - ETA: 1s - loss: 0.9244 - sparse_categorical_accuracy: 0.562 - ETA: 1s - loss: 0.9239 - sparse_categorical_accuracy: 0.562 - ETA: 0s - loss: 0.9192 - sparse_categorical_accuracy: 0.566 - ETA: 0s - loss: 0.9188 - sparse_categorical_accuracy: 0.566 - ETA: 0s - loss: 0.9191 - sparse_categorical_accuracy: 0.566 - ETA: 0s - loss: 0.9170 - sparse_categorical_accuracy: 0.567 - ETA: 0s - loss: 0.9161 - sparse_categorical_accuracy: 0.567 - ETA: 0s - loss: 0.9171 - sparse_categorical_accuracy: 0.566 - ETA: 0s - loss: 0.9161 - sparse_categorical_accuracy: 0.567 - ETA: 0s - loss: 0.9137 - sparse_categorical_accuracy: 0.569 - 9s 4ms/step - loss: 0.9112 - sparse_categorical_accuracy: 0.5710 - val_loss: 0.8300 - val_sparse_categorical_accuracy: 0.6278\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.94607 to 0.83002, saving model to best_e2e_rnn_atten_model.hdf5\n",
      "Epoch 3/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - ETA: 7s - loss: 0.9610 - sparse_categorical_accuracy: 0.533 - ETA: 7s - loss: 0.8594 - sparse_categorical_accuracy: 0.583 - ETA: 7s - loss: 0.8590 - sparse_categorical_accuracy: 0.566 - ETA: 7s - loss: 0.8694 - sparse_categorical_accuracy: 0.558 - ETA: 7s - loss: 0.8762 - sparse_categorical_accuracy: 0.560 - ETA: 7s - loss: 0.8588 - sparse_categorical_accuracy: 0.577 - ETA: 7s - loss: 0.8379 - sparse_categorical_accuracy: 0.590 - ETA: 7s - loss: 0.8314 - sparse_categorical_accuracy: 0.595 - ETA: 7s - loss: 0.8323 - sparse_categorical_accuracy: 0.600 - ETA: 6s - loss: 0.8164 - sparse_categorical_accuracy: 0.616 - ETA: 6s - loss: 0.8016 - sparse_categorical_accuracy: 0.624 - ETA: 6s - loss: 0.8008 - sparse_categorical_accuracy: 0.633 - ETA: 6s - loss: 0.7998 - sparse_categorical_accuracy: 0.641 - ETA: 6s - loss: 0.7892 - sparse_categorical_accuracy: 0.640 - ETA: 6s - loss: 0.7901 - sparse_categorical_accuracy: 0.637 - ETA: 6s - loss: 0.7849 - sparse_categorical_accuracy: 0.643 - ETA: 6s - loss: 0.7888 - sparse_categorical_accuracy: 0.645 - ETA: 5s - loss: 0.7871 - sparse_categorical_accuracy: 0.648 - ETA: 5s - loss: 0.8153 - sparse_categorical_accuracy: 0.636 - ETA: 5s - loss: 0.8250 - sparse_categorical_accuracy: 0.635 - ETA: 5s - loss: 0.8261 - sparse_categorical_accuracy: 0.634 - ETA: 5s - loss: 0.8280 - sparse_categorical_accuracy: 0.631 - ETA: 5s - loss: 0.8246 - sparse_categorical_accuracy: 0.633 - ETA: 5s - loss: 0.8262 - sparse_categorical_accuracy: 0.633 - ETA: 5s - loss: 0.8335 - sparse_categorical_accuracy: 0.629 - ETA: 5s - loss: 0.8340 - sparse_categorical_accuracy: 0.632 - ETA: 4s - loss: 0.8327 - sparse_categorical_accuracy: 0.627 - ETA: 4s - loss: 0.8395 - sparse_categorical_accuracy: 0.623 - ETA: 4s - loss: 0.8409 - sparse_categorical_accuracy: 0.621 - ETA: 4s - loss: 0.8519 - sparse_categorical_accuracy: 0.620 - ETA: 4s - loss: 0.8501 - sparse_categorical_accuracy: 0.621 - ETA: 4s - loss: 0.8565 - sparse_categorical_accuracy: 0.616 - ETA: 4s - loss: 0.8574 - sparse_categorical_accuracy: 0.615 - ETA: 4s - loss: 0.8594 - sparse_categorical_accuracy: 0.612 - ETA: 3s - loss: 0.8582 - sparse_categorical_accuracy: 0.615 - ETA: 3s - loss: 0.8550 - sparse_categorical_accuracy: 0.616 - ETA: 3s - loss: 0.8493 - sparse_categorical_accuracy: 0.619 - ETA: 3s - loss: 0.8457 - sparse_categorical_accuracy: 0.623 - ETA: 3s - loss: 0.8407 - sparse_categorical_accuracy: 0.625 - ETA: 3s - loss: 0.8385 - sparse_categorical_accuracy: 0.626 - ETA: 3s - loss: 0.8380 - sparse_categorical_accuracy: 0.627 - ETA: 3s - loss: 0.8340 - sparse_categorical_accuracy: 0.631 - ETA: 2s - loss: 0.8391 - sparse_categorical_accuracy: 0.629 - ETA: 2s - loss: 0.8381 - sparse_categorical_accuracy: 0.631 - ETA: 2s - loss: 0.8381 - sparse_categorical_accuracy: 0.630 - ETA: 2s - loss: 0.8395 - sparse_categorical_accuracy: 0.630 - ETA: 2s - loss: 0.8393 - sparse_categorical_accuracy: 0.631 - ETA: 2s - loss: 0.8428 - sparse_categorical_accuracy: 0.629 - ETA: 2s - loss: 0.8385 - sparse_categorical_accuracy: 0.632 - ETA: 2s - loss: 0.8404 - sparse_categorical_accuracy: 0.632 - ETA: 1s - loss: 0.8394 - sparse_categorical_accuracy: 0.632 - ETA: 1s - loss: 0.8347 - sparse_categorical_accuracy: 0.633 - ETA: 1s - loss: 0.8321 - sparse_categorical_accuracy: 0.635 - ETA: 1s - loss: 0.8299 - sparse_categorical_accuracy: 0.634 - ETA: 1s - loss: 0.8331 - sparse_categorical_accuracy: 0.633 - ETA: 1s - loss: 0.8359 - sparse_categorical_accuracy: 0.633 - ETA: 1s - loss: 0.8331 - sparse_categorical_accuracy: 0.636 - ETA: 1s - loss: 0.8354 - sparse_categorical_accuracy: 0.634 - ETA: 0s - loss: 0.8345 - sparse_categorical_accuracy: 0.636 - ETA: 0s - loss: 0.8339 - sparse_categorical_accuracy: 0.635 - ETA: 0s - loss: 0.8351 - sparse_categorical_accuracy: 0.633 - ETA: 0s - loss: 0.8376 - sparse_categorical_accuracy: 0.631 - ETA: 0s - loss: 0.8336 - sparse_categorical_accuracy: 0.633 - ETA: 0s - loss: 0.8311 - sparse_categorical_accuracy: 0.633 - ETA: 0s - loss: 0.8288 - sparse_categorical_accuracy: 0.634 - ETA: 0s - loss: 0.8238 - sparse_categorical_accuracy: 0.636 - 9s 4ms/step - loss: 0.8218 - sparse_categorical_accuracy: 0.6370 - val_loss: 0.8002 - val_sparse_categorical_accuracy: 0.6211\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.83002 to 0.80019, saving model to best_e2e_rnn_atten_model.hdf5\n",
      "Epoch 4/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - ETA: 7s - loss: 0.6693 - sparse_categorical_accuracy: 0.666 - ETA: 7s - loss: 0.7497 - sparse_categorical_accuracy: 0.616 - ETA: 7s - loss: 0.7079 - sparse_categorical_accuracy: 0.633 - ETA: 7s - loss: 0.7486 - sparse_categorical_accuracy: 0.633 - ETA: 7s - loss: 0.7371 - sparse_categorical_accuracy: 0.640 - ETA: 7s - loss: 0.7406 - sparse_categorical_accuracy: 0.650 - ETA: 7s - loss: 0.7954 - sparse_categorical_accuracy: 0.638 - ETA: 7s - loss: 0.8041 - sparse_categorical_accuracy: 0.633 - ETA: 6s - loss: 0.8009 - sparse_categorical_accuracy: 0.637 - ETA: 6s - loss: 0.7956 - sparse_categorical_accuracy: 0.643 - ETA: 6s - loss: 0.7950 - sparse_categorical_accuracy: 0.651 - ETA: 6s - loss: 0.7855 - sparse_categorical_accuracy: 0.647 - ETA: 6s - loss: 0.8065 - sparse_categorical_accuracy: 0.641 - ETA: 6s - loss: 0.7925 - sparse_categorical_accuracy: 0.650 - ETA: 6s - loss: 0.7891 - sparse_categorical_accuracy: 0.651 - ETA: 6s - loss: 0.7789 - sparse_categorical_accuracy: 0.662 - ETA: 6s - loss: 0.7695 - sparse_categorical_accuracy: 0.668 - ETA: 5s - loss: 0.7780 - sparse_categorical_accuracy: 0.661 - ETA: 5s - loss: 0.7707 - sparse_categorical_accuracy: 0.666 - ETA: 5s - loss: 0.7737 - sparse_categorical_accuracy: 0.661 - ETA: 5s - loss: 0.7872 - sparse_categorical_accuracy: 0.655 - ETA: 5s - loss: 0.7864 - sparse_categorical_accuracy: 0.659 - ETA: 5s - loss: 0.7929 - sparse_categorical_accuracy: 0.655 - ETA: 5s - loss: 0.7918 - sparse_categorical_accuracy: 0.655 - ETA: 5s - loss: 0.7955 - sparse_categorical_accuracy: 0.652 - ETA: 4s - loss: 0.8020 - sparse_categorical_accuracy: 0.644 - ETA: 4s - loss: 0.7942 - sparse_categorical_accuracy: 0.649 - ETA: 4s - loss: 0.7957 - sparse_categorical_accuracy: 0.651 - ETA: 4s - loss: 0.7984 - sparse_categorical_accuracy: 0.651 - ETA: 4s - loss: 0.8007 - sparse_categorical_accuracy: 0.653 - ETA: 4s - loss: 0.7960 - sparse_categorical_accuracy: 0.657 - ETA: 4s - loss: 0.7934 - sparse_categorical_accuracy: 0.658 - ETA: 4s - loss: 0.7951 - sparse_categorical_accuracy: 0.656 - ETA: 3s - loss: 0.7923 - sparse_categorical_accuracy: 0.658 - ETA: 3s - loss: 0.7931 - sparse_categorical_accuracy: 0.661 - ETA: 3s - loss: 0.8011 - sparse_categorical_accuracy: 0.658 - ETA: 3s - loss: 0.7970 - sparse_categorical_accuracy: 0.659 - ETA: 3s - loss: 0.7975 - sparse_categorical_accuracy: 0.658 - ETA: 3s - loss: 0.7934 - sparse_categorical_accuracy: 0.662 - ETA: 3s - loss: 0.7927 - sparse_categorical_accuracy: 0.664 - ETA: 3s - loss: 0.7992 - sparse_categorical_accuracy: 0.659 - ETA: 2s - loss: 0.7988 - sparse_categorical_accuracy: 0.657 - ETA: 2s - loss: 0.7969 - sparse_categorical_accuracy: 0.655 - ETA: 2s - loss: 0.7950 - sparse_categorical_accuracy: 0.654 - ETA: 2s - loss: 0.7914 - sparse_categorical_accuracy: 0.654 - ETA: 2s - loss: 0.7904 - sparse_categorical_accuracy: 0.654 - ETA: 2s - loss: 0.7910 - sparse_categorical_accuracy: 0.655 - ETA: 2s - loss: 0.7890 - sparse_categorical_accuracy: 0.656 - ETA: 2s - loss: 0.7890 - sparse_categorical_accuracy: 0.657 - ETA: 2s - loss: 0.7888 - sparse_categorical_accuracy: 0.657 - ETA: 1s - loss: 0.7902 - sparse_categorical_accuracy: 0.654 - ETA: 1s - loss: 0.7878 - sparse_categorical_accuracy: 0.657 - ETA: 1s - loss: 0.7905 - sparse_categorical_accuracy: 0.656 - ETA: 1s - loss: 0.7891 - sparse_categorical_accuracy: 0.656 - ETA: 1s - loss: 0.7879 - sparse_categorical_accuracy: 0.656 - ETA: 1s - loss: 0.7861 - sparse_categorical_accuracy: 0.658 - ETA: 1s - loss: 0.7865 - sparse_categorical_accuracy: 0.660 - ETA: 1s - loss: 0.7841 - sparse_categorical_accuracy: 0.661 - ETA: 0s - loss: 0.7850 - sparse_categorical_accuracy: 0.660 - ETA: 0s - loss: 0.7833 - sparse_categorical_accuracy: 0.660 - ETA: 0s - loss: 0.7825 - sparse_categorical_accuracy: 0.660 - ETA: 0s - loss: 0.7850 - sparse_categorical_accuracy: 0.659 - ETA: 0s - loss: 0.7864 - sparse_categorical_accuracy: 0.659 - ETA: 0s - loss: 0.7841 - sparse_categorical_accuracy: 0.659 - ETA: 0s - loss: 0.7842 - sparse_categorical_accuracy: 0.659 - ETA: 0s - loss: 0.7807 - sparse_categorical_accuracy: 0.661 - 9s 4ms/step - loss: 0.7810 - sparse_categorical_accuracy: 0.6605 - val_loss: 0.7668 - val_sparse_categorical_accuracy: 0.6189\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.80019 to 0.76680, saving model to best_e2e_rnn_atten_model.hdf5\n",
      "Epoch 5/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - ETA: 7s - loss: 0.6416 - sparse_categorical_accuracy: 0.733 - ETA: 7s - loss: 0.7823 - sparse_categorical_accuracy: 0.666 - ETA: 7s - loss: 0.8840 - sparse_categorical_accuracy: 0.633 - ETA: 7s - loss: 0.8459 - sparse_categorical_accuracy: 0.641 - ETA: 7s - loss: 0.8667 - sparse_categorical_accuracy: 0.633 - ETA: 7s - loss: 0.8274 - sparse_categorical_accuracy: 0.644 - ETA: 7s - loss: 0.7850 - sparse_categorical_accuracy: 0.671 - ETA: 7s - loss: 0.7831 - sparse_categorical_accuracy: 0.670 - ETA: 7s - loss: 0.7693 - sparse_categorical_accuracy: 0.692 - ETA: 6s - loss: 0.7516 - sparse_categorical_accuracy: 0.693 - ETA: 6s - loss: 0.7435 - sparse_categorical_accuracy: 0.684 - ETA: 6s - loss: 0.7766 - sparse_categorical_accuracy: 0.669 - ETA: 6s - loss: 0.7536 - sparse_categorical_accuracy: 0.676 - ETA: 6s - loss: 0.7440 - sparse_categorical_accuracy: 0.683 - ETA: 6s - loss: 0.7501 - sparse_categorical_accuracy: 0.675 - ETA: 6s - loss: 0.7493 - sparse_categorical_accuracy: 0.670 - ETA: 6s - loss: 0.7600 - sparse_categorical_accuracy: 0.662 - ETA: 5s - loss: 0.7548 - sparse_categorical_accuracy: 0.668 - ETA: 5s - loss: 0.7611 - sparse_categorical_accuracy: 0.664 - ETA: 5s - loss: 0.7605 - sparse_categorical_accuracy: 0.661 - ETA: 5s - loss: 0.7602 - sparse_categorical_accuracy: 0.663 - ETA: 5s - loss: 0.7502 - sparse_categorical_accuracy: 0.666 - ETA: 5s - loss: 0.7541 - sparse_categorical_accuracy: 0.668 - ETA: 5s - loss: 0.7644 - sparse_categorical_accuracy: 0.665 - ETA: 5s - loss: 0.7594 - sparse_categorical_accuracy: 0.668 - ETA: 4s - loss: 0.7604 - sparse_categorical_accuracy: 0.666 - ETA: 4s - loss: 0.7563 - sparse_categorical_accuracy: 0.667 - ETA: 4s - loss: 0.7564 - sparse_categorical_accuracy: 0.667 - ETA: 4s - loss: 0.7577 - sparse_categorical_accuracy: 0.663 - ETA: 4s - loss: 0.7507 - sparse_categorical_accuracy: 0.666 - ETA: 4s - loss: 0.7514 - sparse_categorical_accuracy: 0.664 - ETA: 4s - loss: 0.7619 - sparse_categorical_accuracy: 0.659 - ETA: 4s - loss: 0.7543 - sparse_categorical_accuracy: 0.664 - ETA: 3s - loss: 0.7516 - sparse_categorical_accuracy: 0.668 - ETA: 3s - loss: 0.7531 - sparse_categorical_accuracy: 0.669 - ETA: 3s - loss: 0.7645 - sparse_categorical_accuracy: 0.665 - ETA: 3s - loss: 0.7616 - sparse_categorical_accuracy: 0.667 - ETA: 3s - loss: 0.7628 - sparse_categorical_accuracy: 0.667 - ETA: 3s - loss: 0.7592 - sparse_categorical_accuracy: 0.669 - ETA: 3s - loss: 0.7608 - sparse_categorical_accuracy: 0.668 - ETA: 3s - loss: 0.7545 - sparse_categorical_accuracy: 0.671 - ETA: 3s - loss: 0.7513 - sparse_categorical_accuracy: 0.673 - ETA: 2s - loss: 0.7520 - sparse_categorical_accuracy: 0.672 - ETA: 2s - loss: 0.7519 - sparse_categorical_accuracy: 0.671 - ETA: 2s - loss: 0.7491 - sparse_categorical_accuracy: 0.671 - ETA: 2s - loss: 0.7481 - sparse_categorical_accuracy: 0.669 - ETA: 2s - loss: 0.7476 - sparse_categorical_accuracy: 0.669 - ETA: 2s - loss: 0.7548 - sparse_categorical_accuracy: 0.667 - ETA: 2s - loss: 0.7538 - sparse_categorical_accuracy: 0.668 - ETA: 2s - loss: 0.7513 - sparse_categorical_accuracy: 0.668 - ETA: 1s - loss: 0.7504 - sparse_categorical_accuracy: 0.671 - ETA: 1s - loss: 0.7524 - sparse_categorical_accuracy: 0.670 - ETA: 1s - loss: 0.7546 - sparse_categorical_accuracy: 0.669 - ETA: 1s - loss: 0.7536 - sparse_categorical_accuracy: 0.669 - ETA: 1s - loss: 0.7509 - sparse_categorical_accuracy: 0.670 - ETA: 1s - loss: 0.7585 - sparse_categorical_accuracy: 0.671 - ETA: 1s - loss: 0.7561 - sparse_categorical_accuracy: 0.673 - ETA: 1s - loss: 0.7570 - sparse_categorical_accuracy: 0.673 - ETA: 0s - loss: 0.7589 - sparse_categorical_accuracy: 0.669 - ETA: 0s - loss: 0.7571 - sparse_categorical_accuracy: 0.671 - ETA: 0s - loss: 0.7547 - sparse_categorical_accuracy: 0.672 - ETA: 0s - loss: 0.7534 - sparse_categorical_accuracy: 0.671 - ETA: 0s - loss: 0.7498 - sparse_categorical_accuracy: 0.673 - ETA: 0s - loss: 0.7502 - sparse_categorical_accuracy: 0.674 - ETA: 0s - loss: 0.7464 - sparse_categorical_accuracy: 0.676 - ETA: 0s - loss: 0.7479 - sparse_categorical_accuracy: 0.676 - 9s 4ms/step - loss: 0.7475 - sparse_categorical_accuracy: 0.6765 - val_loss: 0.7945 - val_sparse_categorical_accuracy: 0.6432\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.76680\n",
      "Epoch 6/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - ETA: 7s - loss: 0.6700 - sparse_categorical_accuracy: 0.666 - ETA: 7s - loss: 0.6796 - sparse_categorical_accuracy: 0.716 - ETA: 7s - loss: 0.6682 - sparse_categorical_accuracy: 0.733 - ETA: 7s - loss: 0.6234 - sparse_categorical_accuracy: 0.750 - ETA: 7s - loss: 0.5900 - sparse_categorical_accuracy: 0.760 - ETA: 7s - loss: 0.5874 - sparse_categorical_accuracy: 0.761 - ETA: 7s - loss: 0.6525 - sparse_categorical_accuracy: 0.733 - ETA: 7s - loss: 0.6963 - sparse_categorical_accuracy: 0.708 - ETA: 6s - loss: 0.7042 - sparse_categorical_accuracy: 0.688 - ETA: 6s - loss: 0.7028 - sparse_categorical_accuracy: 0.690 - ETA: 6s - loss: 0.6834 - sparse_categorical_accuracy: 0.697 - ETA: 6s - loss: 0.6769 - sparse_categorical_accuracy: 0.705 - ETA: 6s - loss: 0.6871 - sparse_categorical_accuracy: 0.700 - ETA: 6s - loss: 0.6792 - sparse_categorical_accuracy: 0.707 - ETA: 6s - loss: 0.6927 - sparse_categorical_accuracy: 0.700 - ETA: 6s - loss: 0.6968 - sparse_categorical_accuracy: 0.693 - ETA: 6s - loss: 0.6976 - sparse_categorical_accuracy: 0.698 - ETA: 5s - loss: 0.7047 - sparse_categorical_accuracy: 0.690 - ETA: 5s - loss: 0.7122 - sparse_categorical_accuracy: 0.687 - ETA: 5s - loss: 0.7111 - sparse_categorical_accuracy: 0.688 - ETA: 5s - loss: 0.7106 - sparse_categorical_accuracy: 0.687 - ETA: 5s - loss: 0.7013 - sparse_categorical_accuracy: 0.695 - ETA: 5s - loss: 0.7002 - sparse_categorical_accuracy: 0.695 - ETA: 5s - loss: 0.6953 - sparse_categorical_accuracy: 0.697 - ETA: 5s - loss: 0.6870 - sparse_categorical_accuracy: 0.701 - ETA: 4s - loss: 0.6860 - sparse_categorical_accuracy: 0.703 - ETA: 4s - loss: 0.6829 - sparse_categorical_accuracy: 0.704 - ETA: 4s - loss: 0.6809 - sparse_categorical_accuracy: 0.706 - ETA: 4s - loss: 0.6839 - sparse_categorical_accuracy: 0.708 - ETA: 4s - loss: 0.6893 - sparse_categorical_accuracy: 0.705 - ETA: 4s - loss: 0.6972 - sparse_categorical_accuracy: 0.702 - ETA: 4s - loss: 0.6973 - sparse_categorical_accuracy: 0.702 - ETA: 4s - loss: 0.6951 - sparse_categorical_accuracy: 0.703 - ETA: 3s - loss: 0.6964 - sparse_categorical_accuracy: 0.698 - ETA: 3s - loss: 0.6946 - sparse_categorical_accuracy: 0.698 - ETA: 3s - loss: 0.6912 - sparse_categorical_accuracy: 0.700 - ETA: 3s - loss: 0.6910 - sparse_categorical_accuracy: 0.699 - ETA: 3s - loss: 0.6912 - sparse_categorical_accuracy: 0.699 - ETA: 3s - loss: 0.6935 - sparse_categorical_accuracy: 0.696 - ETA: 3s - loss: 0.6959 - sparse_categorical_accuracy: 0.696 - ETA: 3s - loss: 0.7005 - sparse_categorical_accuracy: 0.695 - ETA: 3s - loss: 0.7015 - sparse_categorical_accuracy: 0.694 - ETA: 2s - loss: 0.6988 - sparse_categorical_accuracy: 0.696 - ETA: 2s - loss: 0.7024 - sparse_categorical_accuracy: 0.695 - ETA: 2s - loss: 0.7046 - sparse_categorical_accuracy: 0.693 - ETA: 2s - loss: 0.7115 - sparse_categorical_accuracy: 0.689 - ETA: 2s - loss: 0.7153 - sparse_categorical_accuracy: 0.684 - ETA: 2s - loss: 0.7162 - sparse_categorical_accuracy: 0.684 - ETA: 2s - loss: 0.7154 - sparse_categorical_accuracy: 0.683 - ETA: 2s - loss: 0.7146 - sparse_categorical_accuracy: 0.684 - ETA: 1s - loss: 0.7149 - sparse_categorical_accuracy: 0.684 - ETA: 1s - loss: 0.7142 - sparse_categorical_accuracy: 0.685 - ETA: 1s - loss: 0.7171 - sparse_categorical_accuracy: 0.684 - ETA: 1s - loss: 0.7133 - sparse_categorical_accuracy: 0.688 - ETA: 1s - loss: 0.7147 - sparse_categorical_accuracy: 0.686 - ETA: 1s - loss: 0.7182 - sparse_categorical_accuracy: 0.686 - ETA: 1s - loss: 0.7194 - sparse_categorical_accuracy: 0.685 - ETA: 1s - loss: 0.7233 - sparse_categorical_accuracy: 0.683 - ETA: 0s - loss: 0.7260 - sparse_categorical_accuracy: 0.683 - ETA: 0s - loss: 0.7263 - sparse_categorical_accuracy: 0.683 - ETA: 0s - loss: 0.7270 - sparse_categorical_accuracy: 0.682 - ETA: 0s - loss: 0.7286 - sparse_categorical_accuracy: 0.681 - ETA: 0s - loss: 0.7272 - sparse_categorical_accuracy: 0.681 - ETA: 0s - loss: 0.7261 - sparse_categorical_accuracy: 0.682 - ETA: 0s - loss: 0.7269 - sparse_categorical_accuracy: 0.683 - ETA: 0s - loss: 0.7266 - sparse_categorical_accuracy: 0.682 - 9s 4ms/step - loss: 0.7289 - sparse_categorical_accuracy: 0.6815 - val_loss: 0.7501 - val_sparse_categorical_accuracy: 0.6564\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.76680 to 0.75009, saving model to best_e2e_rnn_atten_model.hdf5\n",
      "Epoch 7/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - ETA: 7s - loss: 0.6550 - sparse_categorical_accuracy: 0.700 - ETA: 7s - loss: 0.6525 - sparse_categorical_accuracy: 0.716 - ETA: 7s - loss: 0.7428 - sparse_categorical_accuracy: 0.666 - ETA: 7s - loss: 0.7627 - sparse_categorical_accuracy: 0.675 - ETA: 7s - loss: 0.7359 - sparse_categorical_accuracy: 0.700 - ETA: 7s - loss: 0.7276 - sparse_categorical_accuracy: 0.705 - ETA: 7s - loss: 0.7518 - sparse_categorical_accuracy: 0.690 - ETA: 7s - loss: 0.7685 - sparse_categorical_accuracy: 0.675 - ETA: 7s - loss: 0.7522 - sparse_categorical_accuracy: 0.692 - ETA: 6s - loss: 0.7393 - sparse_categorical_accuracy: 0.696 - ETA: 6s - loss: 0.7451 - sparse_categorical_accuracy: 0.690 - ETA: 6s - loss: 0.7332 - sparse_categorical_accuracy: 0.702 - ETA: 6s - loss: 0.7266 - sparse_categorical_accuracy: 0.712 - ETA: 6s - loss: 0.7088 - sparse_categorical_accuracy: 0.723 - ETA: 6s - loss: 0.7028 - sparse_categorical_accuracy: 0.720 - ETA: 6s - loss: 0.6975 - sparse_categorical_accuracy: 0.720 - ETA: 6s - loss: 0.7149 - sparse_categorical_accuracy: 0.707 - ETA: 5s - loss: 0.7107 - sparse_categorical_accuracy: 0.707 - ETA: 5s - loss: 0.7067 - sparse_categorical_accuracy: 0.710 - ETA: 5s - loss: 0.7104 - sparse_categorical_accuracy: 0.708 - ETA: 5s - loss: 0.7086 - sparse_categorical_accuracy: 0.704 - ETA: 5s - loss: 0.7162 - sparse_categorical_accuracy: 0.707 - ETA: 5s - loss: 0.7225 - sparse_categorical_accuracy: 0.704 - ETA: 5s - loss: 0.7172 - sparse_categorical_accuracy: 0.711 - ETA: 5s - loss: 0.7170 - sparse_categorical_accuracy: 0.709 - ETA: 4s - loss: 0.7090 - sparse_categorical_accuracy: 0.712 - ETA: 4s - loss: 0.7143 - sparse_categorical_accuracy: 0.711 - ETA: 4s - loss: 0.7143 - sparse_categorical_accuracy: 0.709 - ETA: 4s - loss: 0.7119 - sparse_categorical_accuracy: 0.708 - ETA: 4s - loss: 0.7168 - sparse_categorical_accuracy: 0.701 - ETA: 4s - loss: 0.7120 - sparse_categorical_accuracy: 0.702 - ETA: 4s - loss: 0.7079 - sparse_categorical_accuracy: 0.704 - ETA: 4s - loss: 0.7080 - sparse_categorical_accuracy: 0.707 - ETA: 3s - loss: 0.7045 - sparse_categorical_accuracy: 0.706 - ETA: 3s - loss: 0.7045 - sparse_categorical_accuracy: 0.705 - ETA: 3s - loss: 0.7102 - sparse_categorical_accuracy: 0.701 - ETA: 3s - loss: 0.7090 - sparse_categorical_accuracy: 0.700 - ETA: 3s - loss: 0.7127 - sparse_categorical_accuracy: 0.698 - ETA: 3s - loss: 0.7132 - sparse_categorical_accuracy: 0.699 - ETA: 3s - loss: 0.7092 - sparse_categorical_accuracy: 0.702 - ETA: 3s - loss: 0.7072 - sparse_categorical_accuracy: 0.700 - ETA: 3s - loss: 0.7074 - sparse_categorical_accuracy: 0.702 - ETA: 2s - loss: 0.7007 - sparse_categorical_accuracy: 0.705 - ETA: 2s - loss: 0.7013 - sparse_categorical_accuracy: 0.705 - ETA: 2s - loss: 0.7054 - sparse_categorical_accuracy: 0.703 - ETA: 2s - loss: 0.7047 - sparse_categorical_accuracy: 0.702 - ETA: 2s - loss: 0.7018 - sparse_categorical_accuracy: 0.702 - ETA: 2s - loss: 0.7014 - sparse_categorical_accuracy: 0.702 - ETA: 2s - loss: 0.7059 - sparse_categorical_accuracy: 0.701 - ETA: 2s - loss: 0.7064 - sparse_categorical_accuracy: 0.701 - ETA: 1s - loss: 0.7073 - sparse_categorical_accuracy: 0.702 - ETA: 1s - loss: 0.7103 - sparse_categorical_accuracy: 0.701 - ETA: 1s - loss: 0.7175 - sparse_categorical_accuracy: 0.700 - ETA: 1s - loss: 0.7173 - sparse_categorical_accuracy: 0.700 - ETA: 1s - loss: 0.7164 - sparse_categorical_accuracy: 0.701 - ETA: 1s - loss: 0.7149 - sparse_categorical_accuracy: 0.701 - ETA: 1s - loss: 0.7152 - sparse_categorical_accuracy: 0.700 - ETA: 1s - loss: 0.7153 - sparse_categorical_accuracy: 0.699 - ETA: 0s - loss: 0.7172 - sparse_categorical_accuracy: 0.698 - ETA: 0s - loss: 0.7153 - sparse_categorical_accuracy: 0.700 - ETA: 0s - loss: 0.7149 - sparse_categorical_accuracy: 0.699 - ETA: 0s - loss: 0.7126 - sparse_categorical_accuracy: 0.700 - ETA: 0s - loss: 0.7105 - sparse_categorical_accuracy: 0.700 - ETA: 0s - loss: 0.7101 - sparse_categorical_accuracy: 0.699 - ETA: 0s - loss: 0.7097 - sparse_categorical_accuracy: 0.699 - ETA: 0s - loss: 0.7095 - sparse_categorical_accuracy: 0.698 - 9s 4ms/step - loss: 0.7092 - sparse_categorical_accuracy: 0.6990 - val_loss: 0.7752 - val_sparse_categorical_accuracy: 0.6278\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.75009\n",
      "Epoch 8/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - ETA: 8s - loss: 0.9347 - sparse_categorical_accuracy: 0.600 - ETA: 8s - loss: 0.8809 - sparse_categorical_accuracy: 0.566 - ETA: 8s - loss: 0.7696 - sparse_categorical_accuracy: 0.644 - ETA: 8s - loss: 0.7834 - sparse_categorical_accuracy: 0.641 - ETA: 8s - loss: 0.8341 - sparse_categorical_accuracy: 0.646 - ETA: 7s - loss: 0.7723 - sparse_categorical_accuracy: 0.677 - ETA: 7s - loss: 0.7152 - sparse_categorical_accuracy: 0.704 - ETA: 7s - loss: 0.7156 - sparse_categorical_accuracy: 0.700 - ETA: 7s - loss: 0.7307 - sparse_categorical_accuracy: 0.692 - ETA: 7s - loss: 0.7256 - sparse_categorical_accuracy: 0.700 - ETA: 7s - loss: 0.7309 - sparse_categorical_accuracy: 0.697 - ETA: 6s - loss: 0.7298 - sparse_categorical_accuracy: 0.697 - ETA: 6s - loss: 0.7118 - sparse_categorical_accuracy: 0.705 - ETA: 6s - loss: 0.7074 - sparse_categorical_accuracy: 0.707 - ETA: 6s - loss: 0.6890 - sparse_categorical_accuracy: 0.713 - ETA: 6s - loss: 0.6968 - sparse_categorical_accuracy: 0.710 - ETA: 6s - loss: 0.7109 - sparse_categorical_accuracy: 0.702 - ETA: 6s - loss: 0.7229 - sparse_categorical_accuracy: 0.690 - ETA: 5s - loss: 0.7138 - sparse_categorical_accuracy: 0.694 - ETA: 5s - loss: 0.7056 - sparse_categorical_accuracy: 0.698 - ETA: 5s - loss: 0.7033 - sparse_categorical_accuracy: 0.695 - ETA: 5s - loss: 0.7010 - sparse_categorical_accuracy: 0.693 - ETA: 5s - loss: 0.6992 - sparse_categorical_accuracy: 0.694 - ETA: 5s - loss: 0.6977 - sparse_categorical_accuracy: 0.698 - ETA: 5s - loss: 0.6948 - sparse_categorical_accuracy: 0.700 - ETA: 5s - loss: 0.6940 - sparse_categorical_accuracy: 0.696 - ETA: 4s - loss: 0.6955 - sparse_categorical_accuracy: 0.696 - ETA: 4s - loss: 0.6976 - sparse_categorical_accuracy: 0.695 - ETA: 4s - loss: 0.6971 - sparse_categorical_accuracy: 0.693 - ETA: 4s - loss: 0.6942 - sparse_categorical_accuracy: 0.693 - ETA: 4s - loss: 0.6957 - sparse_categorical_accuracy: 0.690 - ETA: 4s - loss: 0.6923 - sparse_categorical_accuracy: 0.694 - ETA: 4s - loss: 0.6937 - sparse_categorical_accuracy: 0.697 - ETA: 4s - loss: 0.6865 - sparse_categorical_accuracy: 0.700 - ETA: 3s - loss: 0.6867 - sparse_categorical_accuracy: 0.701 - ETA: 3s - loss: 0.6847 - sparse_categorical_accuracy: 0.700 - ETA: 3s - loss: 0.6860 - sparse_categorical_accuracy: 0.700 - ETA: 3s - loss: 0.6794 - sparse_categorical_accuracy: 0.701 - ETA: 3s - loss: 0.6776 - sparse_categorical_accuracy: 0.701 - ETA: 3s - loss: 0.6806 - sparse_categorical_accuracy: 0.700 - ETA: 3s - loss: 0.6844 - sparse_categorical_accuracy: 0.700 - ETA: 3s - loss: 0.6801 - sparse_categorical_accuracy: 0.700 - ETA: 2s - loss: 0.6778 - sparse_categorical_accuracy: 0.698 - ETA: 2s - loss: 0.6796 - sparse_categorical_accuracy: 0.697 - ETA: 2s - loss: 0.6860 - sparse_categorical_accuracy: 0.696 - ETA: 2s - loss: 0.6831 - sparse_categorical_accuracy: 0.699 - ETA: 2s - loss: 0.6820 - sparse_categorical_accuracy: 0.698 - ETA: 2s - loss: 0.6876 - sparse_categorical_accuracy: 0.696 - ETA: 2s - loss: 0.6888 - sparse_categorical_accuracy: 0.696 - ETA: 2s - loss: 0.6850 - sparse_categorical_accuracy: 0.700 - ETA: 1s - loss: 0.6888 - sparse_categorical_accuracy: 0.701 - ETA: 1s - loss: 0.6864 - sparse_categorical_accuracy: 0.702 - ETA: 1s - loss: 0.6829 - sparse_categorical_accuracy: 0.703 - ETA: 1s - loss: 0.6831 - sparse_categorical_accuracy: 0.704 - ETA: 1s - loss: 0.6807 - sparse_categorical_accuracy: 0.704 - ETA: 1s - loss: 0.6820 - sparse_categorical_accuracy: 0.704 - ETA: 1s - loss: 0.6827 - sparse_categorical_accuracy: 0.704 - ETA: 1s - loss: 0.6867 - sparse_categorical_accuracy: 0.702 - ETA: 0s - loss: 0.6878 - sparse_categorical_accuracy: 0.701 - ETA: 0s - loss: 0.6860 - sparse_categorical_accuracy: 0.702 - ETA: 0s - loss: 0.6836 - sparse_categorical_accuracy: 0.704 - ETA: 0s - loss: 0.6827 - sparse_categorical_accuracy: 0.704 - ETA: 0s - loss: 0.6813 - sparse_categorical_accuracy: 0.704 - ETA: 0s - loss: 0.6775 - sparse_categorical_accuracy: 0.705 - ETA: 0s - loss: 0.6806 - sparse_categorical_accuracy: 0.703 - ETA: 0s - loss: 0.6784 - sparse_categorical_accuracy: 0.705 - 9s 4ms/step - loss: 0.6801 - sparse_categorical_accuracy: 0.7045 - val_loss: 0.7805 - val_sparse_categorical_accuracy: 0.6542\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.75009\n",
      "Epoch 9/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - ETA: 8s - loss: 0.8409 - sparse_categorical_accuracy: 0.700 - ETA: 7s - loss: 0.7713 - sparse_categorical_accuracy: 0.716 - ETA: 7s - loss: 0.7559 - sparse_categorical_accuracy: 0.711 - ETA: 7s - loss: 0.6778 - sparse_categorical_accuracy: 0.741 - ETA: 7s - loss: 0.6838 - sparse_categorical_accuracy: 0.740 - ETA: 7s - loss: 0.7212 - sparse_categorical_accuracy: 0.705 - ETA: 7s - loss: 0.7023 - sparse_categorical_accuracy: 0.714 - ETA: 7s - loss: 0.6877 - sparse_categorical_accuracy: 0.708 - ETA: 7s - loss: 0.6715 - sparse_categorical_accuracy: 0.718 - ETA: 6s - loss: 0.6599 - sparse_categorical_accuracy: 0.726 - ETA: 6s - loss: 0.6604 - sparse_categorical_accuracy: 0.718 - ETA: 6s - loss: 0.6819 - sparse_categorical_accuracy: 0.705 - ETA: 6s - loss: 0.6641 - sparse_categorical_accuracy: 0.715 - ETA: 6s - loss: 0.6663 - sparse_categorical_accuracy: 0.711 - ETA: 6s - loss: 0.6671 - sparse_categorical_accuracy: 0.713 - ETA: 6s - loss: 0.6597 - sparse_categorical_accuracy: 0.716 - ETA: 6s - loss: 0.6668 - sparse_categorical_accuracy: 0.713 - ETA: 5s - loss: 0.6644 - sparse_categorical_accuracy: 0.716 - ETA: 5s - loss: 0.6692 - sparse_categorical_accuracy: 0.712 - ETA: 5s - loss: 0.6798 - sparse_categorical_accuracy: 0.711 - ETA: 5s - loss: 0.6751 - sparse_categorical_accuracy: 0.715 - ETA: 5s - loss: 0.6722 - sparse_categorical_accuracy: 0.716 - ETA: 5s - loss: 0.6741 - sparse_categorical_accuracy: 0.714 - ETA: 5s - loss: 0.6671 - sparse_categorical_accuracy: 0.719 - ETA: 5s - loss: 0.6697 - sparse_categorical_accuracy: 0.716 - ETA: 4s - loss: 0.6698 - sparse_categorical_accuracy: 0.716 - ETA: 4s - loss: 0.6713 - sparse_categorical_accuracy: 0.714 - ETA: 4s - loss: 0.6698 - sparse_categorical_accuracy: 0.711 - ETA: 4s - loss: 0.6729 - sparse_categorical_accuracy: 0.709 - ETA: 4s - loss: 0.6715 - sparse_categorical_accuracy: 0.708 - ETA: 4s - loss: 0.6684 - sparse_categorical_accuracy: 0.711 - ETA: 4s - loss: 0.6724 - sparse_categorical_accuracy: 0.708 - ETA: 4s - loss: 0.6708 - sparse_categorical_accuracy: 0.710 - ETA: 3s - loss: 0.6743 - sparse_categorical_accuracy: 0.707 - ETA: 3s - loss: 0.6743 - sparse_categorical_accuracy: 0.709 - ETA: 3s - loss: 0.6786 - sparse_categorical_accuracy: 0.709 - ETA: 3s - loss: 0.6775 - sparse_categorical_accuracy: 0.708 - ETA: 3s - loss: 0.6769 - sparse_categorical_accuracy: 0.708 - ETA: 3s - loss: 0.6802 - sparse_categorical_accuracy: 0.706 - ETA: 3s - loss: 0.6781 - sparse_categorical_accuracy: 0.704 - ETA: 3s - loss: 0.6733 - sparse_categorical_accuracy: 0.704 - ETA: 3s - loss: 0.6737 - sparse_categorical_accuracy: 0.705 - ETA: 2s - loss: 0.6766 - sparse_categorical_accuracy: 0.706 - ETA: 2s - loss: 0.6738 - sparse_categorical_accuracy: 0.705 - ETA: 2s - loss: 0.6736 - sparse_categorical_accuracy: 0.705 - ETA: 2s - loss: 0.6747 - sparse_categorical_accuracy: 0.703 - ETA: 2s - loss: 0.6694 - sparse_categorical_accuracy: 0.705 - ETA: 2s - loss: 0.6693 - sparse_categorical_accuracy: 0.705 - ETA: 2s - loss: 0.6659 - sparse_categorical_accuracy: 0.708 - ETA: 2s - loss: 0.6614 - sparse_categorical_accuracy: 0.709 - ETA: 1s - loss: 0.6559 - sparse_categorical_accuracy: 0.711 - ETA: 1s - loss: 0.6544 - sparse_categorical_accuracy: 0.712 - ETA: 1s - loss: 0.6519 - sparse_categorical_accuracy: 0.714 - ETA: 1s - loss: 0.6527 - sparse_categorical_accuracy: 0.711 - ETA: 1s - loss: 0.6624 - sparse_categorical_accuracy: 0.708 - ETA: 1s - loss: 0.6639 - sparse_categorical_accuracy: 0.706 - ETA: 1s - loss: 0.6633 - sparse_categorical_accuracy: 0.705 - ETA: 1s - loss: 0.6655 - sparse_categorical_accuracy: 0.705 - ETA: 0s - loss: 0.6651 - sparse_categorical_accuracy: 0.706 - ETA: 0s - loss: 0.6653 - sparse_categorical_accuracy: 0.705 - ETA: 0s - loss: 0.6637 - sparse_categorical_accuracy: 0.706 - ETA: 0s - loss: 0.6602 - sparse_categorical_accuracy: 0.707 - ETA: 0s - loss: 0.6605 - sparse_categorical_accuracy: 0.707 - ETA: 0s - loss: 0.6666 - sparse_categorical_accuracy: 0.707 - ETA: 0s - loss: 0.6653 - sparse_categorical_accuracy: 0.707 - ETA: 0s - loss: 0.6656 - sparse_categorical_accuracy: 0.707 - 9s 4ms/step - loss: 0.6682 - sparse_categorical_accuracy: 0.7070 - val_loss: 0.7554 - val_sparse_categorical_accuracy: 0.6454\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.75009\n",
      "Epoch 10/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - ETA: 8s - loss: 0.6433 - sparse_categorical_accuracy: 0.733 - ETA: 7s - loss: 0.6439 - sparse_categorical_accuracy: 0.733 - ETA: 8s - loss: 0.6314 - sparse_categorical_accuracy: 0.733 - ETA: 7s - loss: 0.6488 - sparse_categorical_accuracy: 0.733 - ETA: 7s - loss: 0.6202 - sparse_categorical_accuracy: 0.726 - ETA: 7s - loss: 0.5976 - sparse_categorical_accuracy: 0.738 - ETA: 7s - loss: 0.6215 - sparse_categorical_accuracy: 0.728 - ETA: 7s - loss: 0.6226 - sparse_categorical_accuracy: 0.733 - ETA: 7s - loss: 0.6431 - sparse_categorical_accuracy: 0.725 - ETA: 7s - loss: 0.6719 - sparse_categorical_accuracy: 0.710 - ETA: 6s - loss: 0.6714 - sparse_categorical_accuracy: 0.712 - ETA: 6s - loss: 0.6667 - sparse_categorical_accuracy: 0.716 - ETA: 6s - loss: 0.6520 - sparse_categorical_accuracy: 0.723 - ETA: 6s - loss: 0.6532 - sparse_categorical_accuracy: 0.726 - ETA: 6s - loss: 0.6658 - sparse_categorical_accuracy: 0.720 - ETA: 6s - loss: 0.6665 - sparse_categorical_accuracy: 0.716 - ETA: 6s - loss: 0.6715 - sparse_categorical_accuracy: 0.717 - ETA: 6s - loss: 0.6616 - sparse_categorical_accuracy: 0.725 - ETA: 5s - loss: 0.6630 - sparse_categorical_accuracy: 0.722 - ETA: 5s - loss: 0.6598 - sparse_categorical_accuracy: 0.723 - ETA: 5s - loss: 0.6587 - sparse_categorical_accuracy: 0.725 - ETA: 5s - loss: 0.6476 - sparse_categorical_accuracy: 0.728 - ETA: 5s - loss: 0.6516 - sparse_categorical_accuracy: 0.724 - ETA: 5s - loss: 0.6629 - sparse_categorical_accuracy: 0.718 - ETA: 5s - loss: 0.6659 - sparse_categorical_accuracy: 0.714 - ETA: 4s - loss: 0.6655 - sparse_categorical_accuracy: 0.715 - ETA: 4s - loss: 0.6641 - sparse_categorical_accuracy: 0.714 - ETA: 4s - loss: 0.6609 - sparse_categorical_accuracy: 0.721 - ETA: 4s - loss: 0.6681 - sparse_categorical_accuracy: 0.721 - ETA: 4s - loss: 0.6666 - sparse_categorical_accuracy: 0.723 - ETA: 4s - loss: 0.6698 - sparse_categorical_accuracy: 0.720 - ETA: 4s - loss: 0.6633 - sparse_categorical_accuracy: 0.724 - ETA: 4s - loss: 0.6656 - sparse_categorical_accuracy: 0.722 - ETA: 4s - loss: 0.6630 - sparse_categorical_accuracy: 0.723 - ETA: 3s - loss: 0.6573 - sparse_categorical_accuracy: 0.727 - ETA: 3s - loss: 0.6538 - sparse_categorical_accuracy: 0.728 - ETA: 3s - loss: 0.6516 - sparse_categorical_accuracy: 0.729 - ETA: 3s - loss: 0.6494 - sparse_categorical_accuracy: 0.729 - ETA: 3s - loss: 0.6549 - sparse_categorical_accuracy: 0.728 - ETA: 3s - loss: 0.6599 - sparse_categorical_accuracy: 0.725 - ETA: 3s - loss: 0.6569 - sparse_categorical_accuracy: 0.726 - ETA: 3s - loss: 0.6528 - sparse_categorical_accuracy: 0.729 - ETA: 2s - loss: 0.6504 - sparse_categorical_accuracy: 0.727 - ETA: 2s - loss: 0.6530 - sparse_categorical_accuracy: 0.725 - ETA: 2s - loss: 0.6536 - sparse_categorical_accuracy: 0.723 - ETA: 2s - loss: 0.6532 - sparse_categorical_accuracy: 0.723 - ETA: 2s - loss: 0.6501 - sparse_categorical_accuracy: 0.727 - ETA: 2s - loss: 0.6530 - sparse_categorical_accuracy: 0.726 - ETA: 2s - loss: 0.6502 - sparse_categorical_accuracy: 0.727 - ETA: 2s - loss: 0.6575 - sparse_categorical_accuracy: 0.724 - ETA: 1s - loss: 0.6556 - sparse_categorical_accuracy: 0.724 - ETA: 1s - loss: 0.6566 - sparse_categorical_accuracy: 0.725 - ETA: 1s - loss: 0.6603 - sparse_categorical_accuracy: 0.723 - ETA: 1s - loss: 0.6571 - sparse_categorical_accuracy: 0.725 - ETA: 1s - loss: 0.6576 - sparse_categorical_accuracy: 0.723 - ETA: 1s - loss: 0.6661 - sparse_categorical_accuracy: 0.721 - ETA: 1s - loss: 0.6636 - sparse_categorical_accuracy: 0.722 - ETA: 1s - loss: 0.6639 - sparse_categorical_accuracy: 0.721 - ETA: 0s - loss: 0.6623 - sparse_categorical_accuracy: 0.720 - ETA: 0s - loss: 0.6667 - sparse_categorical_accuracy: 0.716 - ETA: 0s - loss: 0.6682 - sparse_categorical_accuracy: 0.715 - ETA: 0s - loss: 0.6677 - sparse_categorical_accuracy: 0.713 - ETA: 0s - loss: 0.6660 - sparse_categorical_accuracy: 0.714 - ETA: 0s - loss: 0.6660 - sparse_categorical_accuracy: 0.714 - ETA: 0s - loss: 0.6646 - sparse_categorical_accuracy: 0.714 - ETA: 0s - loss: 0.6652 - sparse_categorical_accuracy: 0.714 - 9s 4ms/step - loss: 0.6625 - sparse_categorical_accuracy: 0.7150 - val_loss: 0.7608 - val_sparse_categorical_accuracy: 0.6498\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.75009\n",
      "Epoch 11/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - ETA: 7s - loss: 0.8320 - sparse_categorical_accuracy: 0.633 - ETA: 8s - loss: 0.5947 - sparse_categorical_accuracy: 0.733 - ETA: 7s - loss: 0.6225 - sparse_categorical_accuracy: 0.688 - ETA: 7s - loss: 0.6634 - sparse_categorical_accuracy: 0.708 - ETA: 7s - loss: 0.6526 - sparse_categorical_accuracy: 0.726 - ETA: 7s - loss: 0.6626 - sparse_categorical_accuracy: 0.722 - ETA: 7s - loss: 0.6607 - sparse_categorical_accuracy: 0.728 - ETA: 7s - loss: 0.6746 - sparse_categorical_accuracy: 0.725 - ETA: 7s - loss: 0.6649 - sparse_categorical_accuracy: 0.725 - ETA: 6s - loss: 0.6569 - sparse_categorical_accuracy: 0.733 - ETA: 6s - loss: 0.6501 - sparse_categorical_accuracy: 0.733 - ETA: 6s - loss: 0.6496 - sparse_categorical_accuracy: 0.722 - ETA: 6s - loss: 0.6397 - sparse_categorical_accuracy: 0.728 - ETA: 6s - loss: 0.6381 - sparse_categorical_accuracy: 0.733 - ETA: 6s - loss: 0.6375 - sparse_categorical_accuracy: 0.735 - ETA: 6s - loss: 0.6413 - sparse_categorical_accuracy: 0.731 - ETA: 6s - loss: 0.6333 - sparse_categorical_accuracy: 0.731 - ETA: 5s - loss: 0.6414 - sparse_categorical_accuracy: 0.729 - ETA: 5s - loss: 0.6440 - sparse_categorical_accuracy: 0.729 - ETA: 5s - loss: 0.6383 - sparse_categorical_accuracy: 0.731 - ETA: 5s - loss: 0.6376 - sparse_categorical_accuracy: 0.734 - ETA: 5s - loss: 0.6348 - sparse_categorical_accuracy: 0.731 - ETA: 5s - loss: 0.6325 - sparse_categorical_accuracy: 0.726 - ETA: 5s - loss: 0.6321 - sparse_categorical_accuracy: 0.725 - ETA: 5s - loss: 0.6317 - sparse_categorical_accuracy: 0.722 - ETA: 4s - loss: 0.6297 - sparse_categorical_accuracy: 0.724 - ETA: 4s - loss: 0.6377 - sparse_categorical_accuracy: 0.716 - ETA: 4s - loss: 0.6393 - sparse_categorical_accuracy: 0.715 - ETA: 4s - loss: 0.6424 - sparse_categorical_accuracy: 0.714 - ETA: 4s - loss: 0.6417 - sparse_categorical_accuracy: 0.717 - ETA: 4s - loss: 0.6537 - sparse_categorical_accuracy: 0.716 - ETA: 4s - loss: 0.6553 - sparse_categorical_accuracy: 0.712 - ETA: 4s - loss: 0.6498 - sparse_categorical_accuracy: 0.715 - ETA: 4s - loss: 0.6463 - sparse_categorical_accuracy: 0.717 - ETA: 3s - loss: 0.6503 - sparse_categorical_accuracy: 0.717 - ETA: 3s - loss: 0.6561 - sparse_categorical_accuracy: 0.713 - ETA: 3s - loss: 0.6586 - sparse_categorical_accuracy: 0.710 - ETA: 3s - loss: 0.6555 - sparse_categorical_accuracy: 0.713 - ETA: 3s - loss: 0.6534 - sparse_categorical_accuracy: 0.712 - ETA: 3s - loss: 0.6549 - sparse_categorical_accuracy: 0.710 - ETA: 3s - loss: 0.6540 - sparse_categorical_accuracy: 0.708 - ETA: 3s - loss: 0.6491 - sparse_categorical_accuracy: 0.712 - ETA: 2s - loss: 0.6508 - sparse_categorical_accuracy: 0.713 - ETA: 2s - loss: 0.6564 - sparse_categorical_accuracy: 0.711 - ETA: 2s - loss: 0.6557 - sparse_categorical_accuracy: 0.711 - ETA: 2s - loss: 0.6558 - sparse_categorical_accuracy: 0.713 - ETA: 2s - loss: 0.6551 - sparse_categorical_accuracy: 0.712 - ETA: 2s - loss: 0.6516 - sparse_categorical_accuracy: 0.716 - ETA: 2s - loss: 0.6505 - sparse_categorical_accuracy: 0.719 - ETA: 2s - loss: 0.6452 - sparse_categorical_accuracy: 0.722 - ETA: 1s - loss: 0.6439 - sparse_categorical_accuracy: 0.722 - ETA: 1s - loss: 0.6430 - sparse_categorical_accuracy: 0.720 - ETA: 1s - loss: 0.6419 - sparse_categorical_accuracy: 0.721 - ETA: 1s - loss: 0.6389 - sparse_categorical_accuracy: 0.722 - ETA: 1s - loss: 0.6401 - sparse_categorical_accuracy: 0.721 - ETA: 1s - loss: 0.6376 - sparse_categorical_accuracy: 0.722 - ETA: 1s - loss: 0.6348 - sparse_categorical_accuracy: 0.724 - ETA: 1s - loss: 0.6338 - sparse_categorical_accuracy: 0.724 - ETA: 0s - loss: 0.6320 - sparse_categorical_accuracy: 0.724 - ETA: 0s - loss: 0.6331 - sparse_categorical_accuracy: 0.722 - ETA: 0s - loss: 0.6336 - sparse_categorical_accuracy: 0.723 - ETA: 0s - loss: 0.6316 - sparse_categorical_accuracy: 0.724 - ETA: 0s - loss: 0.6298 - sparse_categorical_accuracy: 0.725 - ETA: 0s - loss: 0.6340 - sparse_categorical_accuracy: 0.724 - ETA: 0s - loss: 0.6354 - sparse_categorical_accuracy: 0.723 - ETA: 0s - loss: 0.6370 - sparse_categorical_accuracy: 0.722 - 9s 4ms/step - loss: 0.6351 - sparse_categorical_accuracy: 0.7235 - val_loss: 0.7929 - val_sparse_categorical_accuracy: 0.6520\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.75009\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adam = ko.Nadam(clipnorm=1.0)\n",
    "model.compile(adam, loss=\"sparse_categorical_crossentropy\", metrics=[\"sparse_categorical_accuracy\"])\n",
    "\n",
    "file_path = \"best_e2e_rnn_atten_model.hdf5\"\n",
    "check_point = kc.ModelCheckpoint(file_path, monitor = \"val_loss\", verbose = 1, save_best_only = True, mode = \"min\")\n",
    "early_stop = kc.EarlyStopping(monitor = \"val_loss\", mode = \"min\", patience=5)\n",
    "history = model.fit(X_train, y_tra, batch_size=30, epochs=40, validation_data=(X_dev, y_dev),\n",
    "                    callbacks = [check_point, early_stop])\n",
    "\n",
    "file_paths.append(file_path)\n",
    "histories.append(np.min(np.asarray(history.history['val_loss'])))\n",
    "\n",
    "del model, history\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Procedure for testing model on different test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "_uuid": "79cb8d339da04ad2d52e3675a4295f55a6ccd148"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load best model: best_e2e_rnn_atten_model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Isak\\.conda\\envs\\gender\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:110: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\Users\\Isak\\.conda\\envs\\gender\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:110: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\Users\\Isak\\.conda\\envs\\gender\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:110: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "print(\"load best model: \" + str(file_paths[np.argmin(histories)]))\n",
    "model = models.load_model(\n",
    "    file_paths[np.argmin(histories)], cos[np.argmin(histories)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions for calculating statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_f1(gold_df, pred_df):\n",
    "    gold = gold_df.values[:,1:].astype(int)\n",
    "    predictions = pred_df.values[:,1:].astype(int)\n",
    "    \n",
    "    f1 = f1_score(gold, predictions, average=None)\n",
    "    \n",
    "    return f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create bootstrap confidence interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import choices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confidence_interval(statistics, confidence_level = 0.95):\n",
    "    bounds = 50 * (1 - confidence_level)\n",
    "    lower = np.percentile(statistics, bounds)\n",
    "    median = np.percentile(statistics, 50)\n",
    "    upper = np.percentile(statistics, 100 - bounds)\n",
    "    \n",
    "    return lower, median, upper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_ci(f1, f1_male, f1_female, bias, score, confidence_level = 0.95):\n",
    "    lower, median,upper = confidence_interval(f1, confidence_level)\n",
    "    print(\"%.3f, %s%% Bootstrap CI for F1: [%.3f, %.3f]\" % (median, confidence_level*100, lower, upper))\n",
    "    \n",
    "    lower, median,upper = confidence_interval(f1_male, confidence_level)\n",
    "    print(\"%.3f, %s%% Bootstrap CI for F1 (male): [%.3f, %.3f]\" % (median, confidence_level*100, lower, upper))\n",
    "    \n",
    "    lower, median,upper = confidence_interval(f1_female, confidence_level)\n",
    "    print(\"%.3f, %s%% Bootstrap CI for F1 (female): [%.3f, %.3f]\" % (median, confidence_level*100, lower, upper))\n",
    "    \n",
    "    lower, median,upper = confidence_interval(bias, confidence_level)\n",
    "    print(\"%.3f, %s%% Bootstrap CI for bias: [%.3f, %.3f]\" % (median, confidence_level*100, lower, upper))\n",
    "    \n",
    "    lower, median,upper = confidence_interval(score, confidence_level)\n",
    "    print(\"%.3f, %s%% Bootstrap CI for logloss: [%.3f, %.3f]\" % (median, confidence_level*100, lower, upper))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_scheme = \"Original\"\n",
    "\n",
    "test_df_path = os.path.join(GAP_DATA_FOLDER, \"gap-development.tsv\")\n",
    "\n",
    "test_df = pd.read_csv(test_df_path, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_list = list()\n",
    "test_df, test_tokenized = add_sent_columns(test_df, 'Text', 'Pronoun-offset', 'A-offset', 'B-offset')\n",
    "\n",
    "# df apply will call the first row twice, remove the first one\n",
    "test_tokenized = test_tokenized[1:]\n",
    "\n",
    "test_p_tokenized = sentences_to_sequences([row[0] for row in test_tokenized])\n",
    "test_a_tokenized = sentences_to_sequences([row[1] for row in test_tokenized])\n",
    "test_b_tokenized = sentences_to_sequences([row[2] for row in test_tokenized])\n",
    "\n",
    "seq_p_test = sequence.pad_sequences(test_p_tokenized, maxlen = max_len, padding='post')\n",
    "seq_a_test = sequence.pad_sequences(test_a_tokenized, maxlen = max_len, padding='post')\n",
    "seq_b_test = sequence.pad_sequences(test_b_tokenized, maxlen = max_len, padding='post')\n",
    "\n",
    "test_p_pos = poses_to_sequences([row[0] for row in test_tokenized])\n",
    "test_a_pos = poses_to_sequences([row[1] for row in test_tokenized])\n",
    "test_b_pos = poses_to_sequences([row[2] for row in test_tokenized])\n",
    "\n",
    "pos_p_test = sequence.pad_sequences(test_p_pos, maxlen = max_len, padding='post')\n",
    "pos_a_test = sequence.pad_sequences(test_a_pos, maxlen = max_len, padding='post')\n",
    "pos_b_test = sequence.pad_sequences(test_b_pos, maxlen = max_len, padding='post')\n",
    "\n",
    "index_p_test = test_df['Pronoun-Sent-Offset'].values\n",
    "index_a_test = test_df['A-Sent-Offset'].values\n",
    "index_b_test = test_df['B-Sent-Offset'].values\n",
    "\n",
    "pa_pos_test = create_dist_features(test_df, 'Text', 'Pronoun-offset', 'A-offset')\n",
    "pb_pos_test = create_dist_features(test_df, 'Text', 'Pronoun-offset', 'B-offset')\n",
    "\n",
    "X_test = [seq_p_test, seq_a_test, seq_b_test, pos_p_test, pos_a_test, pos_b_test, index_p_test, index_a_test, index_b_test, pa_pos_test, pb_pos_test]\n",
    "\n",
    "y_test = test_df.apply(_row_to_y, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8bb147615d4920a2f3500a0fba12d31cb0869304"
   },
   "source": [
    "###  Make Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "_uuid": "4c465342c3a99a4996a104fdb00ceab4e85b9649"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - ETA:  - 3s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "y_preds = model.predict(X_test, batch_size = 1024, verbose = 1)\n",
    "\n",
    "sub_df_path = os.path.join(SUB_DATA_FOLDER, 'sample_submission_stage_1.csv')\n",
    "sub_df = pd.read_csv(sub_df_path)\n",
    "sub_df.loc[:, 'A'] = pd.Series(y_preds[:, 0])\n",
    "sub_df.loc[:, 'B'] = pd.Series(y_preds[:, 1])\n",
    "sub_df.loc[:, 'NEITHER'] = pd.Series(y_preds[:, 2])\n",
    "\n",
    "sub_df.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary = np.zeros((2000,3),dtype=int)\n",
    "for i, row in sub_df.iterrows():\n",
    "    max_ = max(row['A'], row['B'], row['NEITHER'])\n",
    "    if max_ == row['A']:\n",
    "        binary[i,0] = 1\n",
    "    elif max_ == row['B']:\n",
    "        binary[i,1] = 1\n",
    "    else:\n",
    "        binary[i,2] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pd.DataFrame(binary, columns=['A','B','NEITHER'])\n",
    "predictions.to_csv('data/predictions_end2end_trained1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the necessary dataframes\n",
    "predictions = pd.read_csv('data/predictions_end2end_trained1.csv', index_col=0)\n",
    "right_answers = pd.read_csv('data/right_answers.csv')\n",
    "full_data = pd.read_csv('data/gendered-pronoun-resolution/test_stage_1.tsv', sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_row(row):\n",
    "    row = [max(min(p, 1 - 10e-15), 10e-15) for p in row]\n",
    "    row = [x/sum(row) for x in row]\n",
    "    return row\n",
    "\n",
    "def calculate_score(right_answers, sam_submission):\n",
    "    \n",
    "    # Have to use [:,0:] in the following lines, not [:,1:]. This is a consequence of how the files are loaded\n",
    "    y = right_answers.values[:,0:].astype(int)\n",
    "    submission = sam_submission.values[:,0:].astype(float)\n",
    "\n",
    "    submission = np.apply_along_axis(func1d=preprocess_row, axis=1, arr=submission)\n",
    "    submission = np.log(submission)\n",
    "    temp = np.multiply(submission, y)\n",
    "\n",
    "    return np.sum(temp)/-submission.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bias(gold, pred, data_df):\n",
    "    # Define male and female pronouns\n",
    "    male_pn = ['He', 'His', 'he', 'him', 'his']\n",
    "    female_pn = ['Her', 'She', 'her', 'she']\n",
    "    \n",
    "    # Find list of indices\n",
    "    m_idx = []\n",
    "    fm_idx = []\n",
    "    #for i in range(data_df.shape[0]):\n",
    "    for i in list(data_df.index):\n",
    "        #pn = data_df['Pronoun'][i]\n",
    "        pn = data_df.at[i, \"Pronoun\"]\n",
    "        if not isinstance(pn, str):\n",
    "            pn = pn[0]\n",
    "        #print(\"Row\", i, \",\", pn)\n",
    "        if pn in male_pn:\n",
    "            #m_idx.append(list(data_df.index)[i])\n",
    "            m_idx.append(i)\n",
    "        else:\n",
    "            #m_idx.append(list(data_df.index)[i])\n",
    "            fm_idx.append(i)\n",
    "            \n",
    "    # Male pred\n",
    "    male_pred = pred.loc[m_idx, :]\n",
    "    male_pred = male_pred.values[:, :].astype(int)\n",
    "    male_gold = gold.loc[m_idx, :]\n",
    "    male_gold = male_gold.values[:, :].astype(int)\n",
    "\n",
    "    # Female pred\n",
    "    female_pred = pred.loc[fm_idx, :]\n",
    "    female_pred = female_pred.values[:, :].astype(int)\n",
    "    female_gold = gold.loc[fm_idx, :]\n",
    "    female_gold = female_gold.values[:, :].astype(int)\n",
    "\n",
    "    # Calculate scores\n",
    "    f1_m = f1_score(male_gold, male_pred, average = None)\n",
    "    f1_fm = f1_score(female_gold, female_pred, average = None)\n",
    "    \n",
    "    return f1_fm / f1_m, f1_m, f1_fm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap(gold_df, pred_df, all_data_df, B=10000):\n",
    "    f1_stats = []\n",
    "    f1_male_stats = []\n",
    "    f1_female_stats = []\n",
    "    bias_stats = []\n",
    "    score_stats = []\n",
    "    sample_size = len(gold_df)\n",
    "    indeces = list(range(sample_size))\n",
    "    \n",
    "    for idx in range(B):\n",
    "        rows = choices(indeces, k=len(gold_df))\n",
    "        #boot_gold_df = gold_df[gold_df.index.isin(rows)]\n",
    "        boot_gold_df = gold_df.iloc[rows]\n",
    "        boot_gold_df.__delitem__(\"ID\")\n",
    "        boot_pred_df = pred_df.iloc[rows]\n",
    "        boot_data_df = all_data_df.iloc[rows]\n",
    "        \n",
    "        f1 = calculate_f1(boot_gold_df, boot_pred_df)     \n",
    "        f1_stats.append(f1)\n",
    "        \n",
    "        #boot_pred_df = pd.DataFrame(binary, columns=['A','B','NEITHER'], index = list(df.index))\n",
    "        [bias, male, female] = calculate_bias(boot_gold_df, boot_pred_df, boot_data_df)   \n",
    "        bias_stats.append(bias)\n",
    "        f1_male_stats.append(male)\n",
    "        f1_female_stats.append(female)\n",
    "        \n",
    "        score = calculate_score(boot_gold_df, boot_pred_df)\n",
    "        score_stats.append(score)\n",
    "        \n",
    "        if not idx % 100:\n",
    "            print(\"Progress: %i / %i iterations.\" % (idx, len(gold_df)))\n",
    "    \n",
    "    return [f1_stats, f1_male_stats, f1_female_stats, bias_stats, score_stats]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 0 / 2000 iterations.\n",
      "Progress: 100 / 2000 iterations.\n",
      "Progress: 200 / 2000 iterations.\n",
      "Progress: 300 / 2000 iterations.\n",
      "Progress: 400 / 2000 iterations.\n",
      "Progress: 500 / 2000 iterations.\n",
      "Progress: 600 / 2000 iterations.\n",
      "Progress: 700 / 2000 iterations.\n",
      "Progress: 800 / 2000 iterations.\n",
      "Progress: 900 / 2000 iterations.\n"
     ]
    }
   ],
   "source": [
    "[f1, f1_male, f1_female, bias, score] = bootstrap(right_answers, predictions, full_data, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance for Original test data, trained on Original data:\n",
      "0.578, 95.0% Bootstrap CI for F1: [0.339, 0.719]\n",
      "0.711, 95.0% Bootstrap CI for F1 (male): [0.211, 0.774]\n",
      "0.652, 95.0% Bootstrap CI for F1 (female): [0.348, 0.727]\n",
      "0.955, 95.0% Bootstrap CI for bias: [0.821, 2.362]\n",
      "10.477, 95.0% Bootstrap CI for logloss: [9.848, 11.074]\n"
     ]
    }
   ],
   "source": [
    "print(\"Performance for\", test_scheme, \"test data, trained on\", train_scheme, \"data:\")\n",
    "print_ci(f1, f1_male, f1_female, bias, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
