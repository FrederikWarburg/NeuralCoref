{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d48a6972378bf6116b7a6b9883992d548128640c"
   },
   "source": [
    "This Kernel implements a modified version of **a state-of-art end-to-end neural correference resolution model** published in 2017: https://www.aclweb.org/anthology/D17-1018.\n",
    "This completition only focus on a specific case of  the generic reference resolution problem, and we only need pick out the correct mention from two candidates, which simplifies the model implementation.\n",
    "\n",
    "You can compare the result of this model  with the result by other non-RNN based DL models implemented in another kernel: https://www.kaggle.com/keyit92/coreference-resolution-by-mlp-cnn-coattention-nn. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['predictions_majority_vote1.csv', 'predictions_majority_vote2.csv', '.DS_Store', 'crawl-300d-2M.vec', 'gendered-pronoun-resolution', 'gap', 'gap-new', 'predictions_allen_default1.csv', 'predictions_allen_default2.csv', 'right_answers.csv']\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "import gc\n",
    "print(os.listdir(\"data\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "DATA_ROOT = 'data'\n",
    "GAP_DATA_FOLDER = os.path.join(DATA_ROOT, 'gap')\n",
    "SUB_DATA_FOLDER = os.path.join(DATA_ROOT, 'gendered-pronoun-resolution')\n",
    "#FAST_TEXT_DATA_FOLDER = os.path.join(DATA_ROOT, 'fasttext-crawl-300d-2M.vec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "461cb23b791e2d210e712c933e62de59d19aa20d"
   },
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "e42e2e7f636cf0702cc472f3d855b451554927dd"
   },
   "outputs": [],
   "source": [
    "test_df_path = os.path.join(GAP_DATA_FOLDER, 'gap-development.tsv')\n",
    "train_df_path = os.path.join(GAP_DATA_FOLDER, 'gap-test.tsv')\n",
    "dev_df_path = os.path.join(GAP_DATA_FOLDER, 'gap-validation.tsv')\n",
    "\n",
    "train_df = pd.read_csv(train_df_path, sep='\\t')\n",
    "test_df = pd.read_csv(test_df_path, sep='\\t')\n",
    "dev_df = pd.read_csv(dev_df_path, sep='\\t')\n",
    "\n",
    "# pd.options.display.max_colwidth = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "2eb32fae335b77dae0a60ade52b7ea0a32d8cb3d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Text</th>\n",
       "      <th>Pronoun</th>\n",
       "      <th>Pronoun-offset</th>\n",
       "      <th>A</th>\n",
       "      <th>A-offset</th>\n",
       "      <th>A-coref</th>\n",
       "      <th>B</th>\n",
       "      <th>B-offset</th>\n",
       "      <th>B-coref</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>development-1996</td>\n",
       "      <td>Faye's third husband, Paul Resnick, reported t...</td>\n",
       "      <td>her</td>\n",
       "      <td>433</td>\n",
       "      <td>Nicole</td>\n",
       "      <td>255</td>\n",
       "      <td>False</td>\n",
       "      <td>Faye</td>\n",
       "      <td>328</td>\n",
       "      <td>True</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Faye_Resnick</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>development-1997</td>\n",
       "      <td>The plot of the film focuses on the life of a ...</td>\n",
       "      <td>her</td>\n",
       "      <td>246</td>\n",
       "      <td>Doris Chu</td>\n",
       "      <td>111</td>\n",
       "      <td>False</td>\n",
       "      <td>Mei</td>\n",
       "      <td>215</td>\n",
       "      <td>True</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Two_Lies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>development-1998</td>\n",
       "      <td>Grant played the part in Trevor Nunn's movie a...</td>\n",
       "      <td>she</td>\n",
       "      <td>348</td>\n",
       "      <td>Maria</td>\n",
       "      <td>259</td>\n",
       "      <td>True</td>\n",
       "      <td>Imelda Staunton</td>\n",
       "      <td>266</td>\n",
       "      <td>False</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Sir_Andrew_Aguecheek</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>development-1999</td>\n",
       "      <td>The fashion house specialised in hand-printed ...</td>\n",
       "      <td>She</td>\n",
       "      <td>284</td>\n",
       "      <td>Helen</td>\n",
       "      <td>145</td>\n",
       "      <td>True</td>\n",
       "      <td>Suzanne Bartsch</td>\n",
       "      <td>208</td>\n",
       "      <td>False</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Helen_David</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>development-2000</td>\n",
       "      <td>Watkins was a close friend of Hess' first wife...</td>\n",
       "      <td>her</td>\n",
       "      <td>373</td>\n",
       "      <td>Elizabeth</td>\n",
       "      <td>293</td>\n",
       "      <td>False</td>\n",
       "      <td>Watkins</td>\n",
       "      <td>347</td>\n",
       "      <td>True</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Linda_Watkins</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    ID                                               Text  \\\n",
       "1995  development-1996  Faye's third husband, Paul Resnick, reported t...   \n",
       "1996  development-1997  The plot of the film focuses on the life of a ...   \n",
       "1997  development-1998  Grant played the part in Trevor Nunn's movie a...   \n",
       "1998  development-1999  The fashion house specialised in hand-printed ...   \n",
       "1999  development-2000  Watkins was a close friend of Hess' first wife...   \n",
       "\n",
       "     Pronoun  Pronoun-offset          A  A-offset  A-coref                B  \\\n",
       "1995     her             433     Nicole       255    False             Faye   \n",
       "1996     her             246  Doris Chu       111    False              Mei   \n",
       "1997     she             348      Maria       259     True  Imelda Staunton   \n",
       "1998     She             284      Helen       145     True  Suzanne Bartsch   \n",
       "1999     her             373  Elizabeth       293    False          Watkins   \n",
       "\n",
       "      B-offset  B-coref                                                URL  \n",
       "1995       328     True          http://en.wikipedia.org/wiki/Faye_Resnick  \n",
       "1996       215     True              http://en.wikipedia.org/wiki/Two_Lies  \n",
       "1997       266    False  http://en.wikipedia.org/wiki/Sir_Andrew_Aguecheek  \n",
       "1998       208    False           http://en.wikipedia.org/wiki/Helen_David  \n",
       "1999       347     True         http://en.wikipedia.org/wiki/Linda_Watkins  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d60addb918fc181bd736f5497c3114dbf3d3bfd4"
   },
   "source": [
    "# Explore Features for Building Mention-Pair Distributed Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "8b82c9f910c289c5b21ec982890911bfb579c32b"
   },
   "outputs": [],
   "source": [
    "spacy_model = \"en_core_web_lg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "177d5f8cca6584d7e542ea1ed6112e091e78d787"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.en import English\n",
    "from spacy.pipeline import DependencyParser\n",
    "import spacy\n",
    "from nltk import Tree\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing import text as ktext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.3'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "1668e73e26b428fa0739c1c38653217d14d50aaa"
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load(spacy_model)\n",
    "\n",
    "def bs(list_, target_):\n",
    "    lo, hi = 0, len(list_) -1\n",
    "    \n",
    "    while lo < hi:\n",
    "        mid = lo + int((hi - lo) / 2)\n",
    "        \n",
    "        if target_ < list_[mid]:\n",
    "            hi = mid\n",
    "        elif target_ > list_[mid]:\n",
    "            lo = mid + 1\n",
    "        else:\n",
    "            return mid + 1\n",
    "    return lo\n",
    "\n",
    "def bs_(list_, target_):\n",
    "    lo, hi = 0, len(list_) -1\n",
    "    \n",
    "    while lo < hi:\n",
    "        mid = lo + int((hi - lo) / 2)\n",
    "        \n",
    "        if target_ < list_[mid]:\n",
    "            hi = mid\n",
    "        elif target_ > list_[mid]:\n",
    "            lo = mid + 1\n",
    "        else:\n",
    "            return mid\n",
    "    return lo\n",
    "\n",
    "def ohe_dist(dist, buckets):\n",
    "    idx = bs_(buckets, dist)\n",
    "    oh = np.zeros(shape=(len(buckets),), dtype=np.float32)\n",
    "    oh[idx] = 1\n",
    "    \n",
    "    return oh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2755927ba88262427347c567ae777c92510ffa68"
   },
   "source": [
    " ##  Position Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "15165f2b90503a90de589b37558ebcea28daa867"
   },
   "source": [
    "Encode the absolute positions in the sentence and the relative position between the pronoun and the entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "c1c14436f57e74e80ac4425f77df98768c71f7b9"
   },
   "outputs": [],
   "source": [
    "num_pos_features = 45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "9f3d21e9a626a3290c81c6bc98b817c3e47275be"
   },
   "outputs": [],
   "source": [
    "def extrac_positional_features(text, char_offset1, char_offset2):\n",
    "    doc = nlp(text)\n",
    "    max_len = 64\n",
    "    \n",
    "    # char offset to token offset\n",
    "    lens = [token.idx for token in doc]\n",
    "    mention_offset1 = bs(lens, char_offset1) - 1\n",
    "    mention_offset2 = bs(lens, char_offset2) - 1\n",
    "    \n",
    "    # token offset to sentence offset\n",
    "    lens = [len(sent) for sent in doc.sents]\n",
    "    acc_lens = [len_ for len_ in lens]\n",
    "    pre_len = 0\n",
    "    for i in range(0, len(acc_lens)):\n",
    "        pre_len += acc_lens[i]\n",
    "        acc_lens[i] = pre_len\n",
    "    sent_index1 = bs(acc_lens, mention_offset1)\n",
    "    sent_index2 = bs(acc_lens, mention_offset2)\n",
    "    \n",
    "    sent1 = list(doc.sents)[sent_index1]\n",
    "    sent2 = list(doc.sents)[sent_index2]\n",
    "    \n",
    "    # buckets\n",
    "    bucket_dist = [1, 2, 3, 4, 5, 8, 16, 32, 64]\n",
    "    \n",
    "    # relative distance\n",
    "    dist = mention_offset2 - mention_offset1\n",
    "    dist_oh = ohe_dist(dist, bucket_dist)\n",
    "    \n",
    "    # buckets\n",
    "    bucket_pos = [0, 1, 2, 3, 4, 5, 8, 16, 32]\n",
    "    \n",
    "    # absolute position in the sentence\n",
    "    sent_pos1 = mention_offset1 + 1\n",
    "    if sent_index1 > 0:\n",
    "        sent_pos1 = mention_offset1 - acc_lens[sent_index1-1]\n",
    "    sent_pos_oh1 = ohe_dist(sent_pos1, bucket_pos)\n",
    "    sent_pos_inv1 = len(sent1) - sent_pos1\n",
    "    assert sent_pos_inv1 >= 0\n",
    "    sent_pos_inv_oh1 = ohe_dist(sent_pos_inv1, bucket_pos)\n",
    "    \n",
    "    sent_pos2 = mention_offset2 + 1\n",
    "    if sent_index2 > 0:\n",
    "        sent_pos2 = mention_offset2 - acc_lens[sent_index2-1]\n",
    "    sent_pos_oh2 = ohe_dist(sent_pos2, bucket_pos)\n",
    "    sent_pos_inv2 = len(sent2) - sent_pos2\n",
    "    if sent_pos_inv2 < 0:\n",
    "        print(sent_pos_inv2)\n",
    "        print(len(sent2))\n",
    "        print(sent_pos2)\n",
    "        raise ValueError\n",
    "    sent_pos_inv_oh2 = ohe_dist(sent_pos_inv2, bucket_pos)\n",
    "    \n",
    "    sent_pos_ratio1 = sent_pos1 / len(sent1)\n",
    "    sent_pos_ratio2 = sent_pos2 / len(sent2)\n",
    "    \n",
    "    return dist_oh, sent_pos_oh1, sent_pos_oh2, sent_pos_inv_oh1, sent_pos_inv_oh2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "4a9ec346bff260b62ebea6d9223c6a11e683a9ee"
   },
   "outputs": [],
   "source": [
    "def create_dist_features(df, text_column, pronoun_offset_column, name_offset_column):\n",
    "    text_offset_list = df[[text_column, pronoun_offset_column, name_offset_column]].values.tolist()\n",
    "    num_features = num_pos_features\n",
    "    \n",
    "    pos_feature_matrix = np.zeros(shape=(len(text_offset_list), num_features))\n",
    "    for text_offset_index in range(len(text_offset_list)):\n",
    "        text_offset = text_offset_list[text_offset_index]\n",
    "        dist_oh, sent_pos_oh1, sent_pos_oh2, sent_pos_inv_oh1, sent_pos_inv_oh2 = extrac_positional_features(text_offset[0], text_offset[1], text_offset[2])\n",
    "        \n",
    "        feature_index = 0\n",
    "        pos_feature_matrix[text_offset_index, feature_index:feature_index+len(dist_oh)] = np.asarray(dist_oh)\n",
    "        feature_index += len(dist_oh)\n",
    "        pos_feature_matrix[text_offset_index, feature_index:feature_index+len(sent_pos_oh1)] = np.asarray(sent_pos_oh1)\n",
    "        feature_index += len(sent_pos_oh1)\n",
    "        pos_feature_matrix[text_offset_index, feature_index:feature_index+len(sent_pos_oh2)] = np.asarray(sent_pos_oh2)\n",
    "        feature_index += len(sent_pos_oh2)\n",
    "        pos_feature_matrix[text_offset_index, feature_index:feature_index+len(sent_pos_inv_oh1)] = np.asarray(sent_pos_inv_oh1)\n",
    "        feature_index += len(sent_pos_inv_oh1)\n",
    "        pos_feature_matrix[text_offset_index, feature_index:feature_index+len(sent_pos_inv_oh2)] = np.asarray(sent_pos_inv_oh2)\n",
    "        feature_index += len(sent_pos_inv_oh2)\n",
    "    \n",
    "    return pos_feature_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "488aaec745f7f6eea342a5746bcf7bef206c60d3"
   },
   "source": [
    "## Extract Sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "51de0d6b4223279969593956248e0fb99284c5d7"
   },
   "source": [
    "Select the surrounding 100 words around the mention in the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "2edcfef36582c91246e7ab772d5a23c27a272f92"
   },
   "outputs": [],
   "source": [
    "max_len = 50 # longer than 99% of the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "11af138a12f54e9c0a8ce92a782383c9ad5a92ff"
   },
   "outputs": [],
   "source": [
    "seq_list = list()\n",
    "def extract_sents(text, char_offset_p, char_offset_a, char_offset_b, id):\n",
    "    global max_len\n",
    "    global seq_list\n",
    "    \n",
    "    seq_list.append(list())\n",
    "    \n",
    "    doc = nlp(text)\n",
    "    token_lens = [token.idx for token in doc]\n",
    "    \n",
    "    char_offsets = [char_offset_p, char_offset_a, char_offset_b]\n",
    "    sent_list = list()\n",
    "    \n",
    "    for char_offset in char_offsets:\n",
    "        # char offset to token offset\n",
    "        mention_offset = bs(token_lens, char_offset) - 1\n",
    "        # mention_word\n",
    "        mention = doc[mention_offset]\n",
    "    \n",
    "        # token offset to sentence offset\n",
    "        lens = [len(sent) for sent in doc.sents]\n",
    "        acc_lens = [len_ for len_ in lens]\n",
    "        pre_len = 0\n",
    "        for i in range(0, len(acc_lens)):\n",
    "            pre_len += acc_lens[i]\n",
    "            acc_lens[i] = pre_len\n",
    "        sent_index = bs(acc_lens, mention_offset)\n",
    "        # mention sentence\n",
    "        sent = list(doc.sents)[sent_index]\n",
    "        \n",
    "        # absolute position in the sentence\n",
    "        sent_pos = mention_offset + 1\n",
    "        if sent_index > 0:\n",
    "            sent_pos = mention_offset - acc_lens[sent_index-1]\n",
    "        \n",
    "        # clip the sentence if it is longer than max length\n",
    "        if len(sent) > max_len:\n",
    "            # make sure the mention is in the sentence span\n",
    "            if sent_pos < max_len-1:\n",
    "                sent_list.append(sent[0:max_len].text)\n",
    "                sent_list.append(sent_pos)\n",
    "                seq_list[-1].append(sent[0:max_len])\n",
    "            else:\n",
    "                sent_list.append(sent[sent_pos-max_len+2 : min(sent_pos+2, len(sent))].text)\n",
    "                sent_list.append(max_len-2)\n",
    "                seq_list[-1].append(sent[sent_pos-max_len+2 : min(sent_pos+2, len(sent))])\n",
    "        else:\n",
    "            sent_list.append(sent.text)\n",
    "            sent_list.append(sent_pos)\n",
    "            seq_list[-1].append(sent)\n",
    "        \n",
    "    return pd.Series([id] + sent_list, index=['ID', 'Pronoun-Sent', 'Pronoun-Sent-Offset', 'A-Sent', 'A-Sent-Offset', 'B-Sent', 'B-Sent-Offset'])\n",
    "\n",
    "def add_sent_columns(df, text_column, pronoun_offset_column, a_offset_column, b_offset_column):\n",
    "    global seq_list\n",
    "    seq_list = list()\n",
    "    sent_df = df.apply(lambda row: extract_sents(row.loc[text_column], row[pronoun_offset_column], row[a_offset_column], row[b_offset_column], row['ID']), axis=1)\n",
    "    df = df.join(sent_df.set_index('ID'), on='ID')\n",
    "    return df, seq_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9c0630e230a389ab7ba9bc4bc8140cabf769f5ff"
   },
   "source": [
    "## Create Train, Dev and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "f455cc1f3150d1dcc42a3aef2434c8d5e703c990"
   },
   "outputs": [],
   "source": [
    "seq_list = list()\n",
    "train_df, train_tokenized = add_sent_columns(train_df, 'Text', 'Pronoun-offset', 'A-offset', 'B-offset')\n",
    "seq_list = list()\n",
    "test_df, test_tokenized = add_sent_columns(test_df, 'Text', 'Pronoun-offset', 'A-offset', 'B-offset')\n",
    "seq_list = list()\n",
    "dev_df, dev_tokenized = add_sent_columns(dev_df, 'Text', 'Pronoun-offset', 'A-offset', 'B-offset')\n",
    "\n",
    "# df apply will call the first row twice, remove the first one\n",
    "train_tokenized = train_tokenized[1:]\n",
    "test_tokenized = test_tokenized[1:]\n",
    "dev_tokenized = dev_tokenized[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2873a775c7be71521ef5b8922a61f11a93e62b57"
   },
   "source": [
    "### Create Vocab and Embedding Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "2d34b3f5fdce49c97e9fb9d153126546465b3965"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22297, 300)\n"
     ]
    }
   ],
   "source": [
    "embed_size = 300\n",
    "max_features = 80000\n",
    "\n",
    "# generate word index\n",
    "word_index = dict()\n",
    "idx = 1\n",
    "for text_ in train_tokenized+test_tokenized+dev_tokenized:\n",
    "    for sent_ in text_:\n",
    "        for word_ in sent_:\n",
    "            if word_.text not in word_index and nlp.vocab.has_vector(word_.text):\n",
    "                word_index[word_.text] = idx\n",
    "                idx += 1\n",
    "\n",
    "nb_words = min(max_features, len(word_index))\n",
    "embedding_matrix = np.zeros((nb_words + 1, embed_size))\n",
    "        \n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features: continue\n",
    "    embedding_vector = None\n",
    "    if nlp.vocab.has_vector(word):\n",
    "        embedding_vector = nlp.vocab.vectors[nlp.vocab.strings[word]]\n",
    "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "print(embedding_matrix.shape)\n",
    "\n",
    "# generate pos tag index\n",
    "pos_index = dict()\n",
    "idx = 1\n",
    "for text_ in train_tokenized+test_tokenized+dev_tokenized:\n",
    "    for sent_ in text_:\n",
    "        for word_ in sent_:\n",
    "            if word_.pos not in pos_index:\n",
    "                pos_index[word_.pos] = idx\n",
    "                idx += 1\n",
    "\n",
    "def sentences_to_sequences(tokenized_):\n",
    "    return list(map(\n",
    "        lambda sent_tokenized: list(map(\n",
    "            lambda token_: word_index[token_.text] if token_.text in word_index else 0,\n",
    "            sent_tokenized\n",
    "        )),\n",
    "        tokenized_\n",
    "    ))\n",
    "\n",
    "def poses_to_sequences(tokenized_):\n",
    "    return list(map(\n",
    "        lambda sent_tokenized: list(map(\n",
    "            lambda token_: pos_index[token_.pos] if token_.pos in pos_index else 0,\n",
    "            sent_tokenized\n",
    "        )),\n",
    "        tokenized_\n",
    "    ))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_uuid": "41ca3b1434037a8f7296850ba4af9c837e247368"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x176fcd390>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.distplot(train_df['Pronoun-Sent'].map(lambda ele: len(ele.split(\" \"))), kde_kws={\"label\": \"P\"})\n",
    "\n",
    "sns.distplot(train_df['A-Sent'].map(lambda ele: len(ele.split(\" \"))), kde_kws={\"label\": \"A\"})\n",
    "\n",
    "sns.distplot(train_df['B-Sent'].map(lambda ele: len(ele.split(\" \"))), kde_kws={\"label\": \"B\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_uuid": "35dda1595e80bed9e310dd5af9b89b726a878e8b"
   },
   "outputs": [],
   "source": [
    "train_p_tokenized = sentences_to_sequences([row[0] for row in train_tokenized])\n",
    "train_a_tokenized = sentences_to_sequences([row[1] for row in train_tokenized])\n",
    "train_b_tokenized = sentences_to_sequences([row[2] for row in train_tokenized])\n",
    "\n",
    "test_p_tokenized = sentences_to_sequences([row[0] for row in test_tokenized])\n",
    "test_a_tokenized = sentences_to_sequences([row[1] for row in test_tokenized])\n",
    "test_b_tokenized = sentences_to_sequences([row[2] for row in test_tokenized])\n",
    "\n",
    "dev_p_tokenized = sentences_to_sequences([row[0] for row in dev_tokenized])\n",
    "dev_a_tokenized = sentences_to_sequences([row[1] for row in dev_tokenized])\n",
    "dev_b_tokenized = sentences_to_sequences([row[2] for row in dev_tokenized])\n",
    "\n",
    "seq_p_train = sequence.pad_sequences(train_p_tokenized, maxlen = max_len, padding='post')\n",
    "seq_a_train = sequence.pad_sequences(train_a_tokenized, maxlen = max_len, padding='post')\n",
    "seq_b_train = sequence.pad_sequences(train_b_tokenized, maxlen = max_len, padding='post')\n",
    "\n",
    "seq_p_test = sequence.pad_sequences(test_p_tokenized, maxlen = max_len, padding='post')\n",
    "seq_a_test = sequence.pad_sequences(test_a_tokenized, maxlen = max_len, padding='post')\n",
    "seq_b_test = sequence.pad_sequences(test_b_tokenized, maxlen = max_len, padding='post')\n",
    "\n",
    "seq_p_dev = sequence.pad_sequences(dev_p_tokenized, maxlen = max_len, padding='post')\n",
    "seq_a_dev = sequence.pad_sequences(dev_a_tokenized, maxlen = max_len, padding='post')\n",
    "seq_b_dev = sequence.pad_sequences(dev_b_tokenized, maxlen = max_len, padding='post')\n",
    "\n",
    "train_p_pos = poses_to_sequences([row[0] for row in train_tokenized])\n",
    "train_a_pos = poses_to_sequences([row[1] for row in train_tokenized])\n",
    "train_b_pos = poses_to_sequences([row[2] for row in train_tokenized])\n",
    "\n",
    "test_p_pos = poses_to_sequences([row[0] for row in test_tokenized])\n",
    "test_a_pos = poses_to_sequences([row[1] for row in test_tokenized])\n",
    "test_b_pos = poses_to_sequences([row[2] for row in test_tokenized])\n",
    "\n",
    "dev_p_pos = poses_to_sequences([row[0] for row in dev_tokenized])\n",
    "dev_a_pos = poses_to_sequences([row[1] for row in dev_tokenized])\n",
    "dev_b_pos = poses_to_sequences([row[2] for row in dev_tokenized])\n",
    "\n",
    "pos_p_train = sequence.pad_sequences(train_p_pos, maxlen = max_len, padding='post')\n",
    "pos_a_train = sequence.pad_sequences(train_a_pos, maxlen = max_len, padding='post')\n",
    "pos_b_train = sequence.pad_sequences(train_b_pos, maxlen = max_len, padding='post')\n",
    "\n",
    "pos_p_test = sequence.pad_sequences(test_p_pos, maxlen = max_len, padding='post')\n",
    "pos_a_test = sequence.pad_sequences(test_a_pos, maxlen = max_len, padding='post')\n",
    "pos_b_test = sequence.pad_sequences(test_b_pos, maxlen = max_len, padding='post')\n",
    "\n",
    "pos_p_dev = sequence.pad_sequences(dev_p_pos, maxlen = max_len, padding='post')\n",
    "pos_a_dev = sequence.pad_sequences(dev_a_pos, maxlen = max_len, padding='post')\n",
    "pos_b_dev = sequence.pad_sequences(dev_b_pos, maxlen = max_len, padding='post')\n",
    "\n",
    "index_p_train = train_df['Pronoun-Sent-Offset'].values\n",
    "index_a_train = train_df['A-Sent-Offset'].values\n",
    "index_b_train = train_df['B-Sent-Offset'].values\n",
    "\n",
    "index_p_test = test_df['Pronoun-Sent-Offset'].values\n",
    "index_a_test = test_df['A-Sent-Offset'].values\n",
    "index_b_test = test_df['B-Sent-Offset'].values\n",
    "\n",
    "index_p_dev = dev_df['Pronoun-Sent-Offset'].values\n",
    "index_a_dev = dev_df['A-Sent-Offset'].values\n",
    "index_b_dev = dev_df['B-Sent-Offset'].values\n",
    "\n",
    "pa_pos_tra = create_dist_features(train_df, 'Text', 'Pronoun-offset', 'A-offset')\n",
    "pa_pos_dev = create_dist_features(dev_df, 'Text', 'Pronoun-offset', 'A-offset')\n",
    "pa_pos_test = create_dist_features(test_df, 'Text', 'Pronoun-offset', 'A-offset')\n",
    "\n",
    "pb_pos_tra = create_dist_features(train_df, 'Text', 'Pronoun-offset', 'B-offset')\n",
    "pb_pos_dev = create_dist_features(dev_df, 'Text', 'Pronoun-offset', 'B-offset')\n",
    "pb_pos_test = create_dist_features(test_df, 'Text', 'Pronoun-offset', 'B-offset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_uuid": "f46a80420d56262e4fed998d16f9f9105d502c02"
   },
   "outputs": [],
   "source": [
    "X_train = [seq_p_train, seq_a_train, seq_b_train, pos_p_train, pos_a_train, pos_b_train, index_p_train, index_a_train, index_b_train, pa_pos_tra, pb_pos_tra]\n",
    "X_dev = [seq_p_dev, seq_a_dev, seq_b_dev, pos_p_dev, pos_a_dev, pos_b_dev, index_p_dev, index_a_dev, index_b_dev, pa_pos_dev, pb_pos_dev]\n",
    "X_test = [seq_p_test, seq_a_test, seq_b_test, pos_p_test, pos_a_test, pos_b_test, index_p_test, index_a_test, index_b_test, pa_pos_test, pb_pos_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_uuid": "2842410a113a578c1c274630b195038af0dfd75c"
   },
   "outputs": [],
   "source": [
    "def _row_to_y(row):\n",
    "    if row.loc['A-coref']:\n",
    "        return 0\n",
    "    if row.loc['B-coref']:\n",
    "        return 1\n",
    "    return 2\n",
    "\n",
    "y_tra = train_df.apply(_row_to_y, axis=1)\n",
    "y_dev = dev_df.apply(_row_to_y, axis=1)\n",
    "y_test = test_df.apply(_row_to_y, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "33ee19bc3e355ea58dd27bd5c373a13682536dd4"
   },
   "source": [
    "# Define Keras Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_uuid": "cb3cacc8e7dc332a6b5cacf3d733f314b7f03cba"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras import backend\n",
    "from keras import layers\n",
    "from keras import models\n",
    "\n",
    "from keras import initializers, regularizers, constraints, activations\n",
    "from keras.engine import Layer\n",
    "import keras.backend as K\n",
    "from keras.layers import merge\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_uuid": "3830a23390eaf527992b3676e03857699969957e"
   },
   "outputs": [],
   "source": [
    "def _dot_product(x, kernel):\n",
    "    \"\"\"\n",
    "    Wrapper for dot product operation, in order to be compatible with both\n",
    "    Theano and Tensorflow\n",
    "    Args:\n",
    "        x (): input\n",
    "        kernel (): weights\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    if K.backend() == 'tensorflow':\n",
    "        # todo: check that this is correct\n",
    "        return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n",
    "    else:\n",
    "        return K.dot(x, kernel)\n",
    "    \n",
    "    \n",
    "class AttentionWeight(Layer):\n",
    "    \"\"\"\n",
    "        This code is a modified version of cbaziotis implementation:  GithubGist cbaziotis/AttentionWithContext.py\n",
    "        Attention operation, with a context/query vector, for temporal data.\n",
    "        Supports Masking.\n",
    "        Follows the work of Yang et al. [https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf]\n",
    "        \"Hierarchical Attention Networks for Document Classification\"\n",
    "        by using a context vector to assist the attention\n",
    "        # Input shape\n",
    "            3D tensor with shape: `(samples, steps, features)`.\n",
    "        # Output shape\n",
    "            2D tensor with shape: `(samples, steps)`.\n",
    "        :param kwargs:\n",
    "        Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
    "        The dimensions are inferred based on the output shape of the RNN.\n",
    "        Example:\n",
    "            model.add(LSTM(64, return_sequences=True))\n",
    "            model.add(AttentionWeight())\n",
    "        \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        super(AttentionWeight, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        shape1 = input_shape[0]\n",
    "        shape2 = input_shape[1]\n",
    "\n",
    "        self.W = self.add_weight((shape2[-1], shape1[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((shape2[-1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        x = inputs[0]\n",
    "        u = inputs[1]\n",
    "        \n",
    "        uit = _dot_product(x, self.W)\n",
    "\n",
    "        if self.bias:\n",
    "            uit += self.b\n",
    "\n",
    "        uit = K.tanh(uit)\n",
    "        ait = K.batch_dot(uit, u)\n",
    "\n",
    "        a = K.exp(ait)\n",
    "\n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\n",
    "        # and this results in NaN's. A workaround is to add a very small positive number ε to the sum.\n",
    "        # a /= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx())\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "        \n",
    "        return a\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        if not isinstance(input_shape, list) or len(input_shape) != 2:\n",
    "            raise ValueError('A `Dot` layer should be called '\n",
    "                             'on a list of 2 inputs.')\n",
    "        shape1 = list(input_shape[0])\n",
    "        shape2 = list(input_shape[1])\n",
    "        \n",
    "        return shape1[0], shape1[1]\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'W_regularizer': regularizers.serialize(self.W_regularizer),\n",
    "            'b_regularizer': regularizers.serialize(self.b_regularizer),\n",
    "            'W_constraint': constraints.serialize(self.W_constraint),\n",
    "            'b_constraint': constraints.serialize(self.b_constraint),\n",
    "            'bias': self.bias\n",
    "        }\n",
    "        base_config = super(AttentionWeight, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "    \n",
    "    \n",
    "class FeatureSelection1D(Layer):\n",
    "    \"\"\"\n",
    "        Normalize feature along a specific axis.\n",
    "        Supports Masking.\n",
    "\n",
    "        # Input shape\n",
    "            A ND tensor with shape: `(samples, timesteps, features)\n",
    "            A 2D tensor with shape: [samples, num_selected_features]\n",
    "        # Output shape\n",
    "            ND tensor with shape: `(samples, num_selected_features, features)`.\n",
    "        :param kwargs:\n",
    "        \"\"\"\n",
    "\n",
    "    def __init__(self, num_selects, **kwargs):\n",
    "\n",
    "        self.num_selects = num_selects\n",
    "        self.supports_masking = True\n",
    "        super(FeatureSelection1D, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "\n",
    "        super(FeatureSelection1D, self).build(input_shape)\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # don't pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        if not isinstance(inputs, list) or len(inputs) != 2:\n",
    "            raise ValueError('FeatureSelection1D layer should be called '\n",
    "                             'on a list of 2 inputs.')\n",
    "\n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            a = K.cast(mask, K.floatx()) * inputs[0]\n",
    "        else:\n",
    "            a = inputs[0]\n",
    "\n",
    "        b = inputs[1]\n",
    "\n",
    "        a = tf.batch_gather(\n",
    "            a, b\n",
    "        )\n",
    "\n",
    "        return a\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        if not isinstance(input_shape, list) or len(input_shape) != 2:\n",
    "            raise ValueError('A `FeatureSelection1D` layer should be called '\n",
    "                             'on a list of 2 inputs.')\n",
    "        shape1 = list(input_shape[0])\n",
    "        shape2 = list(input_shape[1])\n",
    "\n",
    "        if shape2[0] != shape1[0]:\n",
    "            raise ValueError(\"batch size must be same\")\n",
    "\n",
    "        if shape2[1] != self.num_selects:\n",
    "            raise ValueError(\"must conform to the num_select\")\n",
    "\n",
    "        return (shape1[0], self.num_selects, shape1[2])\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'num_selects': self.num_selects\n",
    "        }\n",
    "        base_config = super(FeatureSelection1D, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "bf87bf600dac037777be8e3dda560ec65b6640a2"
   },
   "source": [
    "# Build and Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_uuid": "03e8297383ee57a4a23c707a4a6c1cca13da1868"
   },
   "outputs": [],
   "source": [
    "from keras import callbacks as kc\n",
    "from keras import optimizers as ko\n",
    "from keras import initializers, regularizers, constraints\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import SVG\n",
    "\n",
    "histories = list()\n",
    "file_paths = list()\n",
    "cos = list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5503bd66a2a7fea5bcb093c775d69baded3e759d"
   },
   "source": [
    "## End-to-End RNN Attention Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5a954b42159bc4a8a17b4e15083f026f53362a43"
   },
   "source": [
    "### Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_uuid": "8feec4878e5b817dcbf5ed7f49d9b78cce36ebd6"
   },
   "outputs": [],
   "source": [
    "def build_e2e_birnn_attention_model(\n",
    "        voca_dim, time_steps, pos_tag_size, pos_tag_dim, extra_feature_dims,\n",
    "        output_dim, rnn_dim, model_dim, mlp_dim,\n",
    "        item_embedding=None, rnn_depth=1, mlp_depth=1,\n",
    "        drop_out=0.5, rnn_drop_out=0., rnn_state_drop_out=0.,\n",
    "        trainable_embedding=False, gpu=False, return_customized_layers=False):\n",
    "    \"\"\"\n",
    "    Create A End-to-End Bidirectional RNN Attention Model.\n",
    "\n",
    "    :param voca_dim: vocabulary dimension size.\n",
    "    :param time_steps: the length of input\n",
    "    :param extra_feature_dims: the dimention size of the auxilary feature\n",
    "    :param output_dim: the output dimension size\n",
    "    :param model_dim: rrn dimension size\n",
    "    :param mlp_dim: the dimension size of fully connected layer\n",
    "    :param item_embedding: integer, numpy 2D array, or None (default=None)\n",
    "        If item_embedding is a integer, connect a randomly initialized embedding matrix to the input tensor.\n",
    "        If item_embedding is a matrix, this matrix will be used as the embedding matrix.\n",
    "        If item_embedding is None, then connect input tensor to RNN layer directly.\n",
    "    :param rnn_depth: rnn depth\n",
    "    :param mlp_depth: the depth of fully connected layers\n",
    "    :param drop_out: dropout rate of fully connected layers\n",
    "    :param rnn_drop_out: dropout rate of rnn layers\n",
    "    :param rnn_state_drop_out: dropout rate of rnn state tensor\n",
    "    :param trainable_embedding: boolean\n",
    "    :param gpu: boolean, default=False\n",
    "        If True, CuDNNLSTM is used instead of LSTM for RNN layer.\n",
    "    :param return_customized_layers: boolean, default=False\n",
    "        If True, return model and customized object dictionary, otherwise return model only\n",
    "    :return: keras model\n",
    "    \"\"\"\n",
    "    \n",
    "    # sequences inputs\n",
    "    if item_embedding is not None:\n",
    "        inputp = models.Input(shape=(time_steps,), dtype='int32', name='inputp')\n",
    "        inputa = models.Input(shape=(time_steps,), dtype='int32', name='inputa')\n",
    "        inputb = models.Input(shape=(time_steps,), dtype='int32', name='inputb')\n",
    "        inputs = [inputp, inputa, inputb]\n",
    "        \n",
    "        if isinstance(item_embedding, np.ndarray):\n",
    "            assert voca_dim == item_embedding.shape[0]\n",
    "            embed_dim = item_embedding.shape[1]\n",
    "            emb_layer = layers.Embedding(\n",
    "                voca_dim, item_embedding.shape[1], input_length=time_steps,\n",
    "                weights=[item_embedding, ], trainable=trainable_embedding,\n",
    "                mask_zero=False, name='embedding_layer0'\n",
    "            )\n",
    "        elif utils.is_integer(item_embedding):\n",
    "            embed_dim = item_embedding\n",
    "            emb_layer = layers.Embedding(\n",
    "                voca_dim, item_embedding, input_length=time_steps,\n",
    "                trainable=trainable_embedding,\n",
    "                mask_zero=False, name='embedding_layer0'\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\"item_embedding must be either integer or numpy matrix\")\n",
    "\n",
    "        xs = list(map(\n",
    "            lambda input_: emb_layer(input_),\n",
    "            inputs\n",
    "        ))\n",
    "    else:\n",
    "        inputp = models.Input(shape=(time_steps, voca_dim), dtype='float32', name='inputp')\n",
    "        inputa = models.Input(shape=(time_steps, voca_dim), dtype='float32', name='inputa')\n",
    "        inputb = models.Input(shape=(time_steps, voca_dim), dtype='float32', name='inputb')\n",
    "        embed_dim = voca_dim\n",
    "        xs = [inputp, inputa, inputb]\n",
    "        \n",
    "    # pos tag\n",
    "    inputposp = models.Input(shape=(time_steps,), dtype='int32', name='inputposp')\n",
    "    inputposa = models.Input(shape=(time_steps,), dtype='int32', name='inputposa')\n",
    "    inputposb = models.Input(shape=(time_steps,), dtype='int32', name='inputposb')\n",
    "    inputpos = [inputposp, inputposa, inputposb]\n",
    "    pos_emb_layer = layers.Embedding(\n",
    "        pos_tag_size, pos_tag_dim, input_length=time_steps,\n",
    "        trainable=True, mask_zero=False, name='pos_embedding_layer0'\n",
    "    )\n",
    "    xpos = list(map(\n",
    "        lambda input_: pos_emb_layer(input_),\n",
    "        inputpos\n",
    "    ))\n",
    "    \n",
    "    embed_concate_layer = layers.Concatenate(axis=2, name=\"embed_concate_layer\")\n",
    "    for i in range(len(xs)):\n",
    "        xs[i] = embed_concate_layer([xs[i], xpos[i]])\n",
    "    \n",
    "    # mention position in the sentence\n",
    "    inputpi = models.Input(shape=(1,), dtype='int32', name='inputpi')\n",
    "    inputai = models.Input(shape=(1,), dtype='int32', name='inputai')\n",
    "    inputbi = models.Input(shape=(1,), dtype='int32', name='inputbi')\n",
    "    xis = [inputpi, inputai, inputbi]\n",
    "    \n",
    "    # addtional mention-pair features\n",
    "    inputpa = models.Input(shape=(extra_feature_dims,), dtype='float32', name='inputpa')\n",
    "    inputpb = models.Input(shape=(extra_feature_dims,), dtype='float32', name='inputpb')\n",
    "    xextrs = [inputpa, inputpb]\n",
    "    \n",
    "    # rnn\n",
    "    birnns = list()\n",
    "    rnn_batchnorms = list()\n",
    "    rnn_dropouts = list()\n",
    "    if gpu:\n",
    "        # rnn encoding\n",
    "        for i in range(rnn_depth):\n",
    "            rnn_dropout = layers.SpatialDropout1D(rnn_drop_out)\n",
    "            birnn = layers.Bidirectional(\n",
    "                layers.CuDNNGRU(rnn_dim, return_sequences=True),\n",
    "                name='bi_lstm_layer' + str(i))\n",
    "            rnn_batchnorm = layers.BatchNormalization(name='rnn_batch_norm_layer' + str(i))\n",
    "            \n",
    "            birnns.append(birnn)\n",
    "            rnn_dropouts.append(rnn_dropout)\n",
    "            rnn_batchnorms.append(rnn_batchnorm)\n",
    "        \n",
    "        xs_ = list()\n",
    "        for x_ in xs:\n",
    "            for i in range(len(birnns)):\n",
    "                x_ = rnn_dropouts[i](x_)\n",
    "                x_ = birnns[i](x_)\n",
    "                x_ = rnn_batchnorms[i](x_)\n",
    "            xs_.append(x_)\n",
    "        xs = xs_\n",
    "    else:\n",
    "        # rnn encoding\n",
    "        for i in range(rnn_depth):\n",
    "            birnn = layers.Bidirectional(\n",
    "                layers.GRU(rnn_dim, return_sequences=True, dropout=rnn_drop_out,\n",
    "                            recurrent_dropout=rnn_state_drop_out),\n",
    "                name='bi_lstm_layer' + str(i))\n",
    "            rnn_batchnorm = layers.BatchNormalization(name='rnn_batch_norm_layer' + str(i))\n",
    "            \n",
    "            birnns.append(birnn)\n",
    "            rnn_batchnorms.append(rnn_batchnorm)\n",
    "            \n",
    "        xs_ = list()\n",
    "        for x_ in xs:\n",
    "            for i in range(len(birnns)):\n",
    "                x_ = birnns[i](x_)\n",
    "                x_ = rnn_batchnorms[i](x_)\n",
    "            xs_.append(x_)\n",
    "        xs = xs_\n",
    "    \n",
    "    # attention aggregated rnn embedding + mention rnn embedding + mention-pair features\n",
    "    select_layer = FeatureSelection1D(1, name='boundary_selection_layer')\n",
    "    flatten_layer1 = layers.Flatten('channels_first', name=\"flatten_layer1\")\n",
    "    permute_layer = layers.Permute((2, 1), name='permuted_attention_x')\n",
    "    attent_weight = AttentionWeight(name=\"attention_weight\")\n",
    "    focus_layer = layers.Dot([2, 1], name='focus' + '_layer')\n",
    "    reshape_layer = layers.Reshape((1, rnn_dim*2), name=\"reshape_layer\")\n",
    "    concate_layer = layers.Concatenate(axis=1, name=\"attention_concate_layer\")\n",
    "    atten_dropout_layer = layers.Dropout(drop_out, name='attention_dropout_layer')\n",
    "    map_layer1 = layers.Dense(model_dim, activation=\"relu\", name=\"map_layer1\")\n",
    "    #map_layer2 = layers.TimeDistributed(layers.Dense(model_dim, activation=\"relu\"), name=\"map_layer2\")\n",
    "    map_layer2 = map_layer1\n",
    "    flatten_layer = layers.Flatten('channels_first', name=\"flatten_layer\")\n",
    "    for i in range(len(xs)):\n",
    "        if i == 0:\n",
    "            map_layer = map_layer1\n",
    "        else:\n",
    "            map_layer = map_layer2\n",
    "            \n",
    "        select_ = select_layer([xs[i], xis[i]])\n",
    "        flatten_select_ = flatten_layer1(select_)\n",
    "        att = attent_weight([xs[i], flatten_select_])\n",
    "        \n",
    "        focus = focus_layer([permute_layer(xs[i]), att])\n",
    "        xs[i] = concate_layer([select_, reshape_layer(focus)])\n",
    "        xs[i] = flatten_layer(xs[i])\n",
    "        xs[i] = atten_dropout_layer(xs[i])\n",
    "        xs[i] = map_layer(xs[i])\n",
    "    \n",
    "    feature_dropout_layer = layers.Dropout(rate=drop_out, name=\"feature_dropout_layer\")\n",
    "    feature_map_layer = layers.Dense(model_dim, activation=\"relu\",name=\"feature_map_layer\")\n",
    "    xextrs = [feature_map_layer(feature_dropout_layer(xextr)) for xextr in xextrs]\n",
    "    \n",
    "    x = layers.Concatenate(axis=1, name=\"concat_feature_layer\")(xs + xextrs)\n",
    "    x = layers.Dropout(drop_out, name='dropout_layer')(x)\n",
    "\n",
    "    # MLP Layers\n",
    "    for i in range(mlp_depth - 1):\n",
    "        x = layers.Dense(mlp_dim, activation='selu', kernel_initializer='lecun_normal', name='selu_layer' + str(i))(x)\n",
    "        x = layers.AlphaDropout(drop_out, name='alpha_layer' + str(i))(x)\n",
    "\n",
    "    outputs = layers.Dense(output_dim, activation=\"softmax\", name=\"softmax_layer0\")(x)\n",
    "\n",
    "    model = models.Model([inputp, inputa, inputb] + inputpos + xis + [inputpa, inputpb], outputs)\n",
    "\n",
    "    if return_customized_layers:\n",
    "        return model, {'FeatureSelection1D': FeatureSelection1D, 'AttentionWeight': AttentionWeight}\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f134f020b4c1c7f9e8ce4cfc801b015d9090e249"
   },
   "source": [
    "### Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "_uuid": "7c0e4a38b4fa5e55811bdb4672dc5a596e70c433"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/frederikwarburg/anaconda3/envs/neuralcoref/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /Users/frederikwarburg/anaconda3/envs/neuralcoref/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "voca_dim = embedding_matrix.shape[0]\n",
    "pos_tag_size = len(pos_index) + 1 # my edit\n",
    "time_steps = max_len\n",
    "\n",
    "embed_dim = embedding_matrix.shape[1]\n",
    "pos_tag_dim = 5\n",
    "extra_feature_dims = num_pos_features\n",
    "output_dim = 3\n",
    "rnn_dim = 50\n",
    "model_dim = 10\n",
    "mlp_dim = 10\n",
    "rnn_depth = 1\n",
    "mlp_depth=1\n",
    "drop_out=0.2\n",
    "rnn_drop_out=0.5\n",
    "gpu = False\n",
    "return_customized_layers=True\n",
    "\n",
    "model, co = build_e2e_birnn_attention_model(\n",
    "        voca_dim, time_steps, pos_tag_size, pos_tag_dim, extra_feature_dims, output_dim, rnn_dim, model_dim, mlp_dim,\n",
    "        item_embedding=embedding_matrix, rnn_depth=rnn_depth, mlp_depth=mlp_depth,\n",
    "        drop_out=drop_out, rnn_drop_out=rnn_drop_out, rnn_state_drop_out=rnn_drop_out,\n",
    "        trainable_embedding=False, gpu=gpu, return_customized_layers=return_customized_layers)\n",
    "cos.append(co)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from keras.utils import plot_model\n",
    "plot_model(model, to_file='model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "_uuid": "f56526943859c2cd1c3738e043059a4951dd92f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "inputp (InputLayer)             (None, 50)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "inputposp (InputLayer)          (None, 50)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "inputa (InputLayer)             (None, 50)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "inputposa (InputLayer)          (None, 50)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "inputb (InputLayer)             (None, 50)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "inputposb (InputLayer)          (None, 50)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_layer0 (Embedding)    (None, 50, 300)      6689100     inputp[0][0]                     \n",
      "                                                                 inputa[0][0]                     \n",
      "                                                                 inputb[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "pos_embedding_layer0 (Embedding (None, 50, 5)        90          inputposp[0][0]                  \n",
      "                                                                 inputposa[0][0]                  \n",
      "                                                                 inputposb[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "embed_concate_layer (Concatenat (None, 50, 305)      0           embedding_layer0[0][0]           \n",
      "                                                                 pos_embedding_layer0[0][0]       \n",
      "                                                                 embedding_layer0[1][0]           \n",
      "                                                                 pos_embedding_layer0[1][0]       \n",
      "                                                                 embedding_layer0[2][0]           \n",
      "                                                                 pos_embedding_layer0[2][0]       \n",
      "__________________________________________________________________________________________________\n",
      "bi_lstm_layer0 (Bidirectional)  (None, 50, 100)      106800      embed_concate_layer[0][0]        \n",
      "                                                                 embed_concate_layer[1][0]        \n",
      "                                                                 embed_concate_layer[2][0]        \n",
      "__________________________________________________________________________________________________\n",
      "rnn_batch_norm_layer0 (BatchNor (None, 50, 100)      400         bi_lstm_layer0[0][0]             \n",
      "                                                                 bi_lstm_layer0[1][0]             \n",
      "                                                                 bi_lstm_layer0[2][0]             \n",
      "__________________________________________________________________________________________________\n",
      "inputpi (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "inputai (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "inputbi (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "boundary_selection_layer (Featu (None, 1, 100)       0           rnn_batch_norm_layer0[0][0]      \n",
      "                                                                 inputpi[0][0]                    \n",
      "                                                                 rnn_batch_norm_layer0[1][0]      \n",
      "                                                                 inputai[0][0]                    \n",
      "                                                                 rnn_batch_norm_layer0[2][0]      \n",
      "                                                                 inputbi[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_layer1 (Flatten)        (None, 100)          0           boundary_selection_layer[0][0]   \n",
      "                                                                 boundary_selection_layer[1][0]   \n",
      "                                                                 boundary_selection_layer[2][0]   \n",
      "__________________________________________________________________________________________________\n",
      "permuted_attention_x (Permute)  (None, 100, 50)      0           rnn_batch_norm_layer0[0][0]      \n",
      "                                                                 rnn_batch_norm_layer0[1][0]      \n",
      "                                                                 rnn_batch_norm_layer0[2][0]      \n",
      "__________________________________________________________________________________________________\n",
      "attention_weight (AttentionWeig (None, 50)           10100       rnn_batch_norm_layer0[0][0]      \n",
      "                                                                 flatten_layer1[0][0]             \n",
      "                                                                 rnn_batch_norm_layer0[1][0]      \n",
      "                                                                 flatten_layer1[1][0]             \n",
      "                                                                 rnn_batch_norm_layer0[2][0]      \n",
      "                                                                 flatten_layer1[2][0]             \n",
      "__________________________________________________________________________________________________\n",
      "focus_layer (Dot)               (None, 100)          0           permuted_attention_x[0][0]       \n",
      "                                                                 attention_weight[0][0]           \n",
      "                                                                 permuted_attention_x[1][0]       \n",
      "                                                                 attention_weight[1][0]           \n",
      "                                                                 permuted_attention_x[2][0]       \n",
      "                                                                 attention_weight[2][0]           \n",
      "__________________________________________________________________________________________________\n",
      "reshape_layer (Reshape)         (None, 1, 100)       0           focus_layer[0][0]                \n",
      "                                                                 focus_layer[1][0]                \n",
      "                                                                 focus_layer[2][0]                \n",
      "__________________________________________________________________________________________________\n",
      "attention_concate_layer (Concat (None, 2, 100)       0           boundary_selection_layer[0][0]   \n",
      "                                                                 reshape_layer[0][0]              \n",
      "                                                                 boundary_selection_layer[1][0]   \n",
      "                                                                 reshape_layer[1][0]              \n",
      "                                                                 boundary_selection_layer[2][0]   \n",
      "                                                                 reshape_layer[2][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_layer (Flatten)         (None, 200)          0           attention_concate_layer[0][0]    \n",
      "                                                                 attention_concate_layer[1][0]    \n",
      "                                                                 attention_concate_layer[2][0]    \n",
      "__________________________________________________________________________________________________\n",
      "inputpa (InputLayer)            (None, 45)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "inputpb (InputLayer)            (None, 45)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "attention_dropout_layer (Dropou (None, 200)          0           flatten_layer[0][0]              \n",
      "                                                                 flatten_layer[1][0]              \n",
      "                                                                 flatten_layer[2][0]              \n",
      "__________________________________________________________________________________________________\n",
      "feature_dropout_layer (Dropout) (None, 45)           0           inputpa[0][0]                    \n",
      "                                                                 inputpb[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "map_layer1 (Dense)              (None, 10)           2010        attention_dropout_layer[0][0]    \n",
      "                                                                 attention_dropout_layer[1][0]    \n",
      "                                                                 attention_dropout_layer[2][0]    \n",
      "__________________________________________________________________________________________________\n",
      "feature_map_layer (Dense)       (None, 10)           460         feature_dropout_layer[0][0]      \n",
      "                                                                 feature_dropout_layer[1][0]      \n",
      "__________________________________________________________________________________________________\n",
      "concat_feature_layer (Concatena (None, 50)           0           map_layer1[0][0]                 \n",
      "                                                                 map_layer1[1][0]                 \n",
      "                                                                 map_layer1[2][0]                 \n",
      "                                                                 feature_map_layer[0][0]          \n",
      "                                                                 feature_map_layer[1][0]          \n",
      "__________________________________________________________________________________________________\n",
      "dropout_layer (Dropout)         (None, 50)           0           concat_feature_layer[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "softmax_layer0 (Dense)          (None, 3)            153         dropout_layer[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 6,809,113\n",
      "Trainable params: 119,813\n",
      "Non-trainable params: 6,689,300\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2c926e888dce8ff928430dc76d4771cb67aea2a6"
   },
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.13.1'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow\n",
    "tensorflow.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "_uuid": "02490fd68b2c304432f68d8c356b3013084d8d3d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/frederikwarburg/anaconda3/envs/neuralcoref/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/frederikwarburg/anaconda3/envs/neuralcoref/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:110: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/Users/frederikwarburg/anaconda3/envs/neuralcoref/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:110: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/Users/frederikwarburg/anaconda3/envs/neuralcoref/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:110: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2000 samples, validate on 454 samples\n",
      "Epoch 1/40\n",
      "2000/2000 [==============================] - 74s 37ms/step - loss: 1.1541 - sparse_categorical_accuracy: 0.4350 - val_loss: 0.9692 - val_sparse_categorical_accuracy: 0.4956\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.96921, saving model to best_e2e_rnn_atten_model.hdf5\n",
      "Epoch 2/40\n",
      "2000/2000 [==============================] - 51s 26ms/step - loss: 0.9406 - sparse_categorical_accuracy: 0.5485 - val_loss: 0.8808 - val_sparse_categorical_accuracy: 0.6013\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.96921 to 0.88080, saving model to best_e2e_rnn_atten_model.hdf5\n",
      "Epoch 3/40\n",
      "2000/2000 [==============================] - 50s 25ms/step - loss: 0.8324 - sparse_categorical_accuracy: 0.6355 - val_loss: 0.8074 - val_sparse_categorical_accuracy: 0.6189\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.88080 to 0.80744, saving model to best_e2e_rnn_atten_model.hdf5\n",
      "Epoch 4/40\n",
      "2000/2000 [==============================] - 47s 24ms/step - loss: 0.7817 - sparse_categorical_accuracy: 0.6555 - val_loss: 0.7711 - val_sparse_categorical_accuracy: 0.6189\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.80744 to 0.77109, saving model to best_e2e_rnn_atten_model.hdf5\n",
      "Epoch 5/40\n",
      "2000/2000 [==============================] - 47s 24ms/step - loss: 0.7477 - sparse_categorical_accuracy: 0.6730 - val_loss: 0.7613 - val_sparse_categorical_accuracy: 0.6322\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.77109 to 0.76130, saving model to best_e2e_rnn_atten_model.hdf5\n",
      "Epoch 6/40\n",
      "2000/2000 [==============================] - 50s 25ms/step - loss: 0.7173 - sparse_categorical_accuracy: 0.6910 - val_loss: 0.7571 - val_sparse_categorical_accuracy: 0.6564\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.76130 to 0.75715, saving model to best_e2e_rnn_atten_model.hdf5\n",
      "Epoch 7/40\n",
      "2000/2000 [==============================] - 56s 28ms/step - loss: 0.6922 - sparse_categorical_accuracy: 0.6965 - val_loss: 0.7443 - val_sparse_categorical_accuracy: 0.6740\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.75715 to 0.74433, saving model to best_e2e_rnn_atten_model.hdf5\n",
      "Epoch 8/40\n",
      "2000/2000 [==============================] - 52s 26ms/step - loss: 0.6762 - sparse_categorical_accuracy: 0.7120 - val_loss: 0.7604 - val_sparse_categorical_accuracy: 0.6564\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.74433\n",
      "Epoch 9/40\n",
      "2000/2000 [==============================] - 47s 23ms/step - loss: 0.6608 - sparse_categorical_accuracy: 0.7150 - val_loss: 0.7699 - val_sparse_categorical_accuracy: 0.6542\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.74433\n",
      "Epoch 10/40\n",
      "2000/2000 [==============================] - 47s 23ms/step - loss: 0.6495 - sparse_categorical_accuracy: 0.7160 - val_loss: 0.7627 - val_sparse_categorical_accuracy: 0.6652\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.74433\n",
      "Epoch 11/40\n",
      "2000/2000 [==============================] - 47s 23ms/step - loss: 0.6358 - sparse_categorical_accuracy: 0.7215 - val_loss: 0.7630 - val_sparse_categorical_accuracy: 0.6718\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.74433\n",
      "Epoch 12/40\n",
      "2000/2000 [==============================] - 53s 26ms/step - loss: 0.6132 - sparse_categorical_accuracy: 0.7425 - val_loss: 0.7707 - val_sparse_categorical_accuracy: 0.6740\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.74433\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adam = ko.Nadam(clipnorm=1.0)\n",
    "model.compile(adam, loss=\"sparse_categorical_crossentropy\", metrics=[\"sparse_categorical_accuracy\"])\n",
    "\n",
    "file_path = \"best_e2e_rnn_atten_model.hdf5\"\n",
    "check_point = kc.ModelCheckpoint(file_path, monitor = \"val_loss\", verbose = 1, save_best_only = True, mode = \"min\")\n",
    "early_stop = kc.EarlyStopping(monitor = \"val_loss\", mode = \"min\", patience=5)\n",
    "history = model.fit(X_train, y_tra, batch_size=30, epochs=40, validation_data=(X_dev, y_dev),\n",
    "                    callbacks = [check_point, early_stop])\n",
    "\n",
    "file_paths.append(file_path)\n",
    "histories.append(np.min(np.asarray(history.history['val_loss'])))\n",
    "\n",
    "del model, history\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8bb147615d4920a2f3500a0fba12d31cb0869304"
   },
   "source": [
    "###  Make Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "_uuid": "79cb8d339da04ad2d52e3675a4295f55a6ccd148"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load best model: best_e2e_rnn_atten_model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/frederikwarburg/anaconda3/envs/neuralcoref/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:110: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/Users/frederikwarburg/anaconda3/envs/neuralcoref/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:110: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/Users/frederikwarburg/anaconda3/envs/neuralcoref/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:110: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "print(\"load best model: \" + str(file_paths[np.argmin(histories)]))\n",
    "model = models.load_model(\n",
    "    file_paths[np.argmin(histories)], cos[np.argmin(histories)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "_uuid": "4c465342c3a99a4996a104fdb00ceab4e85b9649"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 10s 5ms/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>NEITHER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>development-1</td>\n",
       "      <td>0.137643</td>\n",
       "      <td>0.780434</td>\n",
       "      <td>0.081923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>development-2</td>\n",
       "      <td>0.866506</td>\n",
       "      <td>0.107988</td>\n",
       "      <td>0.025506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>development-3</td>\n",
       "      <td>0.282791</td>\n",
       "      <td>0.608398</td>\n",
       "      <td>0.108811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>development-4</td>\n",
       "      <td>0.294924</td>\n",
       "      <td>0.508139</td>\n",
       "      <td>0.196937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>development-5</td>\n",
       "      <td>0.190956</td>\n",
       "      <td>0.471134</td>\n",
       "      <td>0.337910</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              ID         A         B   NEITHER\n",
       "0  development-1  0.137643  0.780434  0.081923\n",
       "1  development-2  0.866506  0.107988  0.025506\n",
       "2  development-3  0.282791  0.608398  0.108811\n",
       "3  development-4  0.294924  0.508139  0.196937\n",
       "4  development-5  0.190956  0.471134  0.337910"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_preds = model.predict(X_test, batch_size = 1024, verbose = 1)\n",
    "\n",
    "sub_df_path = os.path.join(SUB_DATA_FOLDER, 'sample_submission_stage_1.csv')\n",
    "sub_df = pd.read_csv(sub_df_path)\n",
    "sub_df.loc[:, 'A'] = pd.Series(y_preds[:, 0])\n",
    "sub_df.loc[:, 'B'] = pd.Series(y_preds[:, 1])\n",
    "sub_df.loc[:, 'NEITHER'] = pd.Series(y_preds[:, 2])\n",
    "\n",
    "sub_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "_uuid": "6cfa1933801af3de8d5d6cfc1cd9c3e23481e0bd"
   },
   "outputs": [],
   "source": [
    "sub_df.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from calculatescore import calculate_score, eval_gender, evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary = np.zeros((2000,3),dtype=int)\n",
    "for i, row in sub_df.iterrows():\n",
    "    max_ = max(row['A'], row['B'], row['NEITHER'])\n",
    "    if max_ == row['A']:\n",
    "        binary[i,0] = 1\n",
    "    elif max_ == row['B']:\n",
    "        binary[i,1] = 1\n",
    "    else:\n",
    "        binary[i,2] = 1\n",
    "    \n",
    "binary[:10,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>NEITHER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   A  B  NEITHER\n",
       "0  0  1        0\n",
       "1  1  0        0\n",
       "2  0  1        0\n",
       "3  0  1        0\n",
       "4  0  1        0"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random guess\n",
    "predictions = pd.DataFrame(binary, columns=['A','B','NEITHER'])\n",
    "predictions.to_csv('data/predictions_end2end_trained1.csv')\n",
    "predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.42840788617005"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_score('data/right_answers.csv','data/predictions_end2end_trained1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.67505241 0.70942982 0.46268657] [0.73684211 0.69945946 0.30845771] [0.70459519 0.70440936 0.37014925]\n"
     ]
    }
   ],
   "source": [
    "precision, recall, f1 = evaluate('data/right_answers.csv', 'data/predictions_end2end_trained1.csv')\n",
    "print(precision, recall, f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Male precision: A = 0.7063, B = 0.7267, Neither = 0.4615\n",
      "Male recall: A = 0.7465753424657534, B = 0.7472766884531591, Neither = 0.2912621359223301\n",
      "Male F1: A = 0.7259, B = 0.7368, Neither = 0.3571 \n",
      "\n",
      "Female precision: A = 0.6456211812627292, B = 0.6909090909090909, Neither = 0.463768115942029\n",
      "Female recall: A = 0.7270642201834863, B = 0.6523605150214592, Neither = 0.32653061224489793\n",
      "Female F1: A = 0.6839, B = 0.6711, Neither = 0.3832\n"
     ]
    }
   ],
   "source": [
    "# Gold path\n",
    "path_gold = 'data/right_answers.csv'\n",
    "\n",
    "# Path full data #\n",
    "path_full = 'data/gendered-pronoun-resolution/test_stage_1.tsv'\n",
    "df = pd.read_csv(path_full, sep = '\\t', index_col = 0) \n",
    "\n",
    "# Create predictions #\n",
    "predictions = pd.DataFrame(binary, columns=['A','B','NEITHER'], index = list(df.index))\n",
    "predictions.to_csv('data/predictions_end2end_trained2.csv')\n",
    "path_pred = 'data/predictions_end2end_trained2.csv'\n",
    "\n",
    "male_sc, female_sc = eval_gender(path_gold, path_pred, path_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "female\n",
      "0.6000994627046163 0.5686517824832812 0.5794139519100066\n",
      "male\n",
      "0.6314989585709284 0.5950380556137476 0.6066150392629743\n"
     ]
    }
   ],
   "source": [
    "print(\"female\")\n",
    "print(np.mean(female_sc['precision']), np.mean(female_sc['recall']),np.mean(female_sc['f1']))\n",
    "\n",
    "print(\"male\")\n",
    "print(np.mean(male_sc['precision']), np.mean(male_sc['recall']),np.mean(male_sc['f1']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuralcoref",
   "language": "python",
   "name": "neuralcoref"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
